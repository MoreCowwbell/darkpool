{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2eeaa72d",
      "metadata": {},
      "source": [
        "# Circos-Style Money Flow Chord Diagram\n",
        "\n",
        "This notebook builds a **self-contained** Circos-style chord diagram from Accumulation Score time series in a local database.\n",
        "\n",
        "Constraints honored:\n",
        "- No project modules are imported or modified.\n",
        "- Database access is read-only (SELECT-only).\n",
        "- No files are written or mutated; all outputs are in-notebook only.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Average of daily changes over the last 5 trading days\n",
        "\n",
        "WINDOW_DAYS = 1\n",
        "FLOW_AVG_DAYS = 5\n",
        "This answers ?who consistently lost/gained flow day-to-day over the past 5 days.?\n",
        "\n",
        "Single 5-day change (today vs 5 trading days ago)\n",
        "\n",
        "WINDOW_DAYS = 5\n",
        "FLOW_AVG_DAYS = 1\n",
        "This answers ?who lost/gained the most over the full 5-day window.?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c759775f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ticker groups (hardcoded)\n",
        "SECTOR_CORE_TICKERS = [\n",
        "    \"XLF\",\"KRE\",\"XLK\",\"SMH\",\"XLI\",\"XLY\",\"XLE\",\"XLV\",\"XLP\",\"XLU\"\n",
        "]\n",
        "\n",
        "GLOBAL_MACRO_TICKERS = [\n",
        "    \"SPY\",\"QQQ\",\"TQQQ\",\"IWM\",\"VGK\",\"EWJ\",\"EFA\",\"EEM\",\"FXI\",\"UUP\",\"TLT\",\"GLD\",\"USO\",\"VIXY\"\n",
        "]\n",
        "\n",
        "COMMODITIES_TICKERS = [\n",
        "    \"GLD\",\"SLV\",\"GDX\",\"USO\",\"UNG\",\"URA\"\n",
        "]\n",
        "\n",
        "# Controls\n",
        "END_DATE = None  # e.g., \"2025-12-30\"; None = auto-detect max date in DB\n",
        "WINDOW_DAYS = 5\n",
        "FLOW_AVG_DAYS = 5  # number of end-dates to average flow over\n",
        "TOP_K_WINNERS = 8\n",
        "TOP_K_LOSERS = 8\n",
        "MIN_EDGE_FLOW = 0.0\n",
        "DISTRIBUTION_MODE = \"demand_weighted\"  # \"equal\" or \"demand_weighted\"\n",
        "STRAND_UNIT = 0.5  # controls link strand splitting density\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b232f30",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def find_db_candidates():\n",
        "    root = Path('.').resolve()\n",
        "    search_dirs = [\n",
        "        root / 'data',\n",
        "        root / 'darkpool_analysis' / 'data',\n",
        "        root,\n",
        "    ]\n",
        "    patterns = ['*.duckdb', '*.db', '*.sqlite', '*.sqlite3']\n",
        "    candidates = []\n",
        "    for base in search_dirs:\n",
        "        if not base.exists():\n",
        "            continue\n",
        "        for pattern in patterns:\n",
        "            candidates.extend(list(base.rglob(pattern)))\n",
        "    # Unique + files only\n",
        "    unique = []\n",
        "    seen = set()\n",
        "    for p in candidates:\n",
        "        if not p.is_file():\n",
        "            continue\n",
        "        if p in seen:\n",
        "            continue\n",
        "        seen.add(p)\n",
        "        unique.append(p)\n",
        "    unique.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "    return unique\n",
        "\n",
        "def connect_db(db_path):\n",
        "    db_path = Path(db_path)\n",
        "    suffix = db_path.suffix.lower()\n",
        "    errors = []\n",
        "\n",
        "    def try_duckdb():\n",
        "        import duckdb\n",
        "        return duckdb.connect(database=str(db_path), read_only=True), 'duckdb'\n",
        "\n",
        "    def try_sqlite():\n",
        "        import sqlite3\n",
        "        uri = f\"file:{db_path.resolve().as_posix()}?mode=ro\"\n",
        "        return sqlite3.connect(uri, uri=True), 'sqlite'\n",
        "\n",
        "    if suffix == '.duckdb':\n",
        "        try:\n",
        "            return try_duckdb()\n",
        "        except Exception as e:\n",
        "            errors.append(f'duckdb: {e}')\n",
        "        try:\n",
        "            return try_sqlite()\n",
        "        except Exception as e:\n",
        "            errors.append(f'sqlite: {e}')\n",
        "    else:\n",
        "        try:\n",
        "            return try_sqlite()\n",
        "        except Exception as e:\n",
        "            errors.append(f'sqlite: {e}')\n",
        "        try:\n",
        "            return try_duckdb()\n",
        "        except Exception as e:\n",
        "            errors.append(f'duckdb: {e}')\n",
        "\n",
        "    raise RuntimeError('Unable to open database. ' + '; '.join(errors))\n",
        "\n",
        "def list_tables(conn, db_type):\n",
        "    if db_type == 'duckdb':\n",
        "        return [r[0] for r in conn.execute('SHOW TABLES').fetchall()]\n",
        "    rows = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()\n",
        "    return [r[0] for r in rows]\n",
        "\n",
        "def get_columns(conn, db_type, table):\n",
        "    pragma = f'PRAGMA table_info(\"{table}\")'\n",
        "    rows = conn.execute(pragma).fetchall()\n",
        "    # rows: (cid, name, type, notnull, dflt_value, pk)\n",
        "    return [r[1] for r in rows]\n",
        "\n",
        "def normalize_name(name):\n",
        "    return ''.join([c for c in name.lower() if c.isalnum()])\n",
        "\n",
        "def pick_column(columns, candidates):\n",
        "    norm_map = {normalize_name(c): c for c in columns}\n",
        "    for cand in candidates:\n",
        "        cand_norm = normalize_name(cand)\n",
        "        if cand_norm in norm_map:\n",
        "            return norm_map[cand_norm]\n",
        "    for cand in candidates:\n",
        "        cand_norm = normalize_name(cand)\n",
        "        for col_norm, original in norm_map.items():\n",
        "            if cand_norm in col_norm:\n",
        "                return original\n",
        "    return None\n",
        "\n",
        "def quote_ident(name):\n",
        "    safe = name.replace('\"', '\"\"')\n",
        "    return f'\"{safe}\"'\n",
        "\n",
        "TICKER_COL_CANDIDATES = ['ticker', 'symbol', 'sym']\n",
        "DATE_COL_CANDIDATES = ['date', 'trade_date', 'market_date', 'dt', 'day']\n",
        "ACCUM_COL_CANDIDATES = [\n",
        "    'accumulation_score', 'accum_score', 'accumulation', 'accum',\n",
        "    'accumulation_score_display', 'accum_score_display'\n",
        "]\n",
        "\n",
        "db_candidates = find_db_candidates()\n",
        "print('DB candidates (newest first):')\n",
        "for p in db_candidates:\n",
        "    print(' -', p)\n",
        "\n",
        "if not db_candidates:\n",
        "    print('No database files found under ./data or ./darkpool_analysis/data.')\n",
        "    raise SystemExit\n",
        "\n",
        "DB_PATH = db_candidates[0]\n",
        "print(f'Selected DB: {DB_PATH}')\n",
        "\n",
        "conn, DB_TYPE = connect_db(DB_PATH)\n",
        "print(f'Detected DB type: {DB_TYPE}')\n",
        "\n",
        "tables = list_tables(conn, DB_TYPE)\n",
        "print(f'Found {len(tables)} tables.')\n",
        "\n",
        "scan_rows = []\n",
        "for table in tables:\n",
        "    try:\n",
        "        cols = get_columns(conn, DB_TYPE, table)\n",
        "    except Exception as e:\n",
        "        scan_rows.append({\n",
        "            'table': table,\n",
        "            'ticker_col': None,\n",
        "            'date_col': None,\n",
        "            'accum_col': None,\n",
        "            'row_count': None,\n",
        "            'distinct_tickers': None,\n",
        "            'error': str(e),\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    ticker_col = pick_column(cols, TICKER_COL_CANDIDATES)\n",
        "    date_col = pick_column(cols, DATE_COL_CANDIDATES)\n",
        "    accum_col = pick_column(cols, ACCUM_COL_CANDIDATES)\n",
        "\n",
        "    row_count = None\n",
        "    distinct_tickers = None\n",
        "    if ticker_col and date_col and accum_col:\n",
        "        try:\n",
        "            row_count = conn.execute(\n",
        "                f'SELECT COUNT(*) FROM {quote_ident(table)}'\n",
        "            ).fetchone()[0]\n",
        "            distinct_tickers = conn.execute(\n",
        "                f'SELECT COUNT(DISTINCT {quote_ident(ticker_col)}) FROM {quote_ident(table)}'\n",
        "            ).fetchone()[0]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    scan_rows.append({\n",
        "        'table': table,\n",
        "        'ticker_col': ticker_col,\n",
        "        'date_col': date_col,\n",
        "        'accum_col': accum_col,\n",
        "        'row_count': row_count,\n",
        "        'distinct_tickers': distinct_tickers,\n",
        "        'error': None,\n",
        "    })\n",
        "\n",
        "print('Scanned tables (ticker/date/accum detection):')\n",
        "for row in scan_rows:\n",
        "    print(\n",
        "        f\" - {row['table']}: ticker={row['ticker_col']} date={row['date_col']} accum={row['accum_col']} rows={row['row_count']} distinct_tickers={row['distinct_tickers']}\"\n",
        "    )\n",
        "\n",
        "candidates = [r for r in scan_rows if r['ticker_col'] and r['date_col'] and r['accum_col']]\n",
        "if not candidates:\n",
        "    print('No tables matched the required ticker/date/accumulation schema.')\n",
        "    conn.close()\n",
        "    raise SystemExit\n",
        "\n",
        "multi = [r for r in candidates if (r['distinct_tickers'] or 0) > 1]\n",
        "pool = multi if multi else candidates\n",
        "pool.sort(key=lambda r: (r['row_count'] or 0, r['distinct_tickers'] or 0), reverse=True)\n",
        "best = pool[0]\n",
        "\n",
        "SELECT_TABLE = best['table']\n",
        "TICKER_COL = best['ticker_col']\n",
        "DATE_COL = best['date_col']\n",
        "ACCUM_COL = best['accum_col']\n",
        "\n",
        "print('Selected table:')\n",
        "print(f'  table={SELECT_TABLE}')\n",
        "print(f'  ticker_col={TICKER_COL}, date_col={DATE_COL}, accum_col={ACCUM_COL}')\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9409971c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_ticker_universe():\n",
        "    seen = set()\n",
        "    ordered = []\n",
        "    categories = {}\n",
        "    for t in GLOBAL_MACRO_TICKERS:\n",
        "        if t not in seen:\n",
        "            seen.add(t)\n",
        "            ordered.append(t)\n",
        "            categories[t] = 'GLOBAL_MACRO'\n",
        "    for t in SECTOR_CORE_TICKERS:\n",
        "        if t not in seen:\n",
        "            seen.add(t)\n",
        "            ordered.append(t)\n",
        "            categories[t] = 'SECTOR_CORE'\n",
        "    for t in COMMODITIES_TICKERS:\n",
        "        if t not in seen:\n",
        "            seen.add(t)\n",
        "            ordered.append(t)\n",
        "            categories[t] = 'COMMODITIES'\n",
        "    return ordered, categories\n",
        "\n",
        "ticker_order, ticker_category = build_ticker_universe()\n",
        "ticker_list = [t.upper() for t in ticker_order]\n",
        "\n",
        "conn, DB_TYPE = connect_db(DB_PATH)\n",
        "\n",
        "if DB_TYPE == 'duckdb':\n",
        "    date_expr = f\"CAST({quote_ident(DATE_COL)} AS DATE)\"\n",
        "    accum_expr = f\"TRY_CAST({quote_ident(ACCUM_COL)} AS DOUBLE)\"\n",
        "else:\n",
        "    date_expr = f\"DATE({quote_ident(DATE_COL)})\"\n",
        "    accum_expr = f\"CAST({quote_ident(ACCUM_COL)} AS REAL)\"\n",
        "\n",
        "placeholders = ','.join(['?'] * len(ticker_list))\n",
        "query = (\n",
        "    f\"SELECT UPPER({quote_ident(TICKER_COL)}) AS ticker, \"\n",
        "    f\"{date_expr} AS date, {accum_expr} AS accumulation_score \"\n",
        "    f\"FROM {quote_ident(SELECT_TABLE)} \"\n",
        "    f\"WHERE UPPER({quote_ident(TICKER_COL)}) IN ({placeholders})\"\n",
        ")\n",
        "\n",
        "try:\n",
        "    if DB_TYPE == 'duckdb':\n",
        "        df_raw = conn.execute(query, ticker_list).df()\n",
        "    else:\n",
        "        df_raw = pd.read_sql_query(query, conn, params=ticker_list)\n",
        "finally:\n",
        "    conn.close()\n",
        "\n",
        "if df_raw.empty:\n",
        "    print('No data returned for the specified tickers.')\n",
        "    raise SystemExit\n",
        "\n",
        "df_raw['date'] = pd.to_datetime(df_raw['date'], errors='coerce').dt.date\n",
        "df_raw['ticker'] = df_raw['ticker'].str.upper()\n",
        "df_raw['accumulation_score'] = pd.to_numeric(df_raw['accumulation_score'], errors='coerce')\n",
        "df_raw = df_raw.dropna(subset=['date', 'accumulation_score'])\n",
        "\n",
        "if df_raw.empty:\n",
        "    print('All accumulation score rows are null after parsing.')\n",
        "    raise SystemExit\n",
        "\n",
        "max_date = df_raw['date'].max()\n",
        "if END_DATE is None or str(END_DATE).strip() == '':\n",
        "    END_DATE_RESOLVED = max_date\n",
        "else:\n",
        "    END_DATE_RESOLVED = pd.to_datetime(END_DATE).date()\n",
        "    if END_DATE_RESOLVED > max_date:\n",
        "        print(f'END_DATE {END_DATE_RESOLVED} exceeds DB max date {max_date}; using max date.')\n",
        "        END_DATE_RESOLVED = max_date\n",
        "\n",
        "all_dates = sorted(df_raw['date'].unique())\n",
        "end_dates = [d for d in all_dates if d <= END_DATE_RESOLVED]\n",
        "flow_days = int(FLOW_AVG_DAYS) if FLOW_AVG_DAYS and int(FLOW_AVG_DAYS) > 0 else 1\n",
        "end_dates = end_dates[-flow_days:]\n",
        "if not end_dates:\n",
        "    print('No valid end dates available for flow averaging.')\n",
        "    raise SystemExit\n",
        "\n",
        "start_target = END_DATE_RESOLVED - timedelta(days=WINDOW_DAYS)\n",
        "\n",
        "def value_at_or_before(df_ticker, target_date):\n",
        "    df_leq = df_ticker[df_ticker['date'] <= target_date]\n",
        "    if df_leq.empty:\n",
        "        return None, None\n",
        "    row = df_leq.iloc[-1]\n",
        "    return row['date'], row['accumulation_score']\n",
        "\n",
        "def average_window(df_ticker, end_dates, window_days):\n",
        "    a_end_vals = []\n",
        "    a_start_vals = []\n",
        "    for end_dt in end_dates:\n",
        "        end_date, a_end = value_at_or_before(df_ticker, end_dt)\n",
        "        start_date, a_start = value_at_or_before(df_ticker, end_dt - timedelta(days=window_days))\n",
        "        if a_end is None or a_start is None:\n",
        "            continue\n",
        "        a_end_vals.append(a_end)\n",
        "        a_start_vals.append(a_start)\n",
        "    if not a_end_vals:\n",
        "        return None, None, 0\n",
        "    a_end_avg = float(np.mean(a_end_vals))\n",
        "    a_start_avg = float(np.mean(a_start_vals))\n",
        "    return a_end_avg, a_start_avg, len(a_end_vals)\n",
        "\n",
        "rows = []\n",
        "for ticker in ticker_order:\n",
        "    df_t = df_raw[df_raw['ticker'] == ticker].sort_values('date')\n",
        "    a_end_avg, a_start_avg, samples = average_window(df_t, end_dates, WINDOW_DAYS)\n",
        "    rows.append({\n",
        "        'ticker': ticker,\n",
        "        'category': ticker_category.get(ticker, 'UNKNOWN'),\n",
        "        'A_end': a_end_avg,\n",
        "        'A_start': a_start_avg,\n",
        "        'end_date': END_DATE_RESOLVED,\n",
        "        'start_date': start_target,\n",
        "        'avg_samples': samples,\n",
        "    })\n",
        "\n",
        "df_scores = pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f95f5b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_scores['delta'] = df_scores['A_end'] - df_scores['A_start']\n",
        "df_scores['role'] = np.where(\n",
        "    df_scores['delta'] > 0,\n",
        "    'winner',\n",
        "    np.where(df_scores['delta'] < 0, 'loser', 'neutral')\n",
        ")\n",
        "\n",
        "df_scores_sorted = df_scores.sort_values(\n",
        "    by='delta', key=lambda s: s.abs(), ascending=False\n",
        ").reset_index(drop=True)\n",
        "\n",
        "valid = df_scores.dropna(subset=['delta']).copy()\n",
        "winners = valid[valid['delta'] > 0].nlargest(TOP_K_WINNERS, 'delta')\n",
        "losers = valid[valid['delta'] < 0].copy()\n",
        "losers['supply'] = -losers['delta']\n",
        "losers = losers.nlargest(TOP_K_LOSERS, 'supply')\n",
        "\n",
        "edges = []\n",
        "total_demand = winners['delta'].sum() if not winners.empty else 0.0\n",
        "total_supply = losers['supply'].sum() if not losers.empty else 0.0\n",
        "\n",
        "if winners.empty or losers.empty:\n",
        "    print('No winners or losers found for the selected window. Adjust WINDOW_DAYS or TOP_K settings.')\n",
        "else:\n",
        "    if DISTRIBUTION_MODE not in {'equal', 'demand_weighted'}:\n",
        "        raise ValueError('DISTRIBUTION_MODE must be \"equal\" or \"demand_weighted\"')\n",
        "\n",
        "    if DISTRIBUTION_MODE == 'equal':\n",
        "        for _, loser in losers.iterrows():\n",
        "            flow_each = loser['supply'] / len(winners)\n",
        "            for _, winner in winners.iterrows():\n",
        "                if flow_each >= MIN_EDGE_FLOW:\n",
        "                    edges.append({\n",
        "                        'source': loser['ticker'],\n",
        "                        'dest': winner['ticker'],\n",
        "                        'flow': float(flow_each),\n",
        "                    })\n",
        "    else:\n",
        "        if total_demand <= 0:\n",
        "            print('Total demand is zero; cannot allocate demand-weighted flows.')\n",
        "        else:\n",
        "            for _, loser in losers.iterrows():\n",
        "                for _, winner in winners.iterrows():\n",
        "                    flow = loser['supply'] * (winner['delta'] / total_demand)\n",
        "                    if flow >= MIN_EDGE_FLOW:\n",
        "                        edges.append({\n",
        "                            'source': loser['ticker'],\n",
        "                            'dest': winner['ticker'],\n",
        "                            'flow': float(flow),\n",
        "                        })\n",
        "\n",
        "edges_df = pd.DataFrame(edges)\n",
        "if not edges_df.empty:\n",
        "    edges_df = edges_df.sort_values('flow', ascending=False).reset_index(drop=True)\n",
        "\n",
        "strand_rows = []\n",
        "unit = STRAND_UNIT if STRAND_UNIT and STRAND_UNIT > 0 else None\n",
        "for _, row in edges_df.iterrows():\n",
        "    flow = row['flow']\n",
        "    if unit:\n",
        "        n = max(1, int(np.floor(flow / unit)))\n",
        "    else:\n",
        "        n = 1\n",
        "    weight = flow / n\n",
        "    for _ in range(n):\n",
        "        strand_rows.append({\n",
        "            'source': row['source'],\n",
        "            'dest': row['dest'],\n",
        "            'weight': weight,\n",
        "        })\n",
        "\n",
        "strands_df = pd.DataFrame(strand_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f1ac9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df_scores_sorted[['ticker', 'category', 'A_end', 'A_start', 'delta', 'role']])\n",
        "\n",
        "if edges_df.empty:\n",
        "    display(pd.DataFrame(columns=['source', 'dest', 'flow']))\n",
        "else:\n",
        "    display(edges_df[['source', 'dest', 'flow']])\n",
        "\n",
        "summary = (\n",
        "    f'END_DATE={END_DATE_RESOLVED}, WINDOW_DAYS={WINDOW_DAYS} | '\n",
        "    f'total_supply={total_supply:.2f}, total_demand={total_demand:.2f} | '\n",
        "    f'winners={len(winners)}, losers={len(losers)}, edges={len(edges_df)}, strands={len(strands_df)}'\n",
        ")\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4741ff79",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "from matplotlib.colors import to_rgba\n",
        "\n",
        "CATEGORY_LABELS = {\n",
        "    'GLOBAL_MACRO': 'GLOBAL MACRO',\n",
        "    'SECTOR_CORE': 'SECTORS',\n",
        "    'COMMODITIES': 'COMMODITIES',\n",
        "}\n",
        "CATEGORY_PALETTE = {\n",
        "    'GLOBAL_MACRO': '#5CC8FF',\n",
        "    'SECTOR_CORE': '#F6C453',\n",
        "    'COMMODITIES': '#7CDE8A',\n",
        "    'UNKNOWN': '#A0A0A0',\n",
        "}\n",
        "FLOW_SOURCE_COLOR = '#B300FF'  # neon purple\n",
        "FLOW_DEST_COLOR = '#39FF14'    # neon green\n",
        "\n",
        "\n",
        "def add_gradient_curve(ax, points, color_start, color_end, lw, alpha):\n",
        "    if len(points) < 2:\n",
        "        return\n",
        "    segments = np.stack([points[:-1], points[1:]], axis=1)\n",
        "    c0 = np.array(to_rgba(color_start))\n",
        "    c1 = np.array(to_rgba(color_end))\n",
        "    t = np.linspace(0, 1, len(segments))[:, None]\n",
        "    colors = c0 * (1 - t) + c1 * t\n",
        "    colors[:, 3] = colors[:, 3] * alpha\n",
        "    lc = LineCollection(segments, colors=colors, linewidths=lw, capstyle='round')\n",
        "    ax.add_collection(lc)\n",
        "\n",
        "\n",
        "def plot_chord_matplotlib(strands_df, ticker_order, ticker_category):\n",
        "    if strands_df.empty:\n",
        "        print('No edges to plot.')\n",
        "        return\n",
        "\n",
        "    grouped = {\n",
        "        'GLOBAL_MACRO': [t for t in ticker_order if ticker_category.get(t) == 'GLOBAL_MACRO'],\n",
        "        'SECTOR_CORE': [t for t in ticker_order if ticker_category.get(t) == 'SECTOR_CORE'],\n",
        "        'COMMODITIES': [t for t in ticker_order if ticker_category.get(t) == 'COMMODITIES'],\n",
        "    }\n",
        "\n",
        "    total_nodes = sum(len(v) for v in grouped.values())\n",
        "    if total_nodes == 0:\n",
        "        print('No nodes to plot.')\n",
        "        return\n",
        "\n",
        "    gap = 0.12\n",
        "    total_gap = gap * len(grouped)\n",
        "    usable = 2 * math.pi - total_gap\n",
        "    if usable <= 0:\n",
        "        usable = 2 * math.pi\n",
        "    step = usable / total_nodes\n",
        "\n",
        "    angles = {}\n",
        "    angle = 0.0\n",
        "    for cat in ['GLOBAL_MACRO', 'SECTOR_CORE', 'COMMODITIES']:\n",
        "        angle += gap / 2\n",
        "        for t in grouped[cat]:\n",
        "            angles[t] = angle\n",
        "            angle += step\n",
        "        angle += gap / 2\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw={'aspect': 'equal'})\n",
        "    fig.patch.set_facecolor('#0b0f1a')\n",
        "    ax.set_facecolor('#0b0f1a')\n",
        "    ax.axis('off')\n",
        "\n",
        "    theta = np.linspace(0, 2 * math.pi, 400)\n",
        "    ax.plot(np.cos(theta), np.sin(theta), color='#39424e', lw=1.0, alpha=0.7)\n",
        "\n",
        "    max_w = strands_df['weight'].max() if not strands_df.empty else 1.0\n",
        "    for _, row in strands_df.iterrows():\n",
        "        src = row['source']\n",
        "        dst = row['dest']\n",
        "        w = row['weight']\n",
        "        if src not in angles or dst not in angles:\n",
        "            continue\n",
        "        a0 = angles[src]\n",
        "        a1 = angles[dst]\n",
        "        p0 = np.array([math.cos(a0), math.sin(a0)])\n",
        "        p2 = np.array([math.cos(a1), math.sin(a1)])\n",
        "        p1 = np.array([0.0, 0.0])\n",
        "        t = np.linspace(0, 1, 80)[:, None]\n",
        "        curve = (1 - t) ** 2 * p0 + 2 * (1 - t) * t * p1 + t ** 2 * p2\n",
        "        lw = 0.6 + 3.2 * ((w / max_w) ** 0.7)\n",
        "        add_gradient_curve(\n",
        "            ax,\n",
        "            curve,\n",
        "            FLOW_SOURCE_COLOR,\n",
        "            FLOW_DEST_COLOR,\n",
        "            lw=lw,\n",
        "            alpha=0.6,\n",
        "        )\n",
        "\n",
        "    for t, ang in angles.items():\n",
        "        x = math.cos(ang)\n",
        "        y = math.sin(ang)\n",
        "        r = 1.10\n",
        "        rot = math.degrees(ang)\n",
        "        if math.pi / 2 < ang < 3 * math.pi / 2:\n",
        "            rot += 180\n",
        "        ax.text(\n",
        "            r * x, r * y, t,\n",
        "            color='#E6EDF7', fontsize=9,\n",
        "            ha='center', va='center',\n",
        "            rotation=rot, rotation_mode='anchor'\n",
        "        )\n",
        "\n",
        "    for cat, tickers in grouped.items():\n",
        "        if not tickers:\n",
        "            continue\n",
        "        mid_angle = np.mean([angles[t] for t in tickers])\n",
        "        ax.text(\n",
        "            1.28 * math.cos(mid_angle),\n",
        "            1.28 * math.sin(mid_angle),\n",
        "            CATEGORY_LABELS.get(cat, cat),\n",
        "            color=CATEGORY_PALETTE.get(cat, '#A0A0A0'),\n",
        "            fontsize=12, fontweight='bold',\n",
        "            ha='center', va='center'\n",
        "        )\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_chord_with_fallback(strands_df, ticker_order, ticker_category):\n",
        "    plot_chord_matplotlib(strands_df, ticker_order, ticker_category)\n",
        "\n",
        "\n",
        "plot_chord_with_fallback(strands_df, ticker_order, ticker_category)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
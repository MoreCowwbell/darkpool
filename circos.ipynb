{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2eeaa72d",
      "metadata": {},
      "source": [
        "# Cell 1\n",
        "# Circos-Style Money Flow Chord Diagram\n",
        "\n",
        "This notebook builds a self-contained Circos-style chord diagram from Accumulation Score time series in a local database.\n",
        "\n",
        "Constraints honored:\n",
        "- No project modules are imported or modified.\n",
        "- Database access is read-only (SELECT-only).\n",
        "- No files are written or mutated; all outputs are in-notebook only.\n",
        "\n",
        "Quick usage:\n",
        "- FLOW_PERIOD_DAYS = 1  # \"today's move\"\n",
        "- FLOW_PERIOD_DAYS = 5  # \"past 5 trading days\"\n",
        "\n",
        "Layer toggles:\n",
        "- SHOW_ACCUM_FLOW / SHOW_LIT_FLOW / SHOW_SHORT_FLOW / SHOW_VOLUME_RING\n",
        "\n",
        "Group toggles:\n",
        "- ENABLED_GROUPS[\"MAG8\"] = False to hide a group (recommended).\n",
        "- You may comment out a group list, but then also set its ENABLED_GROUPS entry to False.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c759775f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2\n",
        "# Ticker groups (hardcoded)\n",
        "SECTOR_CORE_TICKERS = [\n",
        "    \"XLF\",\"KRE\",\"XLK\",\"SMH\",\"XLI\",\"XLY\",\"XLE\",\"XLV\",\"XLP\",\"XLU\"\n",
        "]\n",
        "\n",
        "SECTOR_SUMMARY_TICKERS = [\n",
        "    \"XLE\", \"XLF\", \"XLK\", \"XLY\", \"XLP\", \"XLV\", \"XLU\", \"XLI\", \"XLC\", \"XLB\", \"XLRE\", \"SPY\",\n",
        "]\n",
        "\n",
        "GLOBAL_MACRO_TICKERS = [\n",
        "    \"SPY\",\"QQQ\",\"TQQQ\",\"IWM\",\"VGK\",\"EWJ\",\"EFA\",\"EEM\",\"FXI\",\"UUP\",\"TLT\",\"GLD\",\"USO\",\"VIXY\"\n",
        "]\n",
        "\n",
        "COMMODITIES_TICKERS = [\n",
        "    \"GLD\",\"SLV\",\"GDX\",\"USO\",\"UNG\",\"URA\"\n",
        "]\n",
        "\n",
        "MAG8_TICKERS = [\n",
        "    \"AAPL\",\"MSFT\",\"NVDA\",\"AMZN\",\"GOOGL\",\"META\",\"TSLA\",\"AVGO\"\n",
        "]\n",
        "\n",
        "ENABLED_GROUPS = {\n",
        "    \"GLOBAL_MACRO\": False,\n",
        "    \"MAG8\": False,\n",
        "    \"SECTOR_SUMMARY\": False,\n",
        "    \"SECTOR_CORE\": True,\n",
        "    \"COMMODITIES\": False,\n",
        "}\n",
        "\n",
        "# Controls\n",
        "END_DATE = \"2025-12-30\"  # e.g., \"2025-12-30\"; None = auto-detect max date in DB\n",
        "FLOW_PERIOD_DAYS = 3  # number of trading days to measure flow over\n",
        "TOP_K_WINNERS = 8\n",
        "TOP_K_LOSERS = 8\n",
        "MIN_EDGE_FLOW = 0.0\n",
        "DISTRIBUTION_MODE = \"demand_weighted\"  # \"equal\" or \"demand_weighted\"\n",
        "STRAND_UNIT = 0.5  # controls link strand splitting density\n",
        "\n",
        "# Layer toggles\n",
        "SHOW_ACCUM_FLOW = True\n",
        "SHOW_LIT_FLOW = True\n",
        "SHOW_SHORT_FLOW = True\n",
        "SHOW_VOLUME_RING = True\n",
        "SHOW_TICKER_ARC = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b232f30",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def find_db_candidates():\n",
        "    root = Path('.').resolve()\n",
        "    search_dirs = [\n",
        "        root / 'data',\n",
        "        root / 'darkpool_analysis' / 'data',\n",
        "        root,\n",
        "    ]\n",
        "    patterns = ['*.duckdb', '*.db', '*.sqlite', '*.sqlite3']\n",
        "    candidates = []\n",
        "    for base in search_dirs:\n",
        "        if not base.exists():\n",
        "            continue\n",
        "        for pattern in patterns:\n",
        "            candidates.extend(list(base.rglob(pattern)))\n",
        "    # Unique + files only\n",
        "    unique = []\n",
        "    seen = set()\n",
        "    for p in candidates:\n",
        "        if not p.is_file():\n",
        "            continue\n",
        "        if p in seen:\n",
        "            continue\n",
        "        seen.add(p)\n",
        "        unique.append(p)\n",
        "    unique.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "    return unique\n",
        "\n",
        "def connect_db(db_path):\n",
        "    db_path = Path(db_path)\n",
        "    suffix = db_path.suffix.lower()\n",
        "    errors = []\n",
        "\n",
        "    def try_duckdb():\n",
        "        import duckdb\n",
        "        return duckdb.connect(database=str(db_path), read_only=True), 'duckdb'\n",
        "\n",
        "    def try_sqlite():\n",
        "        import sqlite3\n",
        "        uri = f\"file:{db_path.resolve().as_posix()}?mode=ro\"\n",
        "        return sqlite3.connect(uri, uri=True), 'sqlite'\n",
        "\n",
        "    if suffix == '.duckdb':\n",
        "        try:\n",
        "            return try_duckdb()\n",
        "        except Exception as e:\n",
        "            errors.append(f'duckdb: {e}')\n",
        "        try:\n",
        "            return try_sqlite()\n",
        "        except Exception as e:\n",
        "            errors.append(f'sqlite: {e}')\n",
        "    else:\n",
        "        try:\n",
        "            return try_sqlite()\n",
        "        except Exception as e:\n",
        "            errors.append(f'sqlite: {e}')\n",
        "        try:\n",
        "            return try_duckdb()\n",
        "        except Exception as e:\n",
        "            errors.append(f'duckdb: {e}')\n",
        "\n",
        "    raise RuntimeError('Unable to open database. ' + '; '.join(errors))\n",
        "\n",
        "def list_tables(conn, db_type):\n",
        "    if db_type == 'duckdb':\n",
        "        return [r[0] for r in conn.execute('SHOW TABLES').fetchall()]\n",
        "    rows = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()\n",
        "    return [r[0] for r in rows]\n",
        "\n",
        "def get_columns(conn, db_type, table):\n",
        "    pragma = f'PRAGMA table_info(\"{table}\")'\n",
        "    rows = conn.execute(pragma).fetchall()\n",
        "    # rows: (cid, name, type, notnull, dflt_value, pk)\n",
        "    return [r[1] for r in rows]\n",
        "\n",
        "def normalize_name(name):\n",
        "    return ''.join([c for c in name.lower() if c.isalnum()])\n",
        "\n",
        "def pick_column(columns, candidates):\n",
        "    norm_map = {normalize_name(c): c for c in columns}\n",
        "    for cand in candidates:\n",
        "        cand_norm = normalize_name(cand)\n",
        "        if cand_norm in norm_map:\n",
        "            return norm_map[cand_norm]\n",
        "    for cand in candidates:\n",
        "        cand_norm = normalize_name(cand)\n",
        "        for col_norm, original in norm_map.items():\n",
        "            if cand_norm in col_norm:\n",
        "                return original\n",
        "    return None\n",
        "\n",
        "def quote_ident(name):\n",
        "    safe = name.replace('\"', '\"\"')\n",
        "    return f'\"{safe}\"'\n",
        "\n",
        "TICKER_COL_CANDIDATES = ['ticker', 'symbol', 'sym']\n",
        "DATE_COL_CANDIDATES = ['date', 'trade_date', 'market_date', 'dt', 'day']\n",
        "ACCUM_COL_CANDIDATES = [\n",
        "    'accumulation_score', 'accum_score', 'accumulation', 'accum',\n",
        "    'accumulation_score_display', 'accum_score_display'\n",
        "]\n",
        "\n",
        "db_candidates = find_db_candidates()\n",
        "print('DB candidates (newest first):')\n",
        "for p in db_candidates:\n",
        "    print(' -', p)\n",
        "\n",
        "if not db_candidates:\n",
        "    print('No database files found under ./data or ./darkpool_analysis/data.')\n",
        "    raise SystemExit\n",
        "\n",
        "DB_PATH = db_candidates[0]\n",
        "print(f'Selected DB: {DB_PATH}')\n",
        "\n",
        "conn, DB_TYPE = connect_db(DB_PATH)\n",
        "print(f'Detected DB type: {DB_TYPE}')\n",
        "\n",
        "tables = list_tables(conn, DB_TYPE)\n",
        "print(f'Found {len(tables)} tables.')\n",
        "\n",
        "scan_rows = []\n",
        "for table in tables:\n",
        "    try:\n",
        "        cols = get_columns(conn, DB_TYPE, table)\n",
        "    except Exception as e:\n",
        "        scan_rows.append({\n",
        "            'table': table,\n",
        "            'ticker_col': None,\n",
        "            'date_col': None,\n",
        "            'accum_col': None,\n",
        "            'row_count': None,\n",
        "            'distinct_tickers': None,\n",
        "            'error': str(e),\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    ticker_col = pick_column(cols, TICKER_COL_CANDIDATES)\n",
        "    date_col = pick_column(cols, DATE_COL_CANDIDATES)\n",
        "    accum_col = pick_column(cols, ACCUM_COL_CANDIDATES)\n",
        "\n",
        "    row_count = None\n",
        "    distinct_tickers = None\n",
        "    if ticker_col and date_col and accum_col:\n",
        "        try:\n",
        "            row_count = conn.execute(\n",
        "                f'SELECT COUNT(*) FROM {quote_ident(table)}'\n",
        "            ).fetchone()[0]\n",
        "            distinct_tickers = conn.execute(\n",
        "                f'SELECT COUNT(DISTINCT {quote_ident(ticker_col)}) FROM {quote_ident(table)}'\n",
        "            ).fetchone()[0]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    scan_rows.append({\n",
        "        'table': table,\n",
        "        'ticker_col': ticker_col,\n",
        "        'date_col': date_col,\n",
        "        'accum_col': accum_col,\n",
        "        'row_count': row_count,\n",
        "        'distinct_tickers': distinct_tickers,\n",
        "        'error': None,\n",
        "    })\n",
        "\n",
        "print('Scanned tables (ticker/date/accum detection):')\n",
        "for row in scan_rows:\n",
        "    print(\n",
        "        f\" - {row['table']}: ticker={row['ticker_col']} date={row['date_col']} accum={row['accum_col']} rows={row['row_count']} distinct_tickers={row['distinct_tickers']}\"\n",
        "    )\n",
        "\n",
        "candidates = [r for r in scan_rows if r['ticker_col'] and r['date_col'] and r['accum_col']]\n",
        "if not candidates:\n",
        "    print('No tables matched the required ticker/date/accumulation schema.')\n",
        "    conn.close()\n",
        "    raise SystemExit\n",
        "\n",
        "multi = [r for r in candidates if (r['distinct_tickers'] or 0) > 1]\n",
        "pool = multi if multi else candidates\n",
        "pool.sort(key=lambda r: (r['row_count'] or 0, r['distinct_tickers'] or 0), reverse=True)\n",
        "best = pool[0]\n",
        "\n",
        "SELECT_TABLE = best['table']\n",
        "TICKER_COL = best['ticker_col']\n",
        "DATE_COL = best['date_col']\n",
        "ACCUM_COL = best['accum_col']\n",
        "\n",
        "print('Selected table:')\n",
        "print(f'  table={SELECT_TABLE}')\n",
        "print(f'  ticker_col={TICKER_COL}, date_col={DATE_COL}, accum_col={ACCUM_COL}')\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9409971c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4\n",
        "def build_ticker_universe():\n",
        "    seen = set()\n",
        "    ordered = []\n",
        "    categories = {}\n",
        "    enabled = globals().get(\"ENABLED_GROUPS\", {})\n",
        "    group_defs = [\n",
        "        (\"GLOBAL_MACRO\", globals().get(\"GLOBAL_MACRO_TICKERS\", [])),\n",
        "        (\"MAG8\", globals().get(\"MAG8_TICKERS\", [])),\n",
        "        (\"SECTOR_SUMMARY\", globals().get(\"SECTOR_SUMMARY_TICKERS\", [])),\n",
        "        (\"SECTOR_CORE\", globals().get(\"SECTOR_CORE_TICKERS\", [])),\n",
        "        (\"COMMODITIES\", globals().get(\"COMMODITIES_TICKERS\", [])),\n",
        "    ]\n",
        "    for group_name, tickers in group_defs:\n",
        "        if not enabled.get(group_name, True):\n",
        "            continue\n",
        "        for t in tickers:\n",
        "            if t not in seen:\n",
        "                seen.add(t)\n",
        "                ordered.append(t)\n",
        "                categories[t] = group_name\n",
        "    return ordered, categories\n",
        "\n",
        "\n",
        "ticker_order, ticker_category = build_ticker_universe()\n",
        "print(\"Ticker universe size:\", len(ticker_order))\n",
        "print(\"Enabled groups:\", {k: v for k, v in ENABLED_GROUPS.items()})\n",
        "ticker_list = [t.upper() for t in ticker_order]\n",
        "import re\n",
        "\n",
        "conn, DB_TYPE = connect_db(DB_PATH)\n",
        "\n",
        "if DB_TYPE == 'duckdb':\n",
        "    date_expr = f\"CAST({quote_ident(DATE_COL)} AS DATE)\"\n",
        "    accum_expr = f\"{quote_ident(ACCUM_COL)}\"\n",
        "else:\n",
        "    date_expr = f\"DATE({quote_ident(DATE_COL)})\"\n",
        "    accum_expr = f\"{quote_ident(ACCUM_COL)}\"\n",
        "\n",
        "placeholders = ','.join(['?'] * len(ticker_list))\n",
        "query = (\n",
        "    f\"SELECT UPPER({quote_ident(TICKER_COL)}) AS ticker, \"\n",
        "    f\"{date_expr} AS date, {accum_expr} AS accumulation_score \"\n",
        "    f\"FROM {quote_ident(SELECT_TABLE)} \"\n",
        "    f\"WHERE UPPER({quote_ident(TICKER_COL)}) IN ({placeholders})\"\n",
        ")\n",
        "\n",
        "print(f\"Using accumulation table: {SELECT_TABLE} (ticker={TICKER_COL}, date={DATE_COL}, accum={ACCUM_COL})\")\n",
        "\n",
        "try:\n",
        "    if DB_TYPE == 'duckdb':\n",
        "        df_raw = conn.execute(query, ticker_list).df()\n",
        "    else:\n",
        "        df_raw = pd.read_sql_query(query, conn, params=ticker_list)\n",
        "finally:\n",
        "    conn.close()\n",
        "\n",
        "if df_raw.empty:\n",
        "    print('No data returned for the specified tickers.')\n",
        "    raise SystemExit\n",
        "\n",
        "print(\"Accumulation rows loaded:\", len(df_raw))\n",
        "print(\"Accumulation date range:\", df_raw[\"date\"].min(), \"->\", df_raw[\"date\"].max())\n",
        "print(\"Accumulation raw sample:\", df_raw[\"accumulation_score\"].head(5).tolist())\n",
        "df_raw['date'] = pd.to_datetime(df_raw['date'], errors='coerce').dt.date\n",
        "df_raw['ticker'] = df_raw['ticker'].str.upper()\n",
        "\n",
        "def fetch_accum_column(accum_col_name):\n",
        "    conn, db_type = connect_db(DB_PATH)\n",
        "    try:\n",
        "        if db_type == 'duckdb':\n",
        "            date_expr = f\"CAST({quote_ident(DATE_COL)} AS DATE)\"\n",
        "        else:\n",
        "            date_expr = f\"DATE({quote_ident(DATE_COL)})\"\n",
        "        query = (\n",
        "            f\"SELECT UPPER({quote_ident(TICKER_COL)}) AS ticker, \"\n",
        "            f\"{date_expr} AS date, {quote_ident(accum_col_name)} AS accumulation_score \"\n",
        "            f\"FROM {quote_ident(SELECT_TABLE)} \"\n",
        "            f\"WHERE UPPER({quote_ident(TICKER_COL)}) IN ({placeholders})\"\n",
        "        )\n",
        "        if db_type == 'duckdb':\n",
        "            return conn.execute(query, ticker_list).df()\n",
        "        return pd.read_sql_query(query, conn, params=ticker_list)\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "def normalize_accum_df(df):\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.date\n",
        "    df['ticker'] = df['ticker'].str.upper()\n",
        "    df['accumulation_score'] = df['accumulation_score'].apply(parse_accum)\n",
        "    return df.dropna(subset=['date', 'accumulation_score'])\n",
        "\n",
        "def parse_accum(value):\n",
        "    if value is None:\n",
        "        return np.nan\n",
        "    if isinstance(value, (int, float, np.number)):\n",
        "        if np.isnan(value):\n",
        "            return np.nan\n",
        "        return float(value)\n",
        "    s = str(value).strip()\n",
        "    if not s:\n",
        "        return np.nan\n",
        "    s = s.replace('%', '').replace(',', '')\n",
        "    m = re.search(r'-?\\d+(?:\\.\\d+)?', s)\n",
        "    return float(m.group(0)) if m else np.nan\n",
        "\n",
        "df_raw = normalize_accum_df(df_raw)\n",
        "ACCUM_COL_SELECTED = ACCUM_COL\n",
        "if df_raw.empty:\n",
        "    print('Primary accumulation column returned no usable values; trying fallbacks...')\n",
        "    conn, db_type = connect_db(DB_PATH)\n",
        "    try:\n",
        "        cols = get_columns(conn, db_type, SELECT_TABLE)\n",
        "    finally:\n",
        "        conn.close()\n",
        "    candidates = []\n",
        "    for cand in [\n",
        "        'accumulation_score_display', 'accum_score_display',\n",
        "        'accumulation_score', 'accum_score', 'accumulation', 'accum'\n",
        "    ]:\n",
        "        col = pick_column(cols, [cand])\n",
        "        if col and col not in candidates:\n",
        "            candidates.append(col)\n",
        "    candidates = [c for c in candidates if c != ACCUM_COL]\n",
        "    print('Accumulation fallback candidates:', candidates)\n",
        "    for col in candidates:\n",
        "        print('Trying accumulation column:', col)\n",
        "        df_try = fetch_accum_column(col)\n",
        "        print('Fallback rows loaded:', len(df_try))\n",
        "        print('Fallback raw sample:', df_try['accumulation_score'].head(5).tolist())\n",
        "        df_try = normalize_accum_df(df_try)\n",
        "        if not df_try.empty:\n",
        "            ACCUM_COL_SELECTED = col\n",
        "            df_raw = df_try\n",
        "            break\n",
        "\n",
        "missing = sorted(set(ticker_list) - set(df_raw[\"ticker\"].unique()))\n",
        "if missing:\n",
        "    print(\"Missing tickers in accumulation data:\", missing)\n",
        "\n",
        "if df_raw.empty:\n",
        "    print('All accumulation score rows are null after parsing.')\n",
        "    raise SystemExit\n",
        "print('Using accumulation column:', ACCUM_COL_SELECTED)\n",
        "\n",
        "max_date = df_raw['date'].max()\n",
        "if END_DATE is None or str(END_DATE).strip() == '':\n",
        "    END_DATE_RESOLVED = max_date\n",
        "else:\n",
        "    END_DATE_RESOLVED = pd.to_datetime(END_DATE).date()\n",
        "    if END_DATE_RESOLVED > max_date:\n",
        "        print(f'END_DATE {END_DATE_RESOLVED} exceeds DB max date {max_date}; using max date.')\n",
        "        END_DATE_RESOLVED = max_date\n",
        "\n",
        "flow_days = int(FLOW_PERIOD_DAYS) if FLOW_PERIOD_DAYS and int(FLOW_PERIOD_DAYS) > 0 else 1\n",
        "print(\"Flow period days:\", flow_days, \"END_DATE:\", END_DATE_RESOLVED)\n",
        "\n",
        "\n",
        "def tail_n(df_ticker, n, end_date):\n",
        "    df = df_ticker[df_ticker['date'] <= end_date].sort_values('date')\n",
        "    if df.empty:\n",
        "        return df\n",
        "    return df.tail(n)\n",
        "\n",
        "\n",
        "rows = []\n",
        "for ticker in ticker_order:\n",
        "    df_t = df_raw[df_raw['ticker'] == ticker]\n",
        "    tail = tail_n(df_t, flow_days, END_DATE_RESOLVED)\n",
        "    if len(tail) < 2:\n",
        "        a_start = None\n",
        "        a_end = None\n",
        "    else:\n",
        "        a_start = float(tail['accumulation_score'].iloc[0])\n",
        "        a_end = float(tail['accumulation_score'].iloc[-1])\n",
        "    rows.append({\n",
        "        'ticker': ticker,\n",
        "        'category': ticker_category.get(ticker, 'UNKNOWN'),\n",
        "        'A_end': a_end,\n",
        "        'A_start': a_start,\n",
        "        'end_date': END_DATE_RESOLVED,\n",
        "        'start_date': tail['date'].iloc[0] if len(tail) else None,\n",
        "        'samples': len(tail),\n",
        "    })\n",
        "\n",
        "df_scores = pd.DataFrame(rows)\n",
        "\n",
        "# --- Volume data discovery (lit/short buy/sell) ---\n",
        "LIT_BUY_CANDIDATES = ['lit_buy_volume', 'lit_buy_vol', 'lit_buy']\n",
        "LIT_SELL_CANDIDATES = ['lit_sell_volume', 'lit_sell_vol', 'lit_sell']\n",
        "SHORT_BUY_CANDIDATES = ['short_buy_volume', 'short_buy_vol', 'short_buy']\n",
        "SHORT_SELL_CANDIDATES = ['short_sell_volume', 'short_sell_vol', 'short_sell']\n",
        "\n",
        "\n",
        "def find_volume_table(conn, db_type):\n",
        "    tables = list_tables(conn, db_type)\n",
        "    candidates = []\n",
        "    for table in tables:\n",
        "        try:\n",
        "            cols = get_columns(conn, db_type, table)\n",
        "        except Exception:\n",
        "            continue\n",
        "        ticker_col = pick_column(cols, TICKER_COL_CANDIDATES)\n",
        "        date_col = pick_column(cols, DATE_COL_CANDIDATES)\n",
        "        lit_buy_col = pick_column(cols, LIT_BUY_CANDIDATES)\n",
        "        lit_sell_col = pick_column(cols, LIT_SELL_CANDIDATES)\n",
        "        short_buy_col = pick_column(cols, SHORT_BUY_CANDIDATES)\n",
        "        short_sell_col = pick_column(cols, SHORT_SELL_CANDIDATES)\n",
        "        if not (ticker_col and date_col and lit_buy_col and lit_sell_col and short_buy_col and short_sell_col):\n",
        "            continue\n",
        "        row_count = None\n",
        "        distinct_tickers = None\n",
        "        try:\n",
        "            row_count = conn.execute(\n",
        "                f'SELECT COUNT(*) FROM {quote_ident(table)}'\n",
        "            ).fetchone()[0]\n",
        "            distinct_tickers = conn.execute(\n",
        "                f'SELECT COUNT(DISTINCT {quote_ident(ticker_col)}) FROM {quote_ident(table)}'\n",
        "            ).fetchone()[0]\n",
        "        except Exception:\n",
        "            pass\n",
        "        candidates.append({\n",
        "            'table': table,\n",
        "            'ticker_col': ticker_col,\n",
        "            'date_col': date_col,\n",
        "            'lit_buy_col': lit_buy_col,\n",
        "            'lit_sell_col': lit_sell_col,\n",
        "            'short_buy_col': short_buy_col,\n",
        "            'short_sell_col': short_sell_col,\n",
        "            'row_count': row_count,\n",
        "            'distinct_tickers': distinct_tickers,\n",
        "        })\n",
        "    if not candidates:\n",
        "        return None\n",
        "    candidates.sort(key=lambda r: (r['row_count'] or 0, r['distinct_tickers'] or 0), reverse=True)\n",
        "    return candidates[0]\n",
        "\n",
        "\n",
        "conn, DB_TYPE = connect_db(DB_PATH)\n",
        "try:\n",
        "    volume_info = find_volume_table(conn, DB_TYPE)\n",
        "finally:\n",
        "    conn.close()\n",
        "\n",
        "VOLUME_DATA_AVAILABLE = volume_info is not None\n",
        "if VOLUME_DATA_AVAILABLE:\n",
        "    print(\"Volume table selected:\", volume_info[\"table\"])\n",
        "    print(\"Volume columns:\", {k: volume_info[k] for k in [\"ticker_col\", \"date_col\", \"lit_buy_col\", \"lit_sell_col\", \"short_buy_col\", \"short_sell_col\"]})\n",
        "else:\n",
        "    print(\"No volume table found with lit/short buy/sell columns.\")\n",
        "\n",
        "if VOLUME_DATA_AVAILABLE:\n",
        "    conn, DB_TYPE = connect_db(DB_PATH)\n",
        "    try:\n",
        "        if DB_TYPE == 'duckdb':\n",
        "            date_expr = f\"CAST({quote_ident(volume_info['date_col'])} AS DATE)\"\n",
        "            num_cast = \"TRY_CAST\"\n",
        "        else:\n",
        "            date_expr = f\"DATE({quote_ident(volume_info['date_col'])})\"\n",
        "            num_cast = \"CAST\"\n",
        "\n",
        "        query = (\n",
        "            f\"SELECT UPPER({quote_ident(volume_info['ticker_col'])}) AS ticker, \"\n",
        "            f\"{date_expr} AS date, \"\n",
        "            f\"{num_cast}({quote_ident(volume_info['lit_buy_col'])} AS DOUBLE) AS lit_buy, \"\n",
        "            f\"{num_cast}({quote_ident(volume_info['lit_sell_col'])} AS DOUBLE) AS lit_sell, \"\n",
        "            f\"{num_cast}({quote_ident(volume_info['short_buy_col'])} AS DOUBLE) AS short_buy, \"\n",
        "            f\"{num_cast}({quote_ident(volume_info['short_sell_col'])} AS DOUBLE) AS short_sell \"\n",
        "            f\"FROM {quote_ident(volume_info['table'])} \"\n",
        "            f\"WHERE UPPER({quote_ident(volume_info['ticker_col'])}) IN ({placeholders})\"\n",
        "        )\n",
        "\n",
        "        if DB_TYPE == 'duckdb':\n",
        "            df_vol_raw = conn.execute(query, ticker_list).df()\n",
        "        else:\n",
        "            df_vol_raw = pd.read_sql_query(query, conn, params=ticker_list)\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "    df_vol_raw['date'] = pd.to_datetime(df_vol_raw['date'], errors='coerce').dt.date\n",
        "    print('Volume rows loaded:', len(df_vol_raw))\n",
        "    print('Volume date range:', df_vol_raw['date'].min(), '->', df_vol_raw['date'].max())\n",
        "    df_vol_raw['ticker'] = df_vol_raw['ticker'].str.upper()\n",
        "    for col in ['lit_buy', 'lit_sell', 'short_buy', 'short_sell']:\n",
        "        df_vol_raw[col] = pd.to_numeric(df_vol_raw[col], errors='coerce')\n",
        "    df_vol_raw = df_vol_raw.dropna(subset=['date'])\n",
        "\n",
        "    vol_rows = []\n",
        "    for ticker in ticker_order:\n",
        "        df_t = df_vol_raw[df_vol_raw['ticker'] == ticker]\n",
        "        tail = tail_n(df_t, flow_days, END_DATE_RESOLVED)\n",
        "        if tail.empty:\n",
        "            vol_rows.append({\n",
        "                'ticker': ticker,\n",
        "                'lit_buy_sum': None,\n",
        "                'lit_sell_sum': None,\n",
        "                'short_buy_sum': None,\n",
        "                'short_sell_sum': None,\n",
        "                'volume_samples': 0,\n",
        "            })\n",
        "            continue\n",
        "        vol_rows.append({\n",
        "            'ticker': ticker,\n",
        "            'lit_buy_sum': float(tail['lit_buy'].sum(skipna=True)),\n",
        "            'lit_sell_sum': float(tail['lit_sell'].sum(skipna=True)),\n",
        "            'short_buy_sum': float(tail['short_buy'].sum(skipna=True)),\n",
        "            'short_sell_sum': float(tail['short_sell'].sum(skipna=True)),\n",
        "            'volume_samples': len(tail),\n",
        "        })\n",
        "\n",
        "    df_volume = pd.DataFrame(vol_rows)\n",
        "    total_samples = int(df_volume['volume_samples'].sum()) if not df_volume.empty else 0\n",
        "    lit_total = float(df_volume['lit_buy_sum'].fillna(0).sum() + df_volume['lit_sell_sum'].fillna(0).sum()) if not df_volume.empty else 0.0\n",
        "    short_total = float(df_volume['short_buy_sum'].fillna(0).sum() + df_volume['short_sell_sum'].fillna(0).sum()) if not df_volume.empty else 0.0\n",
        "    if total_samples == 0 or (lit_total == 0 and short_total == 0):\n",
        "        print('Volume data missing for selected period. Lit/short chords require non-zero buy/sell volume data.')\n",
        "        raise SystemExit\n",
        "else:\n",
        "    df_volume = pd.DataFrame(columns=[\n",
        "        'ticker', 'lit_buy_sum', 'lit_sell_sum', 'short_buy_sum', 'short_sell_sum', 'volume_samples'\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f95f5b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5\n",
        "def build_edges_from_value(df, value_col, top_k_winners, top_k_losers):\n",
        "    df = df[['ticker', value_col]].dropna().copy()\n",
        "    winners = df[df[value_col] > 0].nlargest(top_k_winners, value_col)\n",
        "    losers = df[df[value_col] < 0].copy()\n",
        "    losers['supply'] = -losers[value_col]\n",
        "    losers = losers.nlargest(top_k_losers, 'supply')\n",
        "\n",
        "    edges = []\n",
        "    total_demand = winners[value_col].sum() if not winners.empty else 0.0\n",
        "    total_supply = losers['supply'].sum() if not losers.empty else 0.0\n",
        "\n",
        "    if winners.empty or losers.empty:\n",
        "        return pd.DataFrame(edges), winners, losers, total_demand, total_supply\n",
        "\n",
        "    if DISTRIBUTION_MODE not in {'equal', 'demand_weighted'}:\n",
        "        raise ValueError('DISTRIBUTION_MODE must be \"equal\" or \"demand_weighted\"')\n",
        "\n",
        "    if DISTRIBUTION_MODE == 'equal':\n",
        "        for _, loser in losers.iterrows():\n",
        "            flow_each = loser['supply'] / len(winners)\n",
        "            for _, winner in winners.iterrows():\n",
        "                if flow_each >= MIN_EDGE_FLOW:\n",
        "                    edges.append({\n",
        "                        'source': loser['ticker'],\n",
        "                        'dest': winner['ticker'],\n",
        "                        'flow': float(flow_each),\n",
        "                    })\n",
        "    else:\n",
        "        if total_demand > 0:\n",
        "            for _, loser in losers.iterrows():\n",
        "                for _, winner in winners.iterrows():\n",
        "                    flow = loser['supply'] * (winner[value_col] / total_demand)\n",
        "                    if flow >= MIN_EDGE_FLOW:\n",
        "                        edges.append({\n",
        "                            'source': loser['ticker'],\n",
        "                            'dest': winner['ticker'],\n",
        "                            'flow': float(flow),\n",
        "                        })\n",
        "\n",
        "    edges_df = pd.DataFrame(edges)\n",
        "    if not edges_df.empty:\n",
        "        edges_df = edges_df.sort_values('flow', ascending=False).reset_index(drop=True)\n",
        "    return edges_df, winners, losers, total_demand, total_supply\n",
        "\n",
        "\n",
        "def split_strands(edges_df):\n",
        "    strand_rows = []\n",
        "    unit = STRAND_UNIT if STRAND_UNIT and STRAND_UNIT > 0 else None\n",
        "    for _, row in edges_df.iterrows():\n",
        "        flow = row['flow']\n",
        "        if unit:\n",
        "            n = max(1, int(np.floor(flow / unit)))\n",
        "        else:\n",
        "            n = 1\n",
        "        weight = flow / n\n",
        "        for _ in range(n):\n",
        "            strand_rows.append({\n",
        "                'source': row['source'],\n",
        "                'dest': row['dest'],\n",
        "                'weight': weight,\n",
        "            })\n",
        "    return pd.DataFrame(strand_rows)\n",
        "\n",
        "\n",
        "# Accumulation flow\n",
        "if df_scores['A_end'].notna().any() and df_scores['A_start'].notna().any():\n",
        "    df_scores['delta'] = df_scores['A_end'] - df_scores['A_start']\n",
        "else:\n",
        "    df_scores['delta'] = np.nan\n",
        "\n",
        "df_scores['role'] = np.where(\n",
        "    df_scores['delta'] > 0,\n",
        "    'winner',\n",
        "    np.where(df_scores['delta'] < 0, 'loser', 'neutral')\n",
        ")\n",
        "\n",
        "df_scores_sorted = df_scores.sort_values(\n",
        "    by='delta', key=lambda s: s.abs(), ascending=False\n",
        ").reset_index(drop=True)\n",
        "\n",
        "accum_edges_df, accum_winners, accum_losers, accum_demand, accum_supply = build_edges_from_value(\n",
        "    df_scores, 'delta', TOP_K_WINNERS, TOP_K_LOSERS\n",
        ")\n",
        "accum_strands_df = split_strands(accum_edges_df)\n",
        "print(\"Accum winners/losers:\", len(accum_winners), len(accum_losers), \"edges:\", len(accum_edges_df))\n",
        "print(\"Accum strands:\", len(accum_strands_df))\n",
        "\n",
        "# Lit and Short flows (net buy - sell)\n",
        "if VOLUME_DATA_AVAILABLE and not df_volume.empty:\n",
        "    df_volume = df_volume.copy()\n",
        "    df_volume['lit_net'] = df_volume['lit_buy_sum'] - df_volume['lit_sell_sum']\n",
        "    df_volume['short_net'] = df_volume['short_buy_sum'] - df_volume['short_sell_sum']\n",
        "\n",
        "    lit_edges_df, lit_winners, lit_losers, lit_demand, lit_supply = build_edges_from_value(\n",
        "        df_volume, 'lit_net', TOP_K_WINNERS, TOP_K_LOSERS\n",
        "    )\n",
        "    short_edges_df, short_winners, short_losers, short_demand, short_supply = build_edges_from_value(\n",
        "        df_volume, 'short_net', TOP_K_WINNERS, TOP_K_LOSERS\n",
        "    )\n",
        "\n",
        "    lit_strands_df = split_strands(lit_edges_df)\n",
        "    short_strands_df = split_strands(short_edges_df)\n",
        "    total_strands = len(accum_strands_df) + len(lit_strands_df) + len(short_strands_df)\n",
        "    if total_strands > 200000:\n",
        "        print(\"WARNING: large strand count:\", total_strands, \"Consider increasing STRAND_UNIT.\")\n",
        "    print(\"Lit winners/losers:\", len(lit_winners), len(lit_losers), \"edges:\", len(lit_edges_df))\n",
        "    print(\"Short winners/losers:\", len(short_winners), len(short_losers), \"edges:\", len(short_edges_df))\n",
        "    print(\"Lit strands:\", len(lit_strands_df), \"Short strands:\", len(short_strands_df))\n",
        "else:\n",
        "    lit_edges_df = pd.DataFrame()\n",
        "    short_edges_df = pd.DataFrame()\n",
        "    lit_strands_df = pd.DataFrame()\n",
        "    short_strands_df = pd.DataFrame()\n",
        "    lit_winners = pd.DataFrame()\n",
        "    short_winners = pd.DataFrame()\n",
        "    lit_losers = pd.DataFrame()\n",
        "    short_losers = pd.DataFrame()\n",
        "    lit_demand = lit_supply = 0.0\n",
        "    short_demand = short_supply = 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f1ac9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6\n",
        "display(df_scores_sorted[['ticker', 'category', 'A_end', 'A_start', 'delta', 'role']])\n",
        "\n",
        "if SHOW_ACCUM_FLOW:\n",
        "    if accum_edges_df.empty:\n",
        "        display(pd.DataFrame(columns=['source', 'dest', 'flow']))\n",
        "    else:\n",
        "        display(accum_edges_df[['source', 'dest', 'flow']])\n",
        "\n",
        "if VOLUME_DATA_AVAILABLE and not df_volume.empty:\n",
        "    display(df_volume[['ticker', 'lit_buy_sum', 'lit_sell_sum', 'short_buy_sum', 'short_sell_sum']])\n",
        "if (df_volume['lit_buy_sum'].fillna(0).sum() == 0 and df_volume['lit_sell_sum'].fillna(0).sum() == 0):\n",
        "    print('Note: lit volumes sum to zero across selected period.')\n",
        "else:\n",
        "    print('Volume data not available for lit/short buy/sell flows.')\n",
        "\n",
        "summary_parts = [\n",
        "    f'END_DATE={END_DATE_RESOLVED}',\n",
        "    f'FLOW_PERIOD_DAYS={FLOW_PERIOD_DAYS}',\n",
        "    f'accum_supply={accum_supply:.2f}',\n",
        "    f'accum_demand={accum_demand:.2f}',\n",
        "]\n",
        "if SHOW_LIT_FLOW:\n",
        "    summary_parts.append(f'lit_supply={lit_supply:.2f}')\n",
        "    summary_parts.append(f'lit_demand={lit_demand:.2f}')\n",
        "if SHOW_SHORT_FLOW:\n",
        "    summary_parts.append(f'short_supply={short_supply:.2f}')\n",
        "    summary_parts.append(f'short_demand={short_demand:.2f}')\n",
        "\n",
        "summary_parts.append(f'accum_edges={len(accum_edges_df)}')\n",
        "summary_parts.append(f'lit_edges={len(lit_edges_df)}')\n",
        "summary_parts.append(f'short_edges={len(short_edges_df)}')\n",
        "summary_parts.append(\n",
        "    f'strands={len(accum_strands_df) + len(lit_strands_df) + len(short_strands_df)}'\n",
        ")\n",
        "\n",
        "print(' | '.join(summary_parts))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4741ff79",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "from matplotlib.colors import to_rgba\n",
        "from matplotlib.patches import Wedge\n",
        "\n",
        "CATEGORY_LABELS = {\n",
        "    'GLOBAL_MACRO': 'GLOBAL MACRO',\n",
        "    'MAG8': 'MAG8',\n",
        "    'SECTOR_SUMMARY': 'SECTOR SUMMARY',\n",
        "    'SECTOR_CORE': 'SECTORS',\n",
        "    'COMMODITIES': 'COMMODITIES',\n",
        "}\n",
        "CATEGORY_PALETTE = {\n",
        "    'GLOBAL_MACRO': '#5CC8FF',\n",
        "    'MAG8': '#DDA0FF',\n",
        "    'SECTOR_SUMMARY': '#A8DADC',\n",
        "    'SECTOR_CORE': '#F6C453',\n",
        "    'COMMODITIES': '#7CDE8A',\n",
        "    'UNKNOWN': '#A0A0A0',\n",
        "}\n",
        "\n",
        "FLOW_ACCUM_SOURCE = '#B300FF'  # neon purple\n",
        "FLOW_ACCUM_DEST = '#39FF14'    # neon green\n",
        "\n",
        "FLOW_LIT_SELL = '#FF7A7A'\n",
        "FLOW_LIT_BUY = '#7AD0FF'\n",
        "FLOW_SHORT_SELL = '#C1121F'\n",
        "FLOW_SHORT_BUY = '#1D4ED8'\n",
        "\n",
        "RING_LIT_BUY = '#8BD3FF'\n",
        "RING_LIT_SELL = '#FF9AA2'\n",
        "RING_SHORT_BUY = '#3B82F6'\n",
        "RING_SHORT_SELL = '#EF4444'\n",
        "\n",
        "\n",
        "def add_gradient_curve(ax, points, color_start, color_end, lw, alpha):\n",
        "    if len(points) < 2:\n",
        "        return\n",
        "    segments = np.stack([points[:-1], points[1:]], axis=1)\n",
        "    c0 = np.array(to_rgba(color_start))\n",
        "    c1 = np.array(to_rgba(color_end))\n",
        "    t = np.linspace(0, 1, len(segments))[:, None]\n",
        "    colors = c0 * (1 - t) + c1 * t\n",
        "    colors[:, 3] = colors[:, 3] * alpha\n",
        "    lc = LineCollection(segments, colors=colors, linewidths=lw, capstyle='round')\n",
        "    ax.add_collection(lc)\n",
        "\n",
        "\n",
        "def draw_band(ax, start_deg, end_deg, inner_r, outer_r, color, alpha):\n",
        "    if alpha <= 0:\n",
        "        return\n",
        "    wedge = Wedge(\n",
        "        (0, 0), outer_r, start_deg, end_deg,\n",
        "        width=outer_r - inner_r,\n",
        "        facecolor=color,\n",
        "        edgecolor='none',\n",
        "        alpha=alpha,\n",
        "    )\n",
        "    ax.add_patch(wedge)\n",
        "\n",
        "\n",
        "def plot_chord_matplotlib():\n",
        "    grouped = {\n",
        "        'GLOBAL_MACRO': [t for t in ticker_order if ticker_category.get(t) == 'GLOBAL_MACRO'],\n",
        "        'MAG8': [t for t in ticker_order if ticker_category.get(t) == 'MAG8'],\n",
        "        'SECTOR_SUMMARY': [t for t in ticker_order if ticker_category.get(t) == 'SECTOR_SUMMARY'],\n",
        "        'SECTOR_CORE': [t for t in ticker_order if ticker_category.get(t) == 'SECTOR_CORE'],\n",
        "        'COMMODITIES': [t for t in ticker_order if ticker_category.get(t) == 'COMMODITIES'],\n",
        "    }\n",
        "\n",
        "    total_nodes = sum(len(v) for v in grouped.values())\n",
        "    if total_nodes == 0:\n",
        "        print('No nodes to plot.')\n",
        "        return\n",
        "\n",
        "    gap = 0.12\n",
        "    total_gap = gap * len([g for g in grouped.values() if g])\n",
        "    usable = 2 * math.pi - total_gap\n",
        "    if usable <= 0:\n",
        "        usable = 2 * math.pi\n",
        "    step = usable / total_nodes\n",
        "    arc_span = step * 0.85\n",
        "\n",
        "    angles = {}\n",
        "    spans = {}\n",
        "    angle = 0.0\n",
        "    for cat in ['GLOBAL_MACRO', 'MAG8', 'SECTOR_SUMMARY', 'SECTOR_CORE', 'COMMODITIES']:\n",
        "\n",
        "        if not grouped[cat]:\n",
        "            continue\n",
        "        angle += gap / 2\n",
        "        for t in grouped[cat]:\n",
        "            angles[t] = angle\n",
        "            spans[t] = (angle - arc_span / 2, angle + arc_span / 2)\n",
        "            angle += step\n",
        "        angle += gap / 2\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 12), subplot_kw={'aspect': 'equal'})\n",
        "    fig.patch.set_facecolor('#0b0f1a')\n",
        "    ax.set_facecolor('#0b0f1a')\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Base circle\n",
        "    theta = np.linspace(0, 2 * math.pi, 400)\n",
        "    ax.plot(np.cos(theta), np.sin(theta), color='#39424e', lw=1.0, alpha=0.6)\n",
        "\n",
        "    # Ticker arcs\n",
        "    if SHOW_TICKER_ARC:\n",
        "        base_outer = 1.05\n",
        "        base_width = 0.08\n",
        "        for t, (a0, a1) in spans.items():\n",
        "            start_deg = math.degrees(a0)\n",
        "            end_deg = math.degrees(a1)\n",
        "            color = CATEGORY_PALETTE.get(ticker_category.get(t, 'UNKNOWN'), '#A0A0A0')\n",
        "            wedge = Wedge(\n",
        "                (0, 0), base_outer, start_deg, end_deg,\n",
        "                width=base_width,\n",
        "                facecolor=color,\n",
        "                edgecolor='#222831',\n",
        "                lw=0.4,\n",
        "                alpha=0.9,\n",
        "            )\n",
        "            ax.add_patch(wedge)\n",
        "    else:\n",
        "        base_outer = 1.02\n",
        "        base_width = 0.02\n",
        "\n",
        "    # Volume rings\n",
        "    if SHOW_VOLUME_RING and VOLUME_DATA_AVAILABLE and not df_volume.empty:\n",
        "        band_width = 0.035\n",
        "        band_gap = 0.01\n",
        "        band_start = base_outer + band_gap\n",
        "        bands = [\n",
        "            ('lit_buy_sum', RING_LIT_BUY),\n",
        "            ('lit_sell_sum', RING_LIT_SELL),\n",
        "            ('short_buy_sum', RING_SHORT_BUY),\n",
        "            ('short_sell_sum', RING_SHORT_SELL),\n",
        "        ]\n",
        "        max_vals = {}\n",
        "        for col, _ in bands:\n",
        "            series = pd.to_numeric(df_volume[col], errors='coerce')\n",
        "            max_vals[col] = float(series.max()) if series.notna().any() else 0.0\n",
        "\n",
        "        for idx, (col, color) in enumerate(bands):\n",
        "            inner_r = band_start + idx * (band_width + band_gap)\n",
        "            outer_r = inner_r + band_width\n",
        "            for t, (a0, a1) in spans.items():\n",
        "                value = df_volume.loc[df_volume['ticker'] == t, col]\n",
        "                if value.empty or pd.isna(value.iloc[0]):\n",
        "                    continue\n",
        "                max_val = max_vals.get(col, 0.0)\n",
        "                alpha = 0.15 + 0.85 * (float(value.iloc[0]) / max_val) if max_val > 0 else 0.0\n",
        "                draw_band(\n",
        "                    ax,\n",
        "                    math.degrees(a0),\n",
        "                    math.degrees(a1),\n",
        "                    inner_r,\n",
        "                    outer_r,\n",
        "                    color,\n",
        "                    alpha,\n",
        "                )\n",
        "\n",
        "    # Chords\n",
        "    def draw_chords(strands_df, color_start, color_end, alpha=0.6):\n",
        "        if strands_df.empty:\n",
        "            return\n",
        "        max_w = strands_df['weight'].max() if not strands_df.empty else 1.0\n",
        "        for _, row in strands_df.iterrows():\n",
        "            src = row['source']\n",
        "            dst = row['dest']\n",
        "            w = row['weight']\n",
        "            if src not in angles or dst not in angles:\n",
        "                continue\n",
        "            a0 = angles[src]\n",
        "            a1 = angles[dst]\n",
        "            p0 = np.array([math.cos(a0), math.sin(a0)])\n",
        "            p2 = np.array([math.cos(a1), math.sin(a1)])\n",
        "            p1 = np.array([0.0, 0.0])\n",
        "            t = np.linspace(0, 1, 80)[:, None]\n",
        "            curve = (1 - t) ** 2 * p0 + 2 * (1 - t) * t * p1 + t ** 2 * p2\n",
        "            lw = 0.6 + 3.2 * ((w / max_w) ** 0.7)\n",
        "            add_gradient_curve(ax, curve, color_start, color_end, lw=lw, alpha=alpha)\n",
        "\n",
        "    if SHOW_LIT_FLOW:\n",
        "        draw_chords(lit_strands_df, FLOW_LIT_SELL, FLOW_LIT_BUY, alpha=0.5)\n",
        "    if SHOW_SHORT_FLOW:\n",
        "        draw_chords(short_strands_df, FLOW_SHORT_SELL, FLOW_SHORT_BUY, alpha=0.55)\n",
        "    if SHOW_ACCUM_FLOW:\n",
        "        draw_chords(accum_strands_df, FLOW_ACCUM_SOURCE, FLOW_ACCUM_DEST, alpha=0.6)\n",
        "\n",
        "    # Ticker labels\n",
        "    for t, ang in angles.items():\n",
        "        x = math.cos(ang)\n",
        "        y = math.sin(ang)\n",
        "        r = 1.18\n",
        "        rot = math.degrees(ang)\n",
        "        if math.pi / 2 < ang < 3 * math.pi / 2:\n",
        "            rot += 180\n",
        "        ax.text(\n",
        "            r * x, r * y, t,\n",
        "            color='#E6EDF7', fontsize=9,\n",
        "            ha='center', va='center',\n",
        "            rotation=rot, rotation_mode='anchor'\n",
        "        )\n",
        "\n",
        "    # Category labels\n",
        "    for cat, tickers in grouped.items():\n",
        "        if not tickers:\n",
        "            continue\n",
        "        mid_angle = np.mean([angles[t] for t in tickers])\n",
        "        ax.text(\n",
        "            1.35 * math.cos(mid_angle),\n",
        "            1.35 * math.sin(mid_angle),\n",
        "            CATEGORY_LABELS.get(cat, cat),\n",
        "            color=CATEGORY_PALETTE.get(cat, '#A0A0A0'),\n",
        "            fontsize=12, fontweight='bold',\n",
        "            ha='center', va='center'\n",
        "        )\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_chord_matplotlib()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
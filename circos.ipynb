{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eeaa72d",
   "metadata": {},
   "source": [
    "# Cell 1\n",
    "# Circos-Style Money Flow Chord Diagram\n",
    "\n",
    "This notebook builds a self-contained Circos-style chord diagram from Accumulation Score time series in a local database.\n",
    "\n",
    "Constraints honored:\n",
    "- No project modules are imported or modified.\n",
    "- Database access is read-only (SELECT-only).\n",
    "- No files are written or mutated; all outputs are in-notebook only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c759775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "# Ticker groups (hardcoded)\n",
    "SECTOR_CORE_TICKERS = [\n",
    "    \"XLF\",\"KRE\",\"XLK\",\"SMH\",\"XLI\",\"XLY\",\"XLE\",\"XLV\",\"XLP\",\"XLU\"\n",
    "]\n",
    "\n",
    "SECTOR_SUMMARY_TICKERS = [\n",
    "    \"XLE\", \"XLF\", \"XLK\", \"XLY\", \"XLP\", \"XLV\", \"XLU\", \"XLI\", \"XLC\", \"XLB\", \"XLRE\", \"SPY\",\n",
    "]\n",
    "\n",
    "GLOBAL_MACRO_TICKERS = [\n",
    "    \"SPY\",\"QQQ\",\"TQQQ\",\"IWM\",\"VGK\",\"EWJ\",\"EFA\",\"EEM\",\"FXI\",\"UUP\",\"TLT\",\"GLD\",\"USO\",\"VIXY\"\n",
    "]\n",
    "\n",
    "COMMODITIES_TICKERS = [\n",
    "    \"GLD\",\"SLV\",\"GDX\",\"USO\",\"UNG\",\"URA\"\n",
    "]\n",
    "\n",
    "MAG8_TICKERS = [\n",
    "    \"AAPL\",\"MSFT\",\"NVDA\",\"AMZN\",\"GOOGL\",\"META\",\"TSLA\",\"AVGO\"\n",
    "]\n",
    "\n",
    "ENABLED_GROUPS = {\n",
    "    \"GLOBAL_MACRO\": False,\n",
    "    \"MAG8\": False,\n",
    "    \"SECTOR_SUMMARY\": True,\n",
    "    \"SECTOR_CORE\": False,\n",
    "    \"COMMODITIES\": False,\n",
    "}\n",
    "\n",
    "# Controls\n",
    "END_DATE = None                         # End of window; None = auto-detect max date in DB\n",
    "FLOW_PERIOD_DAYS = 3                    # Trading-day window used for flows (start->end inside this window)\n",
    "TOP_K_WINNERS = 8                       # Max winners by demand (positive delta) to include\n",
    "TOP_K_LOSERS = 8                        # Max losers by supply (negative delta) to include\n",
    "MIN_EDGE_FLOW = 0.1                     # Drop edges smaller than this flow (pre-strand)\n",
    "DISTRIBUTION_MODE = \"demand_weighted\"   # \"equal\" or \"demand_weighted\" split from sources to winners\n",
    "STRAND_UNIT = 10.0                       # Flow per strand; lower = more ribbons (denser)\n",
    "AUTO_STRAND_SCALING = True              # Auto-increase STRAND_UNIT if strands explode\n",
    "MAX_STRANDS = 50000                     # Hard cap for total strands after auto-scaling\n",
    "\n",
    "# Chord + ring layout\n",
    "METRIC_BAND_MODE = \"proportional\"       # \"equal\" = same band width per metric; \"proportional\" = band width by flow\n",
    "MAX_EDGES_PER_METRIC = 15               # cap edges per metric to keep plot readable\n",
    "CHORD_ARC_FRACTION = 0.9                # fraction of ticker arc reserved for chord endpoints\n",
    "CHORD_RADIUS = 0.78                     # inner radius for chord ribbons\n",
    "TIME_SLICE_BINS = 3                     # number of time slices per outer ring\n",
    "RING_BASE_THICKNESS = 0.012             # minimum ring thickness\n",
    "RING_THICKNESS_SCALE = 0.06             # added thickness scaled by magnitude\n",
    "RING_GAP = 0.01                         # gap between metric rings\n",
    "BAND_GAP_FRAC = 0.035                   # gap between metric bands within a ticker\n",
    "DIR_GAP_FRAC = 0.03                     # gap between outflow/inflow halves inside a band\n",
    "\n",
    "# Render quality (performance vs fidelity)\n",
    "RENDER_MODE = \"balanced\"  # \"fast\", \"balanced\", \"quality\"\n",
    "CHORD_FILL_ALPHA = 0.55\n",
    "CHORD_LINE_ALPHA = 0.8\n",
    "CHORD_COLOR_SOFTEN = 0.25  # blend toward background to reduce saturation\n",
    "if RENDER_MODE == \"fast\":\n",
    "    USE_GRADIENT_FILL = False\n",
    "    CHORD_GRADIENT_STEPS = 8\n",
    "    CHORD_ARC_POINTS = 8\n",
    "    CHORD_CURVE_POINTS = 30\n",
    "elif RENDER_MODE == \"quality\":\n",
    "    USE_GRADIENT_FILL = True\n",
    "    CHORD_GRADIENT_STEPS = 32\n",
    "    CHORD_ARC_POINTS = 18\n",
    "    CHORD_CURVE_POINTS = 70\n",
    "else:\n",
    "    USE_GRADIENT_FILL = True\n",
    "    CHORD_GRADIENT_STEPS = 18\n",
    "    CHORD_ARC_POINTS = 12\n",
    "    CHORD_CURVE_POINTS = 50\n",
    "\n",
    "# Layer toggles\n",
    "SHOW_ACCUM_FLOW = True\n",
    "SHOW_LIT_FLOW = True\n",
    "SHOW_SHORT_FLOW = True\n",
    "SHOW_VOLUME_RING = True\n",
    "SHOW_TICKER_ARC = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b232f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def find_db_candidates():\n",
    "    root = Path('.').resolve()\n",
    "    search_dirs = [\n",
    "        root / 'data',\n",
    "        root / 'darkpool_analysis' / 'data',\n",
    "        root,\n",
    "    ]\n",
    "    patterns = ['*.duckdb', '*.db', '*.sqlite', '*.sqlite3']\n",
    "    candidates = []\n",
    "    for base in search_dirs:\n",
    "        if not base.exists():\n",
    "            continue\n",
    "        for pattern in patterns:\n",
    "            candidates.extend(list(base.rglob(pattern)))\n",
    "    # Unique + files only\n",
    "    unique = []\n",
    "    seen = set()\n",
    "    for p in candidates:\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        if p in seen:\n",
    "            continue\n",
    "        seen.add(p)\n",
    "        unique.append(p)\n",
    "    unique.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return unique\n",
    "\n",
    "def connect_db(db_path):\n",
    "    db_path = Path(db_path)\n",
    "    suffix = db_path.suffix.lower()\n",
    "    errors = []\n",
    "\n",
    "    def try_duckdb():\n",
    "        import duckdb\n",
    "        return duckdb.connect(database=str(db_path), read_only=True), 'duckdb'\n",
    "\n",
    "    def try_sqlite():\n",
    "        import sqlite3\n",
    "        uri = f\"file:{db_path.resolve().as_posix()}->mode=ro\"\n",
    "        return sqlite3.connect(uri, uri=True), 'sqlite'\n",
    "\n",
    "    if suffix == '.duckdb':\n",
    "        try:\n",
    "            return try_duckdb()\n",
    "        except Exception as e:\n",
    "            errors.append(f'duckdb: {e}')\n",
    "        try:\n",
    "            return try_sqlite()\n",
    "        except Exception as e:\n",
    "            errors.append(f'sqlite: {e}')\n",
    "    else:\n",
    "        try:\n",
    "            return try_sqlite()\n",
    "        except Exception as e:\n",
    "            errors.append(f'sqlite: {e}')\n",
    "        try:\n",
    "            return try_duckdb()\n",
    "        except Exception as e:\n",
    "            errors.append(f'duckdb: {e}')\n",
    "\n",
    "    raise RuntimeError('Unable to open database. ' + '; '.join(errors))\n",
    "\n",
    "def list_tables(conn, db_type):\n",
    "    if db_type == 'duckdb':\n",
    "        return [r[0] for r in conn.execute('SHOW TABLES').fetchall()]\n",
    "    rows = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()\n",
    "    return [r[0] for r in rows]\n",
    "\n",
    "def get_columns(conn, db_type, table):\n",
    "    pragma = f'PRAGMA table_info(\"{table}\")'\n",
    "    rows = conn.execute(pragma).fetchall()\n",
    "    # rows: (cid, name, type, notnull, dflt_value, pk)\n",
    "    return [r[1] for r in rows]\n",
    "\n",
    "def normalize_name(name):\n",
    "    return ''.join([c for c in name.lower() if c.isalnum()])\n",
    "\n",
    "def pick_column(columns, candidates):\n",
    "    norm_map = {normalize_name(c): c for c in columns}\n",
    "    for cand in candidates:\n",
    "        cand_norm = normalize_name(cand)\n",
    "        if cand_norm in norm_map:\n",
    "            return norm_map[cand_norm]\n",
    "    for cand in candidates:\n",
    "        cand_norm = normalize_name(cand)\n",
    "        for col_norm, original in norm_map.items():\n",
    "            if cand_norm in col_norm:\n",
    "                return original\n",
    "    return None\n",
    "\n",
    "def quote_ident(name):\n",
    "    safe = name.replace('\"', '\"\"')\n",
    "    return f'\"{safe}\"'\n",
    "\n",
    "TICKER_COL_CANDIDATES = ['ticker', 'symbol', 'sym']\n",
    "DATE_COL_CANDIDATES = ['date', 'trade_date', 'market_date', 'dt', 'day']\n",
    "ACCUM_COL_CANDIDATES = [\n",
    "    'accumulation_score', 'accum_score', 'accumulation', 'accum',\n",
    "    'accumulation_score_display', 'accum_score_display'\n",
    "]\n",
    "\n",
    "db_candidates = find_db_candidates()\n",
    "print('DB candidates (newest first):')\n",
    "for p in db_candidates:\n",
    "    print(' -', p)\n",
    "\n",
    "if not db_candidates:\n",
    "    print('No database files found under ./data or ./darkpool_analysis/data.')\n",
    "    raise SystemExit\n",
    "\n",
    "DB_PATH = db_candidates[0]\n",
    "print(f'Selected DB: {DB_PATH}')\n",
    "\n",
    "conn, DB_TYPE = connect_db(DB_PATH)\n",
    "print(f'Detected DB type: {DB_TYPE}')\n",
    "\n",
    "tables = list_tables(conn, DB_TYPE)\n",
    "print(f'Found {len(tables)} tables.')\n",
    "\n",
    "scan_rows = []\n",
    "for table in tables:\n",
    "    try:\n",
    "        cols = get_columns(conn, DB_TYPE, table)\n",
    "    except Exception as e:\n",
    "        scan_rows.append({\n",
    "            'table': table,\n",
    "            'ticker_col': None,\n",
    "            'date_col': None,\n",
    "            'accum_col': None,\n",
    "            'row_count': None,\n",
    "            'distinct_tickers': None,\n",
    "            'error': str(e),\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    ticker_col = pick_column(cols, TICKER_COL_CANDIDATES)\n",
    "    date_col = pick_column(cols, DATE_COL_CANDIDATES)\n",
    "    accum_col = pick_column(cols, ACCUM_COL_CANDIDATES)\n",
    "\n",
    "    row_count = None\n",
    "    distinct_tickers = None\n",
    "    if ticker_col and date_col and accum_col:\n",
    "        try:\n",
    "            row_count = conn.execute(\n",
    "                f'SELECT COUNT(*) FROM {quote_ident(table)}'\n",
    "            ).fetchone()[0]\n",
    "            distinct_tickers = conn.execute(\n",
    "                f'SELECT COUNT(DISTINCT {quote_ident(ticker_col)}) FROM {quote_ident(table)}'\n",
    "            ).fetchone()[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    scan_rows.append({\n",
    "        'table': table,\n",
    "        'ticker_col': ticker_col,\n",
    "        'date_col': date_col,\n",
    "        'accum_col': accum_col,\n",
    "        'row_count': row_count,\n",
    "        'distinct_tickers': distinct_tickers,\n",
    "        'error': None,\n",
    "    })\n",
    "\n",
    "print('Scanned tables (ticker/date/accum detection):')\n",
    "for row in scan_rows:\n",
    "    print(\n",
    "        f\" - {row['table']}: ticker={row['ticker_col']} date={row['date_col']} accum={row['accum_col']} rows={row['row_count']} distinct_tickers={row['distinct_tickers']}\"\n",
    "    )\n",
    "\n",
    "candidates = [r for r in scan_rows if r['ticker_col'] and r['date_col'] and r['accum_col']]\n",
    "if not candidates:\n",
    "    print('No tables matched the required ticker/date/accumulation schema.')\n",
    "    conn.close()\n",
    "    raise SystemExit\n",
    "\n",
    "multi = [r for r in candidates if (r['distinct_tickers'] or 0) > 1]\n",
    "pool = multi if multi else candidates\n",
    "pool.sort(key=lambda r: (r['row_count'] or 0, r['distinct_tickers'] or 0), reverse=True)\n",
    "best = pool[0]\n",
    "\n",
    "SELECT_TABLE = best['table']\n",
    "TICKER_COL = best['ticker_col']\n",
    "DATE_COL = best['date_col']\n",
    "ACCUM_COL = best['accum_col']\n",
    "\n",
    "print('Selected table:')\n",
    "print(f'  table={SELECT_TABLE}')\n",
    "print(f'  ticker_col={TICKER_COL}, date_col={DATE_COL}, accum_col={ACCUM_COL}')\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "def build_ticker_universe():\n",
    "    seen = set()\n",
    "    ordered = []\n",
    "    categories = {}\n",
    "    enabled = globals().get(\"ENABLED_GROUPS\", {})\n",
    "    group_defs = [\n",
    "        (\"GLOBAL_MACRO\", globals().get(\"GLOBAL_MACRO_TICKERS\", [])),\n",
    "        (\"MAG8\", globals().get(\"MAG8_TICKERS\", [])),\n",
    "        (\"SECTOR_SUMMARY\", globals().get(\"SECTOR_SUMMARY_TICKERS\", [])),\n",
    "        (\"SECTOR_CORE\", globals().get(\"SECTOR_CORE_TICKERS\", [])),\n",
    "        (\"COMMODITIES\", globals().get(\"COMMODITIES_TICKERS\", [])),\n",
    "    ]\n",
    "    for group_name, tickers in group_defs:\n",
    "        if not enabled.get(group_name, True):\n",
    "            continue\n",
    "        for t in tickers:\n",
    "            if t not in seen:\n",
    "                seen.add(t)\n",
    "                ordered.append(t)\n",
    "                categories[t] = group_name\n",
    "    return ordered, categories\n",
    "\n",
    "\n",
    "ticker_order, ticker_category = build_ticker_universe()\n",
    "print(\"Ticker universe size:\", len(ticker_order))\n",
    "print(\"Enabled groups:\", {k: v for k, v in ENABLED_GROUPS.items()})\n",
    "ticker_list = [t.upper() for t in ticker_order]\n",
    "import re\n",
    "\n",
    "conn, DB_TYPE = connect_db(DB_PATH)\n",
    "\n",
    "if DB_TYPE == 'duckdb':\n",
    "    date_expr = f\"CAST({quote_ident(DATE_COL)} AS DATE)\"\n",
    "    accum_expr = f\"{quote_ident(ACCUM_COL)}\"\n",
    "else:\n",
    "    date_expr = f\"DATE({quote_ident(DATE_COL)})\"\n",
    "    accum_expr = f\"{quote_ident(ACCUM_COL)}\"\n",
    "\n",
    "placeholders = ','.join(['?'] * len(ticker_list))\n",
    "query = (\n",
    "    f\"SELECT UPPER({quote_ident(TICKER_COL)}) AS ticker, \"\n",
    "    f\"{date_expr} AS date, {accum_expr} AS accumulation_score \"\n",
    "    f\"FROM {quote_ident(SELECT_TABLE)} \"\n",
    "    f\"WHERE UPPER({quote_ident(TICKER_COL)}) IN ({placeholders})\"\n",
    ")\n",
    "\n",
    "print(f\"Using accumulation table: {SELECT_TABLE} (ticker={TICKER_COL}, date={DATE_COL}, accum={ACCUM_COL})\")\n",
    "\n",
    "try:\n",
    "    if DB_TYPE == 'duckdb':\n",
    "        df_raw = conn.execute(query, ticker_list).df()\n",
    "    else:\n",
    "        df_raw = pd.read_sql_query(query, conn, params=ticker_list)\n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "if df_raw.empty:\n",
    "    print('No data returned for the specified tickers.')\n",
    "    raise SystemExit\n",
    "\n",
    "print(\"Accumulation rows loaded:\", len(df_raw))\n",
    "print(\"Accumulation table date range:\", df_raw[\"date\"].min(), \"->\", df_raw[\"date\"].max())\n",
    "print(\"Accumulation raw sample:\", df_raw[\"accumulation_score\"].head(5).tolist())\n",
    "df_raw['date'] = pd.to_datetime(df_raw['date'], errors='coerce').dt.date\n",
    "df_raw['ticker'] = df_raw['ticker'].str.upper()\n",
    "\n",
    "def fetch_accum_column(accum_col_name):\n",
    "    conn, db_type = connect_db(DB_PATH)\n",
    "    try:\n",
    "        if db_type == 'duckdb':\n",
    "            date_expr = f\"CAST({quote_ident(DATE_COL)} AS DATE)\"\n",
    "        else:\n",
    "            date_expr = f\"DATE({quote_ident(DATE_COL)})\"\n",
    "        query = (\n",
    "            f\"SELECT UPPER({quote_ident(TICKER_COL)}) AS ticker, \"\n",
    "            f\"{date_expr} AS date, {quote_ident(accum_col_name)} AS accumulation_score \"\n",
    "            f\"FROM {quote_ident(SELECT_TABLE)} \"\n",
    "            f\"WHERE UPPER({quote_ident(TICKER_COL)}) IN ({placeholders})\"\n",
    "        )\n",
    "        if db_type == 'duckdb':\n",
    "            return conn.execute(query, ticker_list).df()\n",
    "        return pd.read_sql_query(query, conn, params=ticker_list)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def normalize_accum_df(df):\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.date\n",
    "    df['ticker'] = df['ticker'].str.upper()\n",
    "    df['accumulation_score'] = df['accumulation_score'].apply(parse_accum)\n",
    "    return df.dropna(subset=['date', 'accumulation_score'])\n",
    "\n",
    "def parse_accum(value):\n",
    "    if value is None:\n",
    "        return np.nan\n",
    "    if isinstance(value, (int, float, np.number)):\n",
    "        if np.isnan(value):\n",
    "            return np.nan\n",
    "        return float(value)\n",
    "    s = str(value).strip()\n",
    "    if not s:\n",
    "        return np.nan\n",
    "    s = s.replace('%', '').replace(',', '')\n",
    "    m = re.search(r'-->\\d+(->:\\.\\d+)->', s)\n",
    "    return float(m.group(0)) if m else np.nan\n",
    "\n",
    "df_raw = normalize_accum_df(df_raw)\n",
    "ACCUM_COL_SELECTED = ACCUM_COL\n",
    "if df_raw.empty:\n",
    "    print('Primary accumulation column returned no usable values; trying fallbacks...')\n",
    "    conn, db_type = connect_db(DB_PATH)\n",
    "    try:\n",
    "        cols = get_columns(conn, db_type, SELECT_TABLE)\n",
    "    finally:\n",
    "        conn.close()\n",
    "    candidates = []\n",
    "    for cand in [\n",
    "        'accumulation_score_display', 'accum_score_display',\n",
    "        'accumulation_score', 'accum_score', 'accumulation', 'accum'\n",
    "    ]:\n",
    "        col = pick_column(cols, [cand])\n",
    "        if col and col not in candidates:\n",
    "            candidates.append(col)\n",
    "    candidates = [c for c in candidates if c != ACCUM_COL]\n",
    "    print('Accumulation fallback candidates:', candidates)\n",
    "    for col in candidates:\n",
    "        print('Trying accumulation column:', col)\n",
    "        df_try = fetch_accum_column(col)\n",
    "        print('Fallback rows loaded:', len(df_try))\n",
    "        print('Fallback raw sample:', df_try['accumulation_score'].head(5).tolist())\n",
    "        df_try = normalize_accum_df(df_try)\n",
    "        if not df_try.empty:\n",
    "            ACCUM_COL_SELECTED = col\n",
    "            df_raw = df_try\n",
    "            break\n",
    "\n",
    "if df_raw.empty:\n",
    "    print('All accumulation score rows are null after parsing.')\n",
    "    raise SystemExit\n",
    "print('Using accumulation column:', ACCUM_COL_SELECTED)\n",
    "\n",
    "df_raw_full = df_raw.copy()\n",
    "\n",
    "max_date = df_raw['date'].max()\n",
    "if END_DATE is None or str(END_DATE).strip() == '':\n",
    "    END_DATE_RESOLVED = max_date\n",
    "else:\n",
    "    END_DATE_RESOLVED = pd.to_datetime(END_DATE).date()\n",
    "    if END_DATE_RESOLVED > max_date:\n",
    "        print(f'END_DATE {END_DATE_RESOLVED} exceeds DB max date {max_date}; using max date.')\n",
    "        END_DATE_RESOLVED = max_date\n",
    "\n",
    "flow_days = int(FLOW_PERIOD_DAYS) if FLOW_PERIOD_DAYS and int(FLOW_PERIOD_DAYS) > 0 else 1\n",
    "print(\"Flow period days:\", flow_days, \"END_DATE:\", END_DATE_RESOLVED)\n",
    "\n",
    "all_dates = sorted([d for d in df_raw[\"date\"].unique() if pd.notna(d)])\n",
    "end_dates = [d for d in all_dates if d <= END_DATE_RESOLVED]\n",
    "window_dates = end_dates[-flow_days:]\n",
    "if not window_dates:\n",
    "    print(\"No dates available within FLOW_PERIOD_DAYS window.\")\n",
    "    raise SystemExit\n",
    "print(\"Window date range used:\", window_dates[0], \"->\", window_dates[-1], \"count:\", len(window_dates))\n",
    "df_accum_daily = df_raw_full.sort_values([\"ticker\", \"date\"]).copy()\n",
    "df_accum_daily[\"accum_net\"] = df_accum_daily.groupby(\"ticker\")[\"accumulation_score\"].diff()\n",
    "df_accum_daily[\"accum_net\"] = df_accum_daily[\"accum_net\"].fillna(0.0)\n",
    "df_accum_daily = df_accum_daily[df_accum_daily[\"date\"].isin(window_dates)][[\"ticker\", \"date\", \"accum_net\"]]\n",
    "df_raw = df_raw[df_raw[\"date\"].isin(window_dates)]\n",
    "print(\"Accumulation window rows:\", len(df_raw))\n",
    "missing = sorted(set(ticker_list) - set(df_raw[\"ticker\"].unique()))\n",
    "if missing:\n",
    "    print(\"Missing tickers in accumulation window:\", missing)\n",
    "if df_raw.empty:\n",
    "    print(\"No accumulation data within the selected window.\")\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "def tail_n(df_ticker, n, end_date):\n",
    "    df = df_ticker[df_ticker['date'] <= end_date].sort_values('date')\n",
    "    if df.empty:\n",
    "        return df\n",
    "    return df.tail(n)\n",
    "\n",
    "\n",
    "rows = []\n",
    "for ticker in ticker_order:\n",
    "    df_t = df_raw[df_raw['ticker'] == ticker]\n",
    "    tail = tail_n(df_t, flow_days, END_DATE_RESOLVED)\n",
    "    if len(tail) < 2:\n",
    "        a_start = None\n",
    "        a_end = None\n",
    "    else:\n",
    "        a_start = float(tail['accumulation_score'].iloc[0])\n",
    "        a_end = float(tail['accumulation_score'].iloc[-1])\n",
    "    rows.append({\n",
    "        'ticker': ticker,\n",
    "        'category': ticker_category.get(ticker, 'UNKNOWN'),\n",
    "        'A_end': a_end,\n",
    "        'A_start': a_start,\n",
    "        'end_date': END_DATE_RESOLVED,\n",
    "        'start_date': tail['date'].iloc[0] if len(tail) else None,\n",
    "        'samples': len(tail),\n",
    "    })\n",
    "\n",
    "df_scores = pd.DataFrame(rows)\n",
    "\n",
    "# --- Volume data discovery (lit/short buy/sell) ---\n",
    "LIT_BUY_CANDIDATES = ['lit_buy_volume', 'lit_buy_vol', 'lit_buy']\n",
    "LIT_SELL_CANDIDATES = ['lit_sell_volume', 'lit_sell_vol', 'lit_sell']\n",
    "SHORT_BUY_CANDIDATES = ['short_buy_volume', 'short_buy_vol', 'short_buy']\n",
    "SHORT_SELL_CANDIDATES = ['short_sell_volume', 'short_sell_vol', 'short_sell']\n",
    "\n",
    "\n",
    "def find_volume_table(conn, db_type):\n",
    "    tables = list_tables(conn, db_type)\n",
    "    candidates = []\n",
    "    for table in tables:\n",
    "        try:\n",
    "            cols = get_columns(conn, db_type, table)\n",
    "        except Exception:\n",
    "            continue\n",
    "        ticker_col = pick_column(cols, TICKER_COL_CANDIDATES)\n",
    "        date_col = pick_column(cols, DATE_COL_CANDIDATES)\n",
    "        lit_buy_col = pick_column(cols, LIT_BUY_CANDIDATES)\n",
    "        lit_sell_col = pick_column(cols, LIT_SELL_CANDIDATES)\n",
    "        short_buy_col = pick_column(cols, SHORT_BUY_CANDIDATES)\n",
    "        short_sell_col = pick_column(cols, SHORT_SELL_CANDIDATES)\n",
    "        if not (ticker_col and date_col and lit_buy_col and lit_sell_col and short_buy_col and short_sell_col):\n",
    "            continue\n",
    "        row_count = None\n",
    "        distinct_tickers = None\n",
    "        try:\n",
    "            row_count = conn.execute(\n",
    "                f'SELECT COUNT(*) FROM {quote_ident(table)}'\n",
    "            ).fetchone()[0]\n",
    "            distinct_tickers = conn.execute(\n",
    "                f'SELECT COUNT(DISTINCT {quote_ident(ticker_col)}) FROM {quote_ident(table)}'\n",
    "            ).fetchone()[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "        candidates.append({\n",
    "            'table': table,\n",
    "            'ticker_col': ticker_col,\n",
    "            'date_col': date_col,\n",
    "            'lit_buy_col': lit_buy_col,\n",
    "            'lit_sell_col': lit_sell_col,\n",
    "            'short_buy_col': short_buy_col,\n",
    "            'short_sell_col': short_sell_col,\n",
    "            'row_count': row_count,\n",
    "            'distinct_tickers': distinct_tickers,\n",
    "        })\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates.sort(key=lambda r: (r['row_count'] or 0, r['distinct_tickers'] or 0), reverse=True)\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "conn, DB_TYPE = connect_db(DB_PATH)\n",
    "try:\n",
    "    volume_info = find_volume_table(conn, DB_TYPE)\n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "VOLUME_DATA_AVAILABLE = volume_info is not None\n",
    "if VOLUME_DATA_AVAILABLE:\n",
    "    print(\"Volume table selected:\", volume_info[\"table\"])\n",
    "    print(\"Volume columns:\", {k: volume_info[k] for k in [\"ticker_col\", \"date_col\", \"lit_buy_col\", \"lit_sell_col\", \"short_buy_col\", \"short_sell_col\"]})\n",
    "else:\n",
    "    print(\"No volume table found with lit/short buy/sell columns.\")\n",
    "\n",
    "if VOLUME_DATA_AVAILABLE:\n",
    "    conn, DB_TYPE = connect_db(DB_PATH)\n",
    "    try:\n",
    "        if DB_TYPE == 'duckdb':\n",
    "            date_expr = f\"CAST({quote_ident(volume_info['date_col'])} AS DATE)\"\n",
    "            num_cast = \"TRY_CAST\"\n",
    "        else:\n",
    "            date_expr = f\"DATE({quote_ident(volume_info['date_col'])})\"\n",
    "            num_cast = \"CAST\"\n",
    "\n",
    "        query = (\n",
    "            f\"SELECT UPPER({quote_ident(volume_info['ticker_col'])}) AS ticker, \"\n",
    "            f\"{date_expr} AS date, \"\n",
    "            f\"{num_cast}({quote_ident(volume_info['lit_buy_col'])} AS DOUBLE) AS lit_buy, \"\n",
    "            f\"{num_cast}({quote_ident(volume_info['lit_sell_col'])} AS DOUBLE) AS lit_sell, \"\n",
    "            f\"{num_cast}({quote_ident(volume_info['short_buy_col'])} AS DOUBLE) AS short_buy, \"\n",
    "            f\"{num_cast}({quote_ident(volume_info['short_sell_col'])} AS DOUBLE) AS short_sell \"\n",
    "            f\"FROM {quote_ident(volume_info['table'])} \"\n",
    "            f\"WHERE UPPER({quote_ident(volume_info['ticker_col'])}) IN ({placeholders})\"\n",
    "        )\n",
    "\n",
    "        if DB_TYPE == 'duckdb':\n",
    "            df_vol_raw = conn.execute(query, ticker_list).df()\n",
    "        else:\n",
    "            df_vol_raw = pd.read_sql_query(query, conn, params=ticker_list)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    df_vol_raw['date'] = pd.to_datetime(df_vol_raw['date'], errors='coerce').dt.date\n",
    "    print('Volume table rows loaded:', len(df_vol_raw))\n",
    "    print('Volume table date range:', df_vol_raw['date'].min(), '->', df_vol_raw['date'].max())\n",
    "    df_vol_raw = df_vol_raw[df_vol_raw['date'].isin(window_dates)]\n",
    "    print('Volume window date range:', df_vol_raw['date'].min(), '->', df_vol_raw['date'].max(), 'rows:', len(df_vol_raw))\n",
    "    df_vol_raw['ticker'] = df_vol_raw['ticker'].str.upper()\n",
    "    for col in ['lit_buy', 'lit_sell', 'short_buy', 'short_sell']:\n",
    "        df_vol_raw[col] = pd.to_numeric(df_vol_raw[col], errors='coerce')\n",
    "    df_vol_raw = df_vol_raw.dropna(subset=['date'])\n",
    "    df_lit_daily = (\n",
    "        df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n",
    "        .agg({'lit_buy': 'sum', 'lit_sell': 'sum'})\n",
    "    )\n",
    "    df_lit_daily['lit_net'] = df_lit_daily['lit_buy'] - df_lit_daily['lit_sell']\n",
    "    df_short_daily = (\n",
    "        df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n",
    "        .agg({'short_buy': 'sum', 'short_sell': 'sum'})\n",
    "    )\n",
    "    df_short_daily['short_net'] = df_short_daily['short_buy'] - df_short_daily['short_sell']\n",
    "\n",
    "    vol_rows = []\n",
    "    for ticker in ticker_order:\n",
    "        df_t = df_vol_raw[df_vol_raw['ticker'] == ticker]\n",
    "        tail = tail_n(df_t, flow_days, END_DATE_RESOLVED)\n",
    "        if tail.empty:\n",
    "            vol_rows.append({\n",
    "                'ticker': ticker,\n",
    "                'lit_buy_sum': None,\n",
    "                'lit_sell_sum': None,\n",
    "                'short_buy_sum': None,\n",
    "                'short_sell_sum': None,\n",
    "                'volume_samples': 0,\n",
    "            })\n",
    "            continue\n",
    "        vol_rows.append({\n",
    "            'ticker': ticker,\n",
    "            'lit_buy_sum': float(tail['lit_buy'].sum(skipna=True)),\n",
    "            'lit_sell_sum': float(tail['lit_sell'].sum(skipna=True)),\n",
    "            'short_buy_sum': float(tail['short_buy'].sum(skipna=True)),\n",
    "            'short_sell_sum': float(tail['short_sell'].sum(skipna=True)),\n",
    "            'volume_samples': len(tail),\n",
    "        })\n",
    "\n",
    "    df_volume = pd.DataFrame(vol_rows)\n",
    "    total_samples = int(df_volume['volume_samples'].sum()) if not df_volume.empty else 0\n",
    "    lit_total = float(df_volume['lit_buy_sum'].fillna(0).sum() + df_volume['lit_sell_sum'].fillna(0).sum()) if not df_volume.empty else 0.0\n",
    "    short_total = float(df_volume['short_buy_sum'].fillna(0).sum() + df_volume['short_sell_sum'].fillna(0).sum()) if not df_volume.empty else 0.0\n",
    "    if total_samples == 0 or (lit_total == 0 and short_total == 0):\n",
    "        print('Volume data missing for selected period. Lit/short chords require non-zero buy/sell volume data.')\n",
    "        raise SystemExit\n",
    "else:\n",
    "    df_volume = pd.DataFrame(columns=[\n",
    "        'ticker', 'lit_buy_sum', 'lit_sell_sum', 'short_buy_sum', 'short_sell_sum', 'volume_samples'\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "def build_edges_from_value(df, value_col, top_k_winners, top_k_losers):\n",
    "    df = df[['ticker', value_col]].dropna().copy()\n",
    "    winners = df[df[value_col] > 0].nlargest(top_k_winners, value_col)\n",
    "    losers = df[df[value_col] < 0].copy()\n",
    "    losers['supply'] = -losers[value_col]\n",
    "    losers = losers.nlargest(top_k_losers, 'supply')\n",
    "\n",
    "    edges = []\n",
    "    total_demand = winners[value_col].sum() if not winners.empty else 0.0\n",
    "    total_supply = losers['supply'].sum() if not losers.empty else 0.0\n",
    "\n",
    "    if winners.empty or losers.empty:\n",
    "        return pd.DataFrame(edges), winners, losers, total_demand, total_supply\n",
    "\n",
    "    if DISTRIBUTION_MODE not in {'equal', 'demand_weighted'}:\n",
    "        raise ValueError('DISTRIBUTION_MODE must be \"equal\" or \"demand_weighted\"')\n",
    "\n",
    "    if DISTRIBUTION_MODE == 'equal':\n",
    "        for _, loser in losers.iterrows():\n",
    "            flow_each = loser['supply'] / len(winners)\n",
    "            for _, winner in winners.iterrows():\n",
    "                if flow_each >= MIN_EDGE_FLOW:\n",
    "                    edges.append({\n",
    "                        'source': loser['ticker'],\n",
    "                        'dest': winner['ticker'],\n",
    "                        'flow': float(flow_each),\n",
    "                    })\n",
    "    else:\n",
    "        if total_demand > 0:\n",
    "            for _, loser in losers.iterrows():\n",
    "                for _, winner in winners.iterrows():\n",
    "                    flow = loser['supply'] * (winner[value_col] / total_demand)\n",
    "                    if flow >= MIN_EDGE_FLOW:\n",
    "                        edges.append({\n",
    "                            'source': loser['ticker'],\n",
    "                            'dest': winner['ticker'],\n",
    "                            'flow': float(flow),\n",
    "                        })\n",
    "\n",
    "    edges_df = pd.DataFrame(edges)\n",
    "    if not edges_df.empty:\n",
    "        edges_df = edges_df.sort_values('flow', ascending=False).reset_index(drop=True)\n",
    "    return edges_df, winners, losers, total_demand, total_supply\n",
    "\n",
    "\n",
    "def compute_strand_unit(edges_df, base_unit, max_strands):\n",
    "    base = base_unit if base_unit and base_unit > 0 else 1.0\n",
    "    if edges_df.empty or not max_strands or max_strands <= 0:\n",
    "        return base\n",
    "    total_flow = edges_df['flow'].sum()\n",
    "    if total_flow <= 0:\n",
    "        return base\n",
    "    return max(base, total_flow / max_strands)\n",
    "\n",
    "\n",
    "def estimate_strands(edges_df, unit):\n",
    "    if edges_df.empty:\n",
    "        return 0\n",
    "    u = unit if unit and unit > 0 else 1.0\n",
    "    return int(np.maximum(1, np.floor(edges_df['flow'] / u)).sum())\n",
    "\n",
    "\n",
    "def split_strands(edges_df, unit):\n",
    "    strand_rows = []\n",
    "    u = unit if unit and unit > 0 else 1.0\n",
    "    for _, row in edges_df.iterrows():\n",
    "        flow = row['flow']\n",
    "        n = max(1, int(np.floor(flow / u)))\n",
    "        weight = flow / n\n",
    "        for _ in range(n):\n",
    "            strand_rows.append({\n",
    "                'source': row['source'],\n",
    "                'dest': row['dest'],\n",
    "                'flow': weight,\n",
    "            })\n",
    "    return pd.DataFrame(strand_rows)\n",
    "\n",
    "\n",
    "print('Auto strand scaling:', AUTO_STRAND_SCALING, 'MAX_STRANDS:', MAX_STRANDS)\n",
    "\n",
    "# Accumulation flow\n",
    "if df_scores['A_end'].notna().any() and df_scores['A_start'].notna().any():\n",
    "    df_scores['delta'] = df_scores['A_end'] - df_scores['A_start']\n",
    "else:\n",
    "    df_scores['delta'] = np.nan\n",
    "\n",
    "df_scores['role'] = np.where(\n",
    "    df_scores['delta'] > 0,\n",
    "    'winner',\n",
    "    np.where(df_scores['delta'] < 0, 'loser', 'neutral')\n",
    ")\n",
    "\n",
    "df_scores_sorted = df_scores.sort_values(\n",
    "    by='delta', key=lambda s: s.abs(), ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "accum_edges_df, accum_winners, accum_losers, accum_demand, accum_supply = build_edges_from_value(\n",
    "    df_scores, 'delta', TOP_K_WINNERS, TOP_K_LOSERS\n",
    ")\n",
    "accum_unit = compute_strand_unit(accum_edges_df, STRAND_UNIT, MAX_STRANDS) if AUTO_STRAND_SCALING else STRAND_UNIT\n",
    "accum_strands_df = split_strands(accum_edges_df, accum_unit)\n",
    "print(\"Accum winners/losers:\", len(accum_winners), len(accum_losers), \"edges:\", len(accum_edges_df))\n",
    "print(\"Accum strand unit:\", accum_unit, \"estimated:\", estimate_strands(accum_edges_df, accum_unit), \"actual:\", len(accum_strands_df))\n",
    "\n",
    "# Lit and Short flows (net buy - sell)\n",
    "if VOLUME_DATA_AVAILABLE and not df_volume.empty:\n",
    "    df_volume = df_volume.copy()\n",
    "    df_volume['lit_net'] = df_volume['lit_buy_sum'] - df_volume['lit_sell_sum']\n",
    "    df_volume['short_net'] = df_volume['short_buy_sum'] - df_volume['short_sell_sum']\n",
    "\n",
    "    lit_edges_df, lit_winners, lit_losers, lit_demand, lit_supply = build_edges_from_value(\n",
    "        df_volume, 'lit_net', TOP_K_WINNERS, TOP_K_LOSERS\n",
    "    )\n",
    "    short_edges_df, short_winners, short_losers, short_demand, short_supply = build_edges_from_value(\n",
    "        df_volume, 'short_net', TOP_K_WINNERS, TOP_K_LOSERS\n",
    "    )\n",
    "\n",
    "    lit_unit = compute_strand_unit(lit_edges_df, STRAND_UNIT, MAX_STRANDS) if AUTO_STRAND_SCALING else STRAND_UNIT\n",
    "    short_unit = compute_strand_unit(short_edges_df, STRAND_UNIT, MAX_STRANDS) if AUTO_STRAND_SCALING else STRAND_UNIT\n",
    "\n",
    "    lit_strands_df = split_strands(lit_edges_df, lit_unit)\n",
    "    short_strands_df = split_strands(short_edges_df, short_unit)\n",
    "\n",
    "    total_strands = len(accum_strands_df) + len(lit_strands_df) + len(short_strands_df)\n",
    "    if total_strands > MAX_STRANDS * 2:\n",
    "        print(\"WARNING: large strand count:\", total_strands, \"Consider increasing STRAND_UNIT or MAX_STRANDS.\")\n",
    "\n",
    "    print(\"Lit winners/losers:\", len(lit_winners), len(lit_losers), \"edges:\", len(lit_edges_df))\n",
    "    print(\"Short winners/losers:\", len(short_winners), len(short_losers), \"edges:\", len(short_edges_df))\n",
    "    print(\"Lit strand unit:\", lit_unit, \"estimated:\", estimate_strands(lit_edges_df, lit_unit), \"actual:\", len(lit_strands_df))\n",
    "    print(\"Short strand unit:\", short_unit, \"estimated:\", estimate_strands(short_edges_df, short_unit), \"actual:\", len(short_strands_df))\n",
    "else:\n",
    "    lit_edges_df = pd.DataFrame()\n",
    "    short_edges_df = pd.DataFrame()\n",
    "    lit_strands_df = pd.DataFrame()\n",
    "    short_strands_df = pd.DataFrame()\n",
    "    lit_winners = pd.DataFrame()\n",
    "    short_winners = pd.DataFrame()\n",
    "    lit_losers = pd.DataFrame()\n",
    "    short_losers = pd.DataFrame()\n",
    "    lit_demand = lit_supply = 0.0\n",
    "    short_demand = short_supply = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "display(df_scores_sorted[['ticker', 'category', 'A_end', 'A_start', 'delta', 'role']])\n",
    "\n",
    "if SHOW_ACCUM_FLOW:\n",
    "    if accum_edges_df.empty:\n",
    "        display(pd.DataFrame(columns=['source', 'dest', 'flow']))\n",
    "    else:\n",
    "        display(accum_edges_df[['source', 'dest', 'flow']])\n",
    "\n",
    "print('Volume availability:', VOLUME_DATA_AVAILABLE, 'rows:', len(df_volume))\n",
    "if VOLUME_DATA_AVAILABLE and not df_volume.empty:\n",
    "    display(df_volume[['ticker', 'lit_buy_sum', 'lit_sell_sum', 'short_buy_sum', 'short_sell_sum']])\n",
    "    if (df_volume['lit_buy_sum'].fillna(0).sum() == 0 and df_volume['lit_sell_sum'].fillna(0).sum() == 0):\n",
    "        print('Note: lit volumes sum to zero across selected period.')\n",
    "else:\n",
    "    print('Volume data not available for lit/short buy/sell flows.')\n",
    "\n",
    "summary_parts = [\n",
    "    f'END_DATE={END_DATE_RESOLVED}',\n",
    "    f'FLOW_PERIOD_DAYS={FLOW_PERIOD_DAYS}',\n",
    "    f'accum_supply={accum_supply:.2f}',\n",
    "    f'accum_demand={accum_demand:.2f}',\n",
    "]\n",
    "if SHOW_LIT_FLOW:\n",
    "    summary_parts.append(f'lit_supply={lit_supply:.2f}')\n",
    "    summary_parts.append(f'lit_demand={lit_demand:.2f}')\n",
    "if SHOW_SHORT_FLOW:\n",
    "    summary_parts.append(f'short_supply={short_supply:.2f}')\n",
    "    summary_parts.append(f'short_demand={short_demand:.2f}')\n",
    "\n",
    "summary_parts.append(f'accum_edges={len(accum_edges_df)}')\n",
    "summary_parts.append(f'lit_edges={len(lit_edges_df)}')\n",
    "summary_parts.append(f'short_edges={len(short_edges_df)}')\n",
    "summary_parts.append(\n",
    "    f'strands={len(accum_strands_df) + len(lit_strands_df) + len(short_strands_df)}'\n",
    ")\n",
    "\n",
    "print(' | '.join(summary_parts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection, PolyCollection\n",
    "from matplotlib.colors import to_rgba\n",
    "from matplotlib.patches import Wedge, PathPatch, Rectangle\n",
    "from matplotlib.path import Path\n",
    "\n",
    "BG_COLOR = '#0b0f1a'\n",
    "\n",
    "CATEGORY_LABELS = {\n",
    "    'GLOBAL_MACRO': 'GLOBAL MACRO',\n",
    "    'MAG8': 'MAG8',\n",
    "    'SECTOR_SUMMARY': 'Sector Summary',\n",
    "    'SECTOR_CORE': 'SECTORS',\n",
    "    'COMMODITIES': 'COMMODITIES',\n",
    "}\n",
    "CATEGORY_PALETTE = {\n",
    "    'GLOBAL_MACRO': \"#00AAFF\",\n",
    "    'MAG8': '#DDA0FF',\n",
    "    'SECTOR_SUMMARY': \"#72ADAF\",\n",
    "    'SECTOR_CORE': '#F6C453',\n",
    "    'COMMODITIES': '#7CDE8A',\n",
    "    'UNKNOWN': '#A0A0A0',\n",
    "}\n",
    "\n",
    "METRIC_ORDER = ['accum', 'short', 'lit']\n",
    "BAND_ORDER = ['lit', 'accum', 'short']  # left -> center -> right on each ticker\n",
    "METRIC_LABELS = {\n",
    "    'accum': 'Accumulation',\n",
    "    'short': 'Daily Short',\n",
    "    'lit': 'Lit',\n",
    "}\n",
    "METRIC_COLORS = {\n",
    "    'accum': {'sell': \"#8304B9\", 'buy': \"#26FF00\"},\n",
    "    'short': {'sell': \"#FF1E1E\", 'buy': \"#26FF00\"},\n",
    "    'lit': {'sell': \"#0B2CFF\", 'buy': \"#26FF00\"},\n",
    "}\n",
    "\n",
    "\n",
    "def blend_color(c1, c2, t=0.5):\n",
    "    a = np.array(to_rgba(c1))\n",
    "    b = np.array(to_rgba(c2))\n",
    "    return a * (1 - t) + b * t\n",
    "\n",
    "\n",
    "def soften_color(color, amount, base=BG_COLOR):\n",
    "    amt = max(0.0, min(1.0, amount))\n",
    "    return blend_color(color, base, amt)\n",
    "\n",
    "\n",
    "def add_gradient_curve(ax, points, color_start, color_end, lw, alpha):\n",
    "    if len(points) < 2:\n",
    "        return\n",
    "    segments = np.stack([points[:-1], points[1:]], axis=1)\n",
    "    c0 = np.array(to_rgba(color_start))\n",
    "    c1 = np.array(to_rgba(color_end))\n",
    "    t = np.linspace(0, 1, len(segments))[:, None]\n",
    "    colors = c0 * (1 - t) + c1 * t\n",
    "    colors[:, 3] = colors[:, 3] * alpha\n",
    "    lc = LineCollection(segments, colors=colors, linewidths=lw, capstyle='round')\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "\n",
    "def arc_points(a0, a1, r, n=None):\n",
    "    count = n or CHORD_ARC_POINTS\n",
    "    angles = np.linspace(a0, a1, count)\n",
    "    return np.column_stack([r * np.cos(angles), r * np.sin(angles)])\n",
    "\n",
    "\n",
    "def bezier_curve(p0, p1, p2, n=None):\n",
    "    count = n or CHORD_CURVE_POINTS\n",
    "    t = np.linspace(0, 1, count)[:, None]\n",
    "    return (1 - t) ** 2 * p0 + 2 * (1 - t) * t * p1 + t ** 2 * p2\n",
    "\n",
    "\n",
    "def ribbon_patch(a0, a1, b0, b1, r, color_start, color_end, alpha):\n",
    "    arc1 = arc_points(a0, a1, r, n=16)\n",
    "    arc2 = arc_points(b0, b1, r, n=16)\n",
    "    curve1 = bezier_curve(arc1[-1], np.array([0.0, 0.0]), arc2[0], n=24)\n",
    "    curve2 = bezier_curve(arc2[-1], np.array([0.0, 0.0]), arc1[0], n=24)\n",
    "    poly = np.vstack([arc1, curve1, arc2, curve2])\n",
    "    codes = [Path.MOVETO] + [Path.LINETO] * (len(poly) - 1)\n",
    "    path = Path(poly, codes)\n",
    "    mid = blend_color(color_start, color_end, 0.5)\n",
    "    return PathPatch(path, facecolor=mid, edgecolor='none', alpha=alpha)\n",
    "\n",
    "\n",
    "def gradient_fill_collection(a0, a1, b0, b1, r, color_start, color_end, alpha, steps=None):\n",
    "    steps = steps or CHORD_GRADIENT_STEPS\n",
    "    arc1 = arc_points(a0, a1, r, n=4)\n",
    "    arc2 = arc_points(b0, b1, r, n=4)\n",
    "    p_a0, p_a1 = arc1[0], arc1[-1]\n",
    "    p_b0, p_b1 = arc2[0], arc2[-1]\n",
    "    left = bezier_curve(p_a0, np.array([0.0, 0.0]), p_b0, n=steps + 1)\n",
    "    right = bezier_curve(p_a1, np.array([0.0, 0.0]), p_b1, n=steps + 1)\n",
    "    polys = []\n",
    "    colors = []\n",
    "    for i in range(steps):\n",
    "        quad = np.vstack([left[i], left[i + 1], right[i + 1], right[i]])\n",
    "        t = (i + 0.5) / steps\n",
    "        color = blend_color(color_start, color_end, t)\n",
    "        color[3] = color[3] * alpha\n",
    "        polys.append(quad)\n",
    "        colors.append(color)\n",
    "    return PolyCollection(polys, facecolors=colors, edgecolors='none')\n",
    "\n",
    "\n",
    "def draw_ribbon(ax, a0, a1, b0, b1, r, color_start, color_end, fill_alpha, line_alpha, lw):\n",
    "    if USE_GRADIENT_FILL:\n",
    "        clip = ribbon_patch(a0, a1, b0, b1, r, color_start, color_end, alpha=1.0)\n",
    "        clip.set_facecolor('none')\n",
    "        clip.set_edgecolor('none')\n",
    "        ax.add_patch(clip)\n",
    "\n",
    "        fill = gradient_fill_collection(a0, a1, b0, b1, r, color_start, color_end, fill_alpha)\n",
    "        fill.set_clip_path(clip)\n",
    "        ax.add_collection(fill)\n",
    "    else:\n",
    "        patch = ribbon_patch(a0, a1, b0, b1, r, color_start, color_end, alpha=fill_alpha)\n",
    "        ax.add_patch(patch)\n",
    "\n",
    "    mid_a = (a0 + a1) / 2\n",
    "    mid_b = (b0 + b1) / 2\n",
    "    p0 = np.array([r * np.cos(mid_a), r * np.sin(mid_a)])\n",
    "    p2 = np.array([r * np.cos(mid_b), r * np.sin(mid_b)])\n",
    "    center = np.array([0.0, 0.0])\n",
    "    curve = bezier_curve(p0, center, p2)\n",
    "    add_gradient_curve(ax, curve, color_start, color_end, lw=lw, alpha=line_alpha)\n",
    "\n",
    "\n",
    "def make_time_bins(dates, bins):\n",
    "    if not dates:\n",
    "        return []\n",
    "    if bins is None or bins <= 0:\n",
    "        return [dates]\n",
    "    bins = min(bins, len(dates))\n",
    "    split = np.array_split(dates, bins)\n",
    "    return [list(s) for s in split if len(s)]\n",
    "\n",
    "\n",
    "def compute_metric_totals(edges_df):\n",
    "    totals = {}\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return totals\n",
    "    for row in edges_df.itertuples():\n",
    "        totals[row.source] = totals.get(row.source, 0.0) + row.flow\n",
    "        totals[row.dest] = totals.get(row.dest, 0.0) + row.flow\n",
    "    return totals\n",
    "\n",
    "\n",
    "def filter_edges(edges_df):\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return edges_df\n",
    "    df = edges_df.copy()\n",
    "    if MAX_EDGES_PER_METRIC and MAX_EDGES_PER_METRIC > 0:\n",
    "        df = df.nlargest(MAX_EDGES_PER_METRIC, 'flow')\n",
    "    return df\n",
    "\n",
    "\n",
    "def allocate_intervals(edges_df, band_map, metric_key):\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return []\n",
    "    out_totals = edges_df.groupby('source')['flow'].sum().to_dict()\n",
    "    in_totals = edges_df.groupby('dest')['flow'].sum().to_dict()\n",
    "\n",
    "    cursors_out = {}\n",
    "    cursors_in = {}\n",
    "    for ticker in ticker_order:\n",
    "        spans = band_map.get(ticker, {}).get(metric_key)\n",
    "        if not spans:\n",
    "            continue\n",
    "        cursors_out[ticker] = spans['out'][0]\n",
    "        cursors_in[ticker] = spans['in'][0]\n",
    "\n",
    "    intervals = []\n",
    "    for row in edges_df.sort_values('flow', ascending=False).itertuples():\n",
    "        src = row.source\n",
    "        dst = row.dest\n",
    "        flow = row.flow\n",
    "        if src not in band_map or dst not in band_map:\n",
    "            continue\n",
    "        out_range = band_map[src][metric_key]['out']\n",
    "        in_range = band_map[dst][metric_key]['in']\n",
    "        if out_totals.get(src, 0.0) <= 0 or in_totals.get(dst, 0.0) <= 0:\n",
    "            continue\n",
    "        out_len = (out_range[1] - out_range[0]) * (flow / out_totals[src])\n",
    "        in_len = (in_range[1] - in_range[0]) * (flow / in_totals[dst])\n",
    "        a0 = cursors_out[src]\n",
    "        a1 = min(out_range[1], a0 + out_len)\n",
    "        b0 = cursors_in[dst]\n",
    "        b1 = min(in_range[1], b0 + in_len)\n",
    "        cursors_out[src] = a1\n",
    "        cursors_in[dst] = b1\n",
    "        if a1 > a0 and b1 > b0:\n",
    "            intervals.append({'source': src, 'dest': dst, 'flow': flow, 'a0': a0, 'a1': a1, 'b0': b0, 'b1': b1})\n",
    "    return intervals\n",
    "\n",
    "\n",
    "# Prepare metric datasets\n",
    "def metric_visible(metric_key):\n",
    "    if metric_key == \"accum\":\n",
    "        return SHOW_ACCUM_FLOW\n",
    "    if metric_key == \"short\":\n",
    "        return SHOW_SHORT_FLOW\n",
    "    if metric_key == \"lit\":\n",
    "        return SHOW_LIT_FLOW\n",
    "    return True\n",
    "\n",
    "accum_edges_plot = filter_edges(accum_edges_df)\n",
    "short_edges_plot = filter_edges(short_edges_df)\n",
    "lit_edges_plot = filter_edges(lit_edges_df)\n",
    "\n",
    "metric_edges = {\n",
    "    'accum': accum_edges_plot,\n",
    "    'short': short_edges_plot,\n",
    "    'lit': lit_edges_plot,\n",
    "}\n",
    "\n",
    "metric_nets = {\n",
    "    'accum': df_scores.set_index('ticker')['delta'].to_dict(),\n",
    "    'short': df_volume.set_index('ticker')['short_net'].to_dict() if 'short_net' in df_volume.columns else {},\n",
    "    'lit': df_volume.set_index('ticker')['lit_net'].to_dict() if 'lit_net' in df_volume.columns else {},\n",
    "}\n",
    "\n",
    "df_accum_level = df_raw_full[df_raw_full['date'].isin(window_dates)][['ticker', 'date', 'accumulation_score']].copy()\n",
    "if not df_accum_level.empty:\n",
    "    df_accum_level = df_accum_level.rename(columns={'accumulation_score': 'value'})\n",
    "\n",
    "metric_daily = {\n",
    "    'accum': df_accum_level,\n",
    "    'short': df_short_daily.rename(columns={'short_net': 'net'}),\n",
    "    'lit': df_lit_daily.rename(columns={'lit_net': 'net'}),\n",
    "}\n",
    "\n",
    "# Build ticker layout (grouped by category, preserving order)\n",
    "grouped = {\n",
    "    'GLOBAL_MACRO': [t for t in ticker_order if ticker_category.get(t) == 'GLOBAL_MACRO'],\n",
    "    'MAG8': [t for t in ticker_order if ticker_category.get(t) == 'MAG8'],\n",
    "    'SECTOR_SUMMARY': [t for t in ticker_order if ticker_category.get(t) == 'SECTOR_SUMMARY'],\n",
    "    'SECTOR_CORE': [t for t in ticker_order if ticker_category.get(t) == 'SECTOR_CORE'],\n",
    "    'COMMODITIES': [t for t in ticker_order if ticker_category.get(t) == 'COMMODITIES'],\n",
    "}\n",
    "\n",
    "metric_totals = {m: compute_metric_totals(metric_edges[m]) for m in METRIC_ORDER}\n",
    "\n",
    "total_nodes = sum(len(v) for v in grouped.values())\n",
    "if total_nodes == 0:\n",
    "    print('No nodes to plot.')\n",
    "else:\n",
    "    gap = 0.12\n",
    "    total_gap = gap * len([g for g in grouped.values() if g])\n",
    "    usable = 2 * math.pi - total_gap\n",
    "    if usable <= 0:\n",
    "        usable = 2 * math.pi\n",
    "    step = usable / total_nodes\n",
    "    arc_span = step * 0.85\n",
    "\n",
    "    angles = {}\n",
    "    spans = {}\n",
    "    angle = 0.0\n",
    "    for cat in ['GLOBAL_MACRO', 'MAG8', 'SECTOR_SUMMARY', 'SECTOR_CORE', 'COMMODITIES']:\n",
    "        if not grouped[cat]:\n",
    "            continue\n",
    "        angle += gap / 2\n",
    "        for t in grouped[cat]:\n",
    "            angles[t] = angle\n",
    "            spans[t] = (angle - arc_span / 2, angle + arc_span / 2)\n",
    "            angle += step\n",
    "        angle += gap / 2\n",
    "\n",
    "    # Metric bands per ticker\n",
    "    band_map = {}\n",
    "    for t, (a0, a1) in spans.items():\n",
    "        max_span = (a1 - a0)\n",
    "        chord_span = min(max_span, max_span * CHORD_ARC_FRACTION)\n",
    "        chord_center = (a0 + a1) / 2\n",
    "        chord_start = chord_center - chord_span / 2\n",
    "        chord_end = chord_center + chord_span / 2\n",
    "        band_gap = chord_span * BAND_GAP_FRAC\n",
    "\n",
    "        if METRIC_BAND_MODE == 'proportional':\n",
    "            weights = {m: metric_totals[m].get(t, 0.0) for m in BAND_ORDER}\n",
    "            if sum(weights.values()) <= 0:\n",
    "                weights = {m: 1.0 for m in BAND_ORDER}\n",
    "        else:\n",
    "            weights = {m: 1.0 for m in BAND_ORDER}\n",
    "\n",
    "        total_w = sum(weights.values())\n",
    "        if total_w <= 0:\n",
    "            weights = {m: 1.0 for m in BAND_ORDER}\n",
    "            total_w = sum(weights.values())\n",
    "\n",
    "        available = chord_span - band_gap * 2\n",
    "        if available <= 0:\n",
    "            band_gap = 0.0\n",
    "            available = chord_span\n",
    "\n",
    "        lengths = {m: max(0.0, available * (weights[m] / total_w)) for m in BAND_ORDER}\n",
    "        acc_len = lengths.get('accum', available / 3)\n",
    "        lit_len = lengths.get('lit', available / 3)\n",
    "        short_len = lengths.get('short', available / 3)\n",
    "\n",
    "        # Keep accumulation centered on the ticker arc; trim side bands if needed.\n",
    "        max_acc = max(0.0, chord_span - 2 * band_gap)\n",
    "        if acc_len > max_acc:\n",
    "            acc_len = max_acc\n",
    "        side_capacity = max(0.0, (chord_span - acc_len - 2 * band_gap) / 2)\n",
    "        if side_capacity <= 0:\n",
    "            lit_len = 0.0\n",
    "            short_len = 0.0\n",
    "        else:\n",
    "            if lit_len > side_capacity or short_len > side_capacity:\n",
    "                scale = min(1.0, side_capacity / max(lit_len, short_len))\n",
    "                lit_len *= scale\n",
    "                short_len *= scale\n",
    "\n",
    "        acc_start = chord_center - acc_len / 2\n",
    "        acc_end = chord_center + acc_len / 2\n",
    "        lit_end = acc_start - band_gap\n",
    "        lit_start = lit_end - lit_len\n",
    "        short_start = acc_end + band_gap\n",
    "        short_end = short_start + short_len\n",
    "\n",
    "        def band_slices(start, end):\n",
    "            dir_gap = (end - start) * DIR_GAP_FRAC\n",
    "            dir_gap = min(dir_gap, (end - start) * 0.4)\n",
    "            half = max(0.0, (end - start - dir_gap) / 2)\n",
    "            out_start = start\n",
    "            out_end = start + half\n",
    "            in_start = out_end + dir_gap\n",
    "            in_end = end\n",
    "            return {\n",
    "                'band': (start, end),\n",
    "                'out': (out_start, out_end),\n",
    "                'in': (in_start, in_end),\n",
    "            }\n",
    "\n",
    "        band_map[t] = {\n",
    "            'lit': band_slices(lit_start, lit_end),\n",
    "            'accum': band_slices(acc_start, acc_end),\n",
    "            'short': band_slices(short_start, short_end),\n",
    "        }\n",
    "\n",
    "    # Prepare intervals per metric\n",
    "    metric_intervals = {}\n",
    "    for m in METRIC_ORDER:\n",
    "        if not metric_visible(m):\n",
    "            metric_intervals[m] = []\n",
    "            continue\n",
    "        metric_intervals[m] = allocate_intervals(metric_edges[m], band_map, m)\n",
    "\n",
    "    # Time bins for outer ring\n",
    "    window_dates_sorted = sorted(window_dates)\n",
    "    time_bins = make_time_bins(window_dates_sorted, TIME_SLICE_BINS)\n",
    "    time_bins = list(reversed(time_bins))\n",
    "\n",
    "    # Pre-compute normalization for ring rendering\n",
    "    metric_max_mag = {}\n",
    "    metric_min_val = {}\n",
    "    for m in METRIC_ORDER:\n",
    "        df_m = metric_daily[m]\n",
    "        if df_m is None or df_m.empty:\n",
    "            metric_max_mag[m] = 1.0\n",
    "            metric_min_val[m] = 0.0\n",
    "            continue\n",
    "        if m == 'accum' and 'value' in df_m.columns and df_m['value'].notna().any():\n",
    "            metric_min_val[m] = float(df_m['value'].min())\n",
    "            metric_max_mag[m] = float(df_m['value'].max())\n",
    "        else:\n",
    "            metric_min_val[m] = 0.0\n",
    "            max_mag = float(df_m['net'].abs().max()) if df_m['net'].notna().any() else 1.0\n",
    "            metric_max_mag[m] = max(1.0, max_mag)\n",
    "\n",
    "    # Build bin value/net lookup\n",
    "    metric_bin_net = {m: {} for m in METRIC_ORDER}\n",
    "    for m in METRIC_ORDER:\n",
    "        df_m = metric_daily[m]\n",
    "        if df_m is None or df_m.empty:\n",
    "            for t in ticker_order:\n",
    "                metric_bin_net[m][t] = [0.0 for _ in time_bins]\n",
    "            continue\n",
    "        for t in ticker_order:\n",
    "            net_by_bin = []\n",
    "            for bin_dates in time_bins:\n",
    "                mask = (df_m['ticker'] == t) & (df_m['date'].isin(bin_dates))\n",
    "                if m == 'accum' and 'value' in df_m.columns:\n",
    "                    val = float(df_m.loc[mask, 'value'].mean()) if mask.any() else 0.0\n",
    "                    net_by_bin.append(val)\n",
    "                else:\n",
    "                    net = float(df_m.loc[mask, 'net'].sum()) if mask.any() else 0.0\n",
    "                    net_by_bin.append(net)\n",
    "            metric_bin_net[m][t] = net_by_bin\n",
    "\n",
    "    # Start plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 12), subplot_kw={'aspect': 'equal'})\n",
    "    fig.patch.set_facecolor(BG_COLOR)\n",
    "    ax.set_facecolor(BG_COLOR)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Base circle\n",
    "    theta = np.linspace(0, 2 * math.pi, 400)\n",
    "    ax.plot(np.cos(theta), np.sin(theta), color='#39424e', lw=1.0, alpha=0.6)\n",
    "\n",
    "    # Ticker arcs\n",
    "    if SHOW_TICKER_ARC:\n",
    "        ticker_outer = 1.05\n",
    "        ticker_width = 0.035\n",
    "        for t, (a0, a1) in spans.items():\n",
    "            wedge = Wedge(\n",
    "                (0, 0), ticker_outer, math.degrees(a0), math.degrees(a1),\n",
    "                width=ticker_width,\n",
    "                facecolor=CATEGORY_PALETTE.get(ticker_category.get(t, 'UNKNOWN'), '#A0A0A0'),\n",
    "                edgecolor='#222831', lw=0.4, alpha=0.9,\n",
    "            )\n",
    "            ax.add_patch(wedge)\n",
    "    else:\n",
    "        ticker_outer = 1.02\n",
    "        ticker_width = 0.02\n",
    "\n",
    "    # Outer rings (time-sliced)\n",
    "    if SHOW_VOLUME_RING:\n",
    "        track_span = RING_BASE_THICKNESS + RING_THICKNESS_SCALE + RING_GAP\n",
    "        for idx, m in enumerate(METRIC_ORDER):\n",
    "            inner_base = ticker_outer + 0.02 + idx * track_span\n",
    "            sell_color = METRIC_COLORS[m]['sell']\n",
    "            buy_color = METRIC_COLORS[m]['buy']\n",
    "            max_mag = metric_max_mag[m]\n",
    "            for t, (a0, a1) in spans.items():\n",
    "                bin_nets = metric_bin_net[m].get(t, [])\n",
    "                if not bin_nets:\n",
    "                    continue\n",
    "                slice_gap = (a1 - a0) * 0.02\n",
    "                total_slice = (a1 - a0) - slice_gap * (len(bin_nets) - 1)\n",
    "                if total_slice <= 0:\n",
    "                    total_slice = (a1 - a0)\n",
    "                    slice_gap = 0.0\n",
    "                slice_len = total_slice / len(bin_nets)\n",
    "                cursor = a0\n",
    "                for net in bin_nets:\n",
    "                    if m == 'accum':\n",
    "                        vmin = metric_min_val.get(m, 0.0)\n",
    "                        vmax = metric_max_mag.get(m, 1.0)\n",
    "                        denom = max(1e-9, vmax - vmin)\n",
    "                        tnorm = max(0.0, min(1.0, (net - vmin) / denom))\n",
    "                        thickness = RING_BASE_THICKNESS + RING_THICKNESS_SCALE * tnorm\n",
    "                        color = blend_color(sell_color, buy_color, tnorm)\n",
    "                    else:\n",
    "                        mag = abs(net)\n",
    "                        thickness = RING_BASE_THICKNESS + RING_THICKNESS_SCALE * (mag / max_mag if max_mag > 0 else 0.0)\n",
    "                        color = buy_color if net >= 0 else sell_color\n",
    "                    start_deg = math.degrees(cursor)\n",
    "                    end_deg = math.degrees(cursor + slice_len)\n",
    "                    wedge = Wedge(\n",
    "                        (0, 0), inner_base + thickness, start_deg, end_deg,\n",
    "                        width=thickness,\n",
    "                        facecolor=color, edgecolor='none', alpha=0.85,\n",
    "                    )\n",
    "                    ax.add_patch(wedge)\n",
    "                    cursor += slice_len + slice_gap\n",
    "\n",
    "    # Draw chords per metric\n",
    "    for m in METRIC_ORDER:\n",
    "        if not metric_visible(m):\n",
    "            continue\n",
    "        intervals = metric_intervals[m]\n",
    "        if not intervals:\n",
    "            continue\n",
    "        raw_start = METRIC_COLORS[m]['sell']\n",
    "        raw_end = METRIC_COLORS[m]['buy']\n",
    "        color_start = soften_color(raw_start, CHORD_COLOR_SOFTEN)\n",
    "        color_end = soften_color(raw_end, CHORD_COLOR_SOFTEN)\n",
    "        max_flow = metric_edges[m]['flow'].max() if metric_edges[m] is not None and not metric_edges[m].empty else 1.0\n",
    "        for edge in intervals:\n",
    "            flow = edge['flow']\n",
    "            lw = 0.6 + 2.2 * ((flow / max_flow) ** 0.6) if max_flow > 0 else 1.0\n",
    "            draw_ribbon(ax, edge['a0'], edge['a1'], edge['b0'], edge['b1'], CHORD_RADIUS, color_start, color_end, alpha=0.4, lw=lw)\n",
    "\n",
    "    # Ticker labels\n",
    "    for t, ang in angles.items():\n",
    "        x = math.cos(ang)\n",
    "        y = math.sin(ang)\n",
    "        r = CHORD_RADIUS + (ticker_outer - CHORD_RADIUS) * 0.4\n",
    "        rot = math.degrees(ang)\n",
    "        if math.pi / 2 < ang < 3 * math.pi / 2:\n",
    "            rot += 180\n",
    "        ax.text(\n",
    "            r * x, r * y, t,\n",
    "            color='#FFFFFF', fontsize=9, fontweight='bold',\n",
    "            ha='center', va='center',\n",
    "            rotation=rot, rotation_mode='anchor'\n",
    "        )\n",
    "\n",
    "    # Category labels\n",
    "    for cat, tickers in grouped.items():\n",
    "        if not tickers:\n",
    "            continue\n",
    "        if cat == 'SECTOR_SUMMARY':\n",
    "            continue\n",
    "        mid_angle = np.mean([angles[t] for t in tickers])\n",
    "        ax.text(\n",
    "            (ticker_outer + 0.4) * math.cos(mid_angle),\n",
    "            (ticker_outer + 0.4) * math.sin(mid_angle),\n",
    "            CATEGORY_LABELS.get(cat, cat),\n",
    "            color=CATEGORY_PALETTE.get(cat, '#A0A0A0'),\n",
    "            fontsize=12, fontweight='bold',\n",
    "            ha='center', va='center'\n",
    "        )\n",
    "\n",
    "    # Title\n",
    "    fig.text(0.5, 0.97, 'Sector Money-Flow Chord Summary', ha='center', va='top', color='white', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Settings block\n",
    "    date_range = f\"{window_dates_sorted[0]} -> {window_dates_sorted[-1]}\" if window_dates_sorted else 'n/a'\n",
    "    fig.text(0.5, 0.945, f'Date Range: {date_range}', ha='center', va='top', color='#C9D1D9', fontsize=10)\n",
    "    settings_lines = [\n",
    "        f\"Date range: {date_range}\",\n",
    "        f\"Tickers: {len(ticker_order)} | FLOW_PERIOD_DAYS={FLOW_PERIOD_DAYS}\",\n",
    "        f\"Band mode: {METRIC_BAND_MODE} | MAX_EDGES_PER_METRIC={MAX_EDGES_PER_METRIC}\",\n",
    "        f\"STRAND_UNIT={STRAND_UNIT} | AUTO_STRAND_SCALING={AUTO_STRAND_SCALING} | MAX_STRANDS={MAX_STRANDS}\",\n",
    "        f\"Time bins: {len(time_bins)} | Ring norm: per-metric max\",\n",
    "    ]\n",
    "    settings = '\\n'.join(settings_lines)\n",
    "    fig.text(0.1, 0.05, settings, ha='left', va='bottom', color='#C9D1D9', fontsize=8)\n",
    "\n",
    "    # Legend\n",
    "    leg = fig.add_axes([0.1, 0.78, 0.28, 0.18])\n",
    "    leg.axis('off')\n",
    "    leg.set_facecolor('none')\n",
    "    leg.set_xlim(0, 1)\n",
    "    leg.set_ylim(0, 1)\n",
    "    y = 0.88\n",
    "    legend_order = ['lit', 'short', 'accum']\n",
    "    for m in legend_order:\n",
    "        xs = np.linspace(0.0, 0.25, 30)\n",
    "        ys = np.full_like(xs, y)\n",
    "        points = np.column_stack([xs, ys])\n",
    "        segments = np.stack([points[:-1], points[1:]], axis=1)\n",
    "        c0 = np.array(to_rgba(METRIC_COLORS[m]['sell']))\n",
    "        c1 = np.array(to_rgba(METRIC_COLORS[m]['buy']))\n",
    "        t = np.linspace(0, 1, len(segments))[:, None]\n",
    "        colors = c0 * (1 - t) + c1 * t\n",
    "        lc = LineCollection(segments, colors=colors, linewidths=4)\n",
    "        leg.add_collection(lc)\n",
    "        leg.text(0.3, y, f\"{METRIC_LABELS[m]} (sell->buy)\", color='white', fontsize=8, va='center')\n",
    "        y -= 0.18\n",
    "\n",
    "    # Category legend (top right)\n",
    "    cat_leg = fig.add_axes([0.80, 0.78, 0.16, 0.18])\n",
    "    cat_leg.axis('off')\n",
    "    cat_leg.set_facecolor('none')\n",
    "    cat_leg.set_xlim(0, 1)\n",
    "    cat_leg.set_ylim(0, 1)\n",
    "    cat_order = ['GLOBAL_MACRO', 'MAG8', 'SECTOR_SUMMARY', 'SECTOR_CORE', 'COMMODITIES']\n",
    "    y = 0.88\n",
    "    for cat in cat_order:\n",
    "        if cat not in CATEGORY_PALETTE:\n",
    "            continue\n",
    "        if not grouped.get(cat):\n",
    "            continue\n",
    "        label = CATEGORY_LABELS.get(cat, cat)\n",
    "        color = CATEGORY_PALETTE.get(cat, '#A0A0A0')\n",
    "        cat_leg.add_patch(Rectangle((0.0, y - 0.04), 0.08, 0.08, facecolor=color, edgecolor='none'))\n",
    "        cat_leg.text(0.12, y, label, color='white', fontsize=8, va='center')\n",
    "        y -= 0.18\n",
    "\n",
    "    # Top inflow/outflow callouts (table)\n",
    "    def top_tickers(net_map, positive=True, k=3):\n",
    "        if not net_map:\n",
    "            return 'n/a'\n",
    "        items = [(k, v) for k, v in net_map.items() if v is not None and np.isfinite(v)]\n",
    "        if positive:\n",
    "            items = [x for x in items if x[1] > 0]\n",
    "            items = sorted(items, key=lambda x: x[1], reverse=True)\n",
    "        else:\n",
    "            items = [x for x in items if x[1] < 0]\n",
    "            items = sorted(items, key=lambda x: x[1])\n",
    "        tickers = [k for k, _ in items[:k]]\n",
    "        return ', '.join(tickers) if tickers else 'n/a'\n",
    "\n",
    "    table_rows = [\n",
    "        ('Accumulation Score', top_tickers(metric_nets.get('accum', {}), True), top_tickers(metric_nets.get('accum', {}), False)),\n",
    "        ('Daily Short Buy/Sell Ratio', top_tickers(metric_nets.get('short', {}), True), top_tickers(metric_nets.get('short', {}), False)),\n",
    "        ('Lit Buy/Sell Ratio', top_tickers(metric_nets.get('lit', {}), True), top_tickers(metric_nets.get('lit', {}), False)),\n",
    "    ]\n",
    "\n",
    "    col1, col2, col3 = 26, 24, 24\n",
    "    header = f\"{'Metric':<{col1}}{'Buy (Top)':<{col2}}{'Sell (Top)':<{col3}}\"\n",
    "    divider = '-' * (col1 + col2 + col3)\n",
    "    table_lines = [header, divider]\n",
    "    for label, buy_str, sell_str in table_rows:\n",
    "        table_lines.append(f\"{label:<{col1}}{buy_str:<{col2}}{sell_str:<{col3}}\")\n",
    "\n",
    "    fig.text(\n",
    "        0.50, 0.05,\n",
    "        '\\n'.join(table_lines),\n",
    "        ha='left', va='bottom',\n",
    "        color='#C9D1D9', fontsize=9,\n",
    "        fontfamily='monospace',\n",
    "        linespacing=1.2,\n",
    "    )\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VWBR (finra_buy_volume_z) Heatmap\n",
    "\n",
    "Visualize FINRA buy volume z-scores across multiple ticker categories.\n",
    "\n",
    "**Color scheme**: Purple (negative z) <- Black (neutral) -> Neon Green (positive z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# USER CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Date range for heatmap\n",
    "START_DATE = \"2025-10-15\"\n",
    "END_DATE = \"2026-01-05\"\n",
    "\n",
    "# Ticker Types: [\"SECTOR\", \"SUMMARY\", \"GLOBAL\", \"COMMODITIES\", \"MAG8\", \"SPECULATIVE\", \"CRYPTO\"] or [\"ALL\"]\n",
    "TICKER_TYPES = [\"SECTOR\", \"SUMMARY\", \"GLOBAL\", \"COMMODITIES\", \"MAG8\"]\n",
    "\n",
    "# View Mode: \"indices\" (main ETF/index only) or \"constituents\" (expand to holdings)\n",
    "VIEW_MODE = \"indices\"  # or \"constituents\"\n",
    "\n",
    "# Z-Scale Mode: \"global\" (vmin/vmax across all tickers) or \"per_row\" (vmin/vmax per ticker row)\n",
    "Z_SCALE_MODE = \"global\"\n",
    "\n",
    "# Cell size in inches (for square cells)\n",
    "CELL_SIZE = 0.15\n",
    "\n",
    "# Clustering options\n",
    "ENABLE_CLUSTERING = True  # Toggle Y-axis clustering on/off\n",
    "SHOW_DENDROGRAM = True      # Show dendrogram tree on left side of heatmap\n",
    "\n",
    "# Cumulative subplots\n",
    "SHOW_CUMULATIVE = True      # Show cumulative +/- z-score flow below heatmap\n",
    "SHOW_BREADTH = True         # Show cumulative breadth (ticker count) subplot\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize, TwoSlopeNorm\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Clustering imports\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ticker dictionary from: c:\\Users\\fvign\\Dropbox\\Vscode\\darkpool\\Special_tools\\ticker_dictionary.py\n",
      "Built ticker list: 39 tickers from ['SECTOR', 'SUMMARY', 'GLOBAL', 'COMMODITIES', 'MAG8']\n",
      "Sample tickers: ['XLF', 'KRE', 'XLK', 'SMH', 'XLI', 'XLY', 'XLE', 'XLV', 'XLP', 'XLU']...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: TICKER DICTIONARY LOADING\n",
    "# =============================================================================\n",
    "\n",
    "def _load_ticker_dictionary():\n",
    "    \"\"\"Load ticker_dictionary.py dynamically from known locations.\"\"\"\n",
    "    candidates = [\n",
    "        Path.cwd() / \"ticker_dictionary.py\",\n",
    "        Path.cwd() / \"Special_tools\" / \"ticker_dictionary.py\",\n",
    "        Path.cwd() / \"darkpool\" / \"Special_tools\" / \"ticker_dictionary.py\",\n",
    "    ]\n",
    "    \n",
    "    for path in candidates:\n",
    "        if path.exists():\n",
    "            spec = importlib.util.spec_from_file_location(\"ticker_dictionary\", path)\n",
    "            if spec and spec.loader:\n",
    "                module = importlib.util.module_from_spec(spec)\n",
    "                spec.loader.exec_module(module)\n",
    "                print(f\"Loaded ticker dictionary from: {path}\")\n",
    "                return module\n",
    "    raise FileNotFoundError(f\"Could not find ticker_dictionary.py in: {candidates}\")\n",
    "\n",
    "# Load the module\n",
    "ticker_dict = _load_ticker_dictionary()\n",
    "\n",
    "# Extract data structures\n",
    "SECTOR_ZOOM_MAP = getattr(ticker_dict, 'SECTOR_ZOOM_MAP', {})\n",
    "SPECULATIVE_TICKERS = getattr(ticker_dict, 'SPECULATIVE_TICKERS', [])\n",
    "CRYPTO_TICKERS = getattr(ticker_dict, 'CRYPTO_TICKERS', [])\n",
    "\n",
    "# Mapping from user-friendly names to SECTOR_ZOOM_MAP keys\n",
    "TYPE_TO_ZOOM_KEY = {\n",
    "    \"SECTOR\": \"SECTOR_CORE\",\n",
    "    \"SUMMARY\": \"SECTOR_SUMMARY\",\n",
    "    \"GLOBAL\": \"GLOBAL_MACRO\",\n",
    "    \"COMMODITIES\": \"COMMODITIES\",\n",
    "    \"MAG8\": \"MAG8\",\n",
    "}\n",
    "\n",
    "# Synthetic parent tickers that don't exist as real tradeable tickers in the database\n",
    "# These will always be expanded to constituents, even in \"indices\" mode\n",
    "SYNTHETIC_PARENTS = {\"MAG8\"}\n",
    "\n",
    "def build_ticker_list(ticker_types, view_mode):\n",
    "    \"\"\"\n",
    "    Build list of tickers based on selected types and view mode.\n",
    "    \n",
    "    Returns:\n",
    "        tickers: List of ticker symbols to query\n",
    "        tickers_meta: Dict with category and parent info for each ticker\n",
    "    \"\"\"\n",
    "    # Normalize \"ALL\" to full list\n",
    "    if \"ALL\" in ticker_types:\n",
    "        ticker_types = [\"SECTOR\", \"SUMMARY\", \"GLOBAL\", \"COMMODITIES\", \"MAG8\", \"SPECULATIVE\", \"CRYPTO\"]\n",
    "    \n",
    "    tickers = []\n",
    "    tickers_meta = {}  # ticker -> {category, parent, order}\n",
    "    order = 0\n",
    "    \n",
    "    for ttype in ticker_types:\n",
    "        if ttype in [\"SPECULATIVE\", \"CRYPTO\"]:\n",
    "            # These are flat lists\n",
    "            flat_list = SPECULATIVE_TICKERS if ttype == \"SPECULATIVE\" else CRYPTO_TICKERS\n",
    "            for ticker in flat_list:\n",
    "                if ticker not in tickers:\n",
    "                    tickers.append(ticker)\n",
    "                    tickers_meta[ticker] = {\n",
    "                        \"category\": ttype,\n",
    "                        \"parent\": None,\n",
    "                        \"order\": order\n",
    "                    }\n",
    "                    order += 1\n",
    "        elif ttype in TYPE_TO_ZOOM_KEY:\n",
    "            zoom_key = TYPE_TO_ZOOM_KEY[ttype]\n",
    "            if zoom_key in SECTOR_ZOOM_MAP:\n",
    "                category_data = SECTOR_ZOOM_MAP[zoom_key]\n",
    "                for parent_ticker, constituents in category_data.items():\n",
    "                    # Check if this is a synthetic parent (not a real tradeable ticker)\n",
    "                    is_synthetic = parent_ticker in SYNTHETIC_PARENTS\n",
    "                    \n",
    "                    if view_mode == \"indices\" and not is_synthetic:\n",
    "                        # Real ETF/index - only add the parent ticker\n",
    "                        if parent_ticker not in tickers:\n",
    "                            tickers.append(parent_ticker)\n",
    "                            tickers_meta[parent_ticker] = {\n",
    "                                \"category\": ttype,\n",
    "                                \"parent\": None,\n",
    "                                \"order\": order\n",
    "                            }\n",
    "                            order += 1\n",
    "                    else:\n",
    "                        # Constituents mode OR synthetic parent - add all constituents\n",
    "                        if not is_synthetic and parent_ticker not in tickers:\n",
    "                            # Add real parent first (not for synthetic)\n",
    "                            tickers.append(parent_ticker)\n",
    "                            tickers_meta[parent_ticker] = {\n",
    "                                \"category\": ttype,\n",
    "                                \"parent\": None,\n",
    "                                \"is_parent\": True,\n",
    "                                \"order\": order\n",
    "                            }\n",
    "                            order += 1\n",
    "                        # Add constituents\n",
    "                        for constituent in constituents:\n",
    "                            if constituent not in tickers:\n",
    "                                tickers.append(constituent)\n",
    "                                tickers_meta[constituent] = {\n",
    "                                    \"category\": ttype,\n",
    "                                    \"parent\": parent_ticker if not is_synthetic else f\"({parent_ticker})\",\n",
    "                                    \"is_parent\": False,\n",
    "                                    \"order\": order\n",
    "                                }\n",
    "                                order += 1\n",
    "    \n",
    "    print(f\"Built ticker list: {len(tickers)} tickers from {ticker_types}\")\n",
    "    return tickers, tickers_meta\n",
    "\n",
    "# Test\n",
    "test_tickers, test_meta = build_ticker_list(TICKER_TYPES, VIEW_MODE)\n",
    "print(f\"Sample tickers: {test_tickers[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to database: C:\\Users\\fvign\\Dropbox\\Vscode\\darkpool\\darkpool_analysis\\data\\darkpool.duckdb\n"
     ]
    },
    {
     "ename": "IOException",
     "evalue": "IO Error: Cannot open file \"c:\\users\\fvign\\dropbox\\vscode\\darkpool\\darkpool_analysis\\data\\darkpool.duckdb\": The process cannot access the file because it is being used by another process.\r\n\nFile is already open in \nC:\\ProgramData\\anaconda3\\python.exe (PID 145768)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOException\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m duckdb\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;28mstr\u001b[39m(db_path), read_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Test connection\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m connect_db() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m     29\u001b[0m     tables \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT table_name FROM information_schema.tables WHERE table_schema = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable tables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mt\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mtables]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m, in \u001b[0;36mconnect_db\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m db_path \u001b[38;5;241m=\u001b[39m get_db_path()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnecting to database: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m duckdb\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;28mstr\u001b[39m(db_path), read_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mIOException\u001b[0m: IO Error: Cannot open file \"c:\\users\\fvign\\dropbox\\vscode\\darkpool\\darkpool_analysis\\data\\darkpool.duckdb\": The process cannot access the file because it is being used by another process.\r\n\nFile is already open in \nC:\\ProgramData\\anaconda3\\python.exe (PID 145768)"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: DATABASE CONNECTION\n",
    "# =============================================================================\n",
    "\n",
    "def get_db_path():\n",
    "    \"\"\"Find the darkpool.duckdb database using relative paths.\"\"\"\n",
    "    candidates = [\n",
    "        Path.cwd() / \"darkpool_analysis\" / \"data\" / \"darkpool.duckdb\",\n",
    "        Path.cwd().parent / \"darkpool_analysis\" / \"data\" / \"darkpool.duckdb\",\n",
    "        Path.cwd() / \"..\" / \"darkpool_analysis\" / \"data\" / \"darkpool.duckdb\",\n",
    "        Path.cwd() / \"darkpool\" / \"darkpool_analysis\" / \"data\" / \"darkpool.duckdb\",\n",
    "    ]\n",
    "    \n",
    "    for path in candidates:\n",
    "        resolved = path.resolve()\n",
    "        if resolved.exists():\n",
    "            return resolved\n",
    "    \n",
    "    raise FileNotFoundError(f\"Could not find darkpool.duckdb in: {[str(p) for p in candidates]}\")\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"Connect to the DuckDB database in read-only mode.\"\"\"\n",
    "    db_path = get_db_path()\n",
    "    print(f\"Connecting to database: {db_path}\")\n",
    "    return duckdb.connect(str(db_path), read_only=True)\n",
    "\n",
    "# Test connection\n",
    "with connect_db() as conn:\n",
    "    tables = conn.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'main'\").fetchall()\n",
    "    print(f\"Available tables: {[t[0] for t in tables]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: DATA FETCHING\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_heatmap_data(conn, tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch finra_buy_volume_z data for specified tickers and date range.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with tickers as rows, dates as columns, values are finra_buy_volume_z\n",
    "    \"\"\"\n",
    "    # Build parameterized query\n",
    "    placeholders = \", \".join([\"?\" for _ in tickers])\n",
    "    query = f\"\"\"\n",
    "        SELECT date, symbol, finra_buy_volume_z\n",
    "        FROM daily_metrics\n",
    "        WHERE symbol IN ({placeholders})\n",
    "          AND date BETWEEN ? AND ?\n",
    "        ORDER BY date, symbol\n",
    "    \"\"\"\n",
    "    \n",
    "    params = list(tickers) + [start_date, end_date]\n",
    "    result = conn.execute(query, params).fetchdf()\n",
    "    \n",
    "    if result.empty:\n",
    "        print(f\"Warning: No data found for tickers in date range {start_date} to {end_date}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Pivot: rows=tickers, columns=dates\n",
    "    pivot = result.pivot(index='symbol', columns='date', values='finra_buy_volume_z')\n",
    "    \n",
    "    # Reorder rows to match input ticker order\n",
    "    available_tickers = [t for t in tickers if t in pivot.index]\n",
    "    missing_tickers = [t for t in tickers if t not in pivot.index]\n",
    "    if missing_tickers:\n",
    "        print(f\"Warning: {len(missing_tickers)} tickers not found in database: {missing_tickers[:5]}...\")\n",
    "    \n",
    "    pivot = pivot.reindex(available_tickers)\n",
    "    \n",
    "    print(f\"Fetched data: {pivot.shape[0]} tickers x {pivot.shape[1]} days\")\n",
    "    return pivot\n",
    "\n",
    "# Test fetch\n",
    "with connect_db() as conn:\n",
    "    test_data = fetch_heatmap_data(conn, test_tickers, START_DATE, END_DATE)\n",
    "    print(f\"Data shape: {test_data.shape}\")\n",
    "    if not test_data.empty:\n",
    "        print(f\"Date range: {test_data.columns.min()} to {test_data.columns.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4b: DIAGNOSTIC - Check for null values in date range\n",
    "# =============================================================================\n",
    "\n",
    "def diagnose_missing_data(conn, tickers, start_date, end_date):\n",
    "    \"\"\"Check which dates/tickers have null finra_buy_volume_z values.\"\"\"\n",
    "    \n",
    "    placeholders = \", \".join([\"?\" for _ in tickers])\n",
    "    \n",
    "    # Check null counts per date\n",
    "    query_dates = f\"\"\"\n",
    "        SELECT date, \n",
    "               COUNT(*) as total_tickers,\n",
    "               SUM(CASE WHEN finra_buy_volume_z IS NULL THEN 1 ELSE 0 END) as null_count\n",
    "        FROM daily_metrics\n",
    "        WHERE symbol IN ({placeholders})\n",
    "          AND date BETWEEN ? AND ?\n",
    "        GROUP BY date\n",
    "        ORDER BY date\n",
    "        LIMIT 20\n",
    "    \"\"\"\n",
    "    params = list(tickers) + [start_date, end_date]\n",
    "    date_nulls = conn.execute(query_dates, params).fetchdf()\n",
    "    \n",
    "    print(\"=== Null counts per date (first 20 days) ===\")\n",
    "    print(date_nulls.to_string())\n",
    "    \n",
    "    # Check which tickers have nulls in the first week\n",
    "    query_tickers = f\"\"\"\n",
    "        SELECT symbol, \n",
    "               COUNT(*) as days,\n",
    "               SUM(CASE WHEN finra_buy_volume_z IS NULL THEN 1 ELSE 0 END) as null_days\n",
    "        FROM daily_metrics\n",
    "        WHERE symbol IN ({placeholders})\n",
    "          AND date BETWEEN ? AND DATE_ADD(CAST(? AS DATE), INTERVAL 7 DAY)\n",
    "        GROUP BY symbol\n",
    "        HAVING null_days > 0\n",
    "        ORDER BY null_days DESC\n",
    "    \"\"\"\n",
    "    params2 = list(tickers) + [start_date, start_date]\n",
    "    ticker_nulls = conn.execute(query_tickers, params2).fetchdf()\n",
    "    \n",
    "    print(\"\\n=== Tickers with nulls in first week ===\")\n",
    "    if ticker_nulls.empty:\n",
    "        print(\"No tickers with null values in first week\")\n",
    "    else:\n",
    "        print(ticker_nulls.to_string())\n",
    "    \n",
    "    return date_nulls, ticker_nulls\n",
    "\n",
    "# Run diagnostic\n",
    "with connect_db() as conn:\n",
    "    diagnose_missing_data(conn, test_tickers, START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: COLOR MAPPING\n",
    "# =============================================================================\n",
    "\n",
    "# Dark theme colors\n",
    "BG_COLOR = '#0b0f1a'\n",
    "TEXT_COLOR = '#e6e6e6'\n",
    "GRID_COLOR = '#2a2a3a'\n",
    "\n",
    "# Diverging colormap: Purple <- Black -> Neon Green\n",
    "PURPLE = '#8304B9'\n",
    "BLACK = \"#FFFFFF\"\n",
    "NEON_GREEN = '#39FF14'\n",
    "\n",
    "def create_z_colormap(gamma=1.0):\n",
    "    \"\"\"\n",
    "    Create diverging colormap for z-scores with gamma correction.\n",
    "    \n",
    "    Args:\n",
    "        gamma: Power factor for non-linear scaling.\n",
    "               gamma > 1: more contrast near extremes, subtle mid-tones\n",
    "               gamma < 1: more contrast near center, compressed extremes\n",
    "    \"\"\"\n",
    "    # Sample colors along linear gradient, then apply gamma to positions\n",
    "    n_samples = 256\n",
    "    colors = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Linear position [0, 1]\n",
    "        t = i / (n_samples - 1)\n",
    "        \n",
    "        # Apply gamma correction (centered at 0.5)\n",
    "        if t < 0.5:\n",
    "            # Negative side: map [0, 0.5] with gamma\n",
    "            t_adj = 0.5 - (0.5 - t) ** gamma / (0.5 ** (gamma - 1))\n",
    "        elif t > 0.5:\n",
    "            # Positive side: map [0.5, 1] with gamma\n",
    "            t_adj = 0.5 + (t - 0.5) ** gamma / (0.5 ** (gamma - 1))\n",
    "        else:\n",
    "            t_adj = 0.5\n",
    "        \n",
    "        t_adj = max(0, min(1, t_adj))\n",
    "        \n",
    "        # Interpolate color: Purple -> Black -> Green\n",
    "        if t_adj < 0.5:\n",
    "            # Purple to Black\n",
    "            ratio = t_adj / 0.5\n",
    "            r = int(0x83 * (1 - ratio))\n",
    "            g = int(0x04 * (1 - ratio))\n",
    "            b = int(0xB9 * (1 - ratio))\n",
    "        else:\n",
    "            # Black to Green\n",
    "            ratio = (t_adj - 0.5) / 0.5\n",
    "            r = int(0x39 * ratio)\n",
    "            g = int(0xFF * ratio)\n",
    "            b = int(0x14 * ratio)\n",
    "        \n",
    "        colors.append(f'#{r:02x}{g:02x}{b:02x}')\n",
    "    \n",
    "    return LinearSegmentedColormap.from_list('z_diverging', colors, N=256)\n",
    "\n",
    "# Gamma=0.8 expands mid-range colors, making subtle z-scores more visible\n",
    "Z_CMAP = create_z_colormap(gamma=0.8)\n",
    "\n",
    "def normalize_data(data, z_scale_mode):\n",
    "    \"\"\"\n",
    "    Normalize z-score data for colormap application.\n",
    "    \"\"\"\n",
    "    values = data.values.copy()\n",
    "    \n",
    "    if z_scale_mode == \"global\":\n",
    "        abs_max = np.nanmax(np.abs(values))\n",
    "        if abs_max > 0:\n",
    "            normalized = values / abs_max\n",
    "        else:\n",
    "            normalized = values\n",
    "        vmin, vmax = -abs_max, abs_max\n",
    "        \n",
    "    elif z_scale_mode == \"per_row\":\n",
    "        normalized = np.zeros_like(values)\n",
    "        for i in range(values.shape[0]):\n",
    "            row = values[i, :]\n",
    "            row_abs_max = np.nanmax(np.abs(row))\n",
    "            if row_abs_max > 0:\n",
    "                normalized[i, :] = row / row_abs_max\n",
    "            else:\n",
    "                normalized[i, :] = 0\n",
    "        vmin, vmax = -1, 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown z_scale_mode: {z_scale_mode}\")\n",
    "    \n",
    "    return pd.DataFrame(normalized, index=data.index, columns=data.columns), vmin, vmax\n",
    "\n",
    "# Test colormap comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 1))\n",
    "fig.patch.set_facecolor(BG_COLOR)\n",
    "gradient = np.linspace(-1, 1, 256).reshape(1, -1)\n",
    "\n",
    "axes[0].imshow(gradient, aspect='auto', cmap=create_z_colormap(gamma=1.0))\n",
    "axes[0].set_title('gamma=1.0 (linear)', color=TEXT_COLOR, fontsize=9, loc='left')\n",
    "axes[0].set_yticks([]); axes[0].set_xticks([])\n",
    "\n",
    "axes[1].imshow(gradient, aspect='auto', cmap=Z_CMAP)\n",
    "axes[1].set_title('gamma=0.8 (current)', color=TEXT_COLOR, fontsize=9, loc='left')\n",
    "axes[1].set_yticks([]); axes[1].set_xticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5b: HIERARCHICAL CLUSTERING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_correlation_distance(data):\n",
    "    \"\"\"\n",
    "    Compute correlation-based distance matrix.\n",
    "    Distance = 1 - Pearson correlation (range: 0 to 2)\n",
    "    \"\"\"\n",
    "    # Handle NaN values by filling with row mean\n",
    "    row_means = data.mean(axis=1)\n",
    "    clean_data = data.T.fillna(row_means).T\n",
    "    \n",
    "    # Compute correlation matrix (ticker-to-ticker)\n",
    "    corr_matrix = clean_data.T.corr()\n",
    "    \n",
    "    # Convert to distance: d = 1 - correlation\n",
    "    dist_matrix = 1 - corr_matrix\n",
    "    \n",
    "    # Ensure diagonal is 0 and matrix is symmetric\n",
    "    np.fill_diagonal(dist_matrix.values, 0)\n",
    "    \n",
    "    return dist_matrix\n",
    "\n",
    "def cluster_within_categories(data, tickers_meta):\n",
    "    \"\"\"\n",
    "    Apply hierarchical clustering within each category block.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame (tickers x dates) with z-scores\n",
    "        tickers_meta: Dict with category info for each ticker\n",
    "        \n",
    "    Returns:\n",
    "        clustered_order: List of ticker symbols in new order\n",
    "        linkage_by_category: OrderedDict of {category: (Z, tickers)} for dendrograms\n",
    "    \"\"\"\n",
    "    from collections import OrderedDict\n",
    "    \n",
    "    clustered_order = []\n",
    "    linkage_by_category = OrderedDict()\n",
    "    \n",
    "    # Group tickers by category, preserving order\n",
    "    category_groups = OrderedDict()\n",
    "    for ticker in data.index:\n",
    "        meta = tickers_meta.get(ticker, {})\n",
    "        cat = meta.get('category', 'UNKNOWN')\n",
    "        if cat not in category_groups:\n",
    "            category_groups[cat] = []\n",
    "        category_groups[cat].append(ticker)\n",
    "    \n",
    "    # Cluster each category independently\n",
    "    for category, tickers in category_groups.items():\n",
    "        if len(tickers) <= 2:\n",
    "            # Too few tickers to cluster meaningfully\n",
    "            clustered_order.extend(tickers)\n",
    "            linkage_by_category[category] = None\n",
    "            continue\n",
    "        \n",
    "        # Extract subset for this category\n",
    "        subset = data.loc[tickers]\n",
    "        \n",
    "        # Compute distance matrix\n",
    "        dist_matrix = compute_correlation_distance(subset)\n",
    "        \n",
    "        # Convert to condensed form for scipy\n",
    "        condensed_dist = squareform(dist_matrix.values, checks=False)\n",
    "        \n",
    "        # Handle potential NaN/inf values\n",
    "        condensed_dist = np.nan_to_num(condensed_dist, nan=1.0, posinf=2.0, neginf=0.0)\n",
    "        \n",
    "        # Ensure non-negative distances\n",
    "        condensed_dist = np.clip(condensed_dist, 0, 2)\n",
    "        \n",
    "        # Perform hierarchical clustering (average linkage works well for correlation)\n",
    "        Z = linkage(condensed_dist, method='average')\n",
    "        \n",
    "        # Get optimal leaf ordering\n",
    "        leaf_order = leaves_list(Z)\n",
    "        \n",
    "        # Reorder tickers\n",
    "        reordered = [tickers[i] for i in leaf_order]\n",
    "        clustered_order.extend(reordered)\n",
    "        linkage_by_category[category] = (Z, reordered)\n",
    "    \n",
    "    return clustered_order, linkage_by_category\n",
    "\n",
    "print(\"Clustering functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7-8: MAIN HEATMAP RENDERING WITH CUMULATIVE FLOW & BREADTH\n",
    "# =============================================================================\n",
    "\n",
    "# Watermark path\n",
    "WATERMARK_PATH = r\"C:\\Users\\fvign\\Dropbox\\Vscode\\darkpool\\cowwbell_waterrmark.png\"\n",
    "\n",
    "def draw_stacked_dendrograms(ax, linkage_by_category, tickers_meta, data_index):\n",
    "    \"\"\"\n",
    "    Draw multiple dendrograms stacked vertically, aligned with heatmap rows.\n",
    "    \"\"\"\n",
    "    ax.set_facecolor(BG_COLOR)\n",
    "    \n",
    "    current_y_offset = 0\n",
    "    max_x = 0\n",
    "    \n",
    "    for category, cat_data in linkage_by_category.items():\n",
    "        cat_tickers = [t for t in data_index if tickers_meta.get(t, {}).get('category') == category]\n",
    "        n_cat = len(cat_tickers)\n",
    "        \n",
    "        if cat_data is None or n_cat <= 2:\n",
    "            current_y_offset += n_cat\n",
    "            continue\n",
    "        \n",
    "        Z, reordered_tickers = cat_data\n",
    "        dendro_data = dendrogram(Z, orientation='left', no_plot=True)\n",
    "        \n",
    "        icoord = dendro_data['icoord']\n",
    "        dcoord = dendro_data['dcoord']\n",
    "        \n",
    "        for ic, dc in zip(icoord, dcoord):\n",
    "            y_coords = [(y - 5) / 10 + current_y_offset for y in ic]\n",
    "            ax.plot(dc, y_coords, color=TEXT_COLOR, linewidth=0.7)\n",
    "            max_x = max(max_x, max(dc))\n",
    "        \n",
    "        current_y_offset += n_cat\n",
    "    \n",
    "    ax.set_xlim(max_x * 1.1, 0)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "def calculate_cumulative_flow(data):\n",
    "    \"\"\"\n",
    "    Calculate cumulative positive and negative z-score flows.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with z-scores (tickers x dates)\n",
    "        \n",
    "    Returns cumulative sum of positive z-scores, negative z-scores, and net.\n",
    "    \"\"\"\n",
    "    # Sum z-scores per day (raw values)\n",
    "    daily_positive = data.apply(lambda col: col[col > 0].sum(), axis=0).fillna(0)\n",
    "    daily_negative = data.apply(lambda col: col[col < 0].sum(), axis=0).fillna(0)\n",
    "    \n",
    "    # Cumulative sums\n",
    "    cum_positive = daily_positive.cumsum()\n",
    "    cum_negative = daily_negative.cumsum()\n",
    "    cum_net = (daily_positive + daily_negative).cumsum()\n",
    "    \n",
    "    return cum_positive, cum_negative, cum_net\n",
    "\n",
    "def calculate_cumulative_breadth(data):\n",
    "    \"\"\"\n",
    "    Calculate cumulative breadth (count of positive/negative z-score tickers per day).\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with z-scores (tickers x dates)\n",
    "        \n",
    "    Returns cumulative count of positive tickers, negative tickers, and net difference.\n",
    "    \"\"\"\n",
    "    # Count tickers with positive/negative z-scores per day\n",
    "    daily_positive_count = data.apply(lambda col: (col > 0).sum(), axis=0)\n",
    "    daily_negative_count = data.apply(lambda col: (col < 0).sum(), axis=0)\n",
    "    \n",
    "    # Cumulative sums\n",
    "    cum_positive = daily_positive_count.cumsum()\n",
    "    cum_negative = daily_negative_count.cumsum()\n",
    "    cum_net = (daily_positive_count - daily_negative_count).cumsum()\n",
    "    \n",
    "    return cum_positive, cum_negative, cum_net\n",
    "\n",
    "def plot_vwbr_heatmap(data, tickers_meta, z_scale_mode, view_mode, cell_size=0.15,\n",
    "                       show_dendrogram=False, linkage_by_category=None, \n",
    "                       show_cumulative=True, show_breadth=True):\n",
    "    \"\"\"\n",
    "    Main heatmap plotting function with optional dendrogram, cumulative flow, and breadth.\n",
    "    \"\"\"\n",
    "    if data.empty:\n",
    "        print(\"No data to plot!\")\n",
    "        return None\n",
    "    \n",
    "    n_tickers, n_days = data.shape\n",
    "    normalized_data, vmin, vmax = normalize_data(data, z_scale_mode)\n",
    "    \n",
    "    # Count active subplots\n",
    "    n_subplots = int(show_cumulative) + int(show_breadth)\n",
    "    subplot_height = 0.12 if n_subplots == 2 else 0.18\n",
    "    \n",
    "    # Calculate figure size\n",
    "    plot_width = n_days * cell_size\n",
    "    plot_height = n_tickers * cell_size\n",
    "    cumulative_height = subplot_height * n_subplots * 10 if n_subplots > 0 else 0\n",
    "    \n",
    "    # Margins\n",
    "    left_margin = 1.0 if not show_dendrogram else 0.3\n",
    "    right_margin = 1.5\n",
    "    top_margin = 1.2\n",
    "    bottom_margin = 2.0 if n_subplots > 0 else 1.5\n",
    "    dendro_width = 1.5 if show_dendrogram else 0\n",
    "    \n",
    "    fig_width = plot_width + left_margin + right_margin + dendro_width\n",
    "    fig_height = plot_height + top_margin + bottom_margin + cumulative_height\n",
    "    fig_width = max(fig_width, 10)\n",
    "    fig_height = max(fig_height, 6)\n",
    "    \n",
    "    fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "    fig.patch.set_facecolor(BG_COLOR)\n",
    "    \n",
    "    # Calculate layout proportions\n",
    "    gap = 0.02\n",
    "    if n_subplots == 2:\n",
    "        heat_height_frac = 0.46\n",
    "        cum_height_frac = subplot_height\n",
    "        breadth_height_frac = subplot_height\n",
    "        bottom_breadth = 0.08\n",
    "        bottom_cum = bottom_breadth + breadth_height_frac + gap\n",
    "        bottom_heat = bottom_cum + cum_height_frac + gap + 0.04\n",
    "    elif n_subplots == 1:\n",
    "        heat_height_frac = 0.54\n",
    "        cum_height_frac = 0.18 if show_cumulative else 0\n",
    "        breadth_height_frac = 0.18 if show_breadth else 0\n",
    "        bottom_cum = 0.10 if show_cumulative else 0\n",
    "        bottom_breadth = 0.10 if show_breadth else 0\n",
    "        bottom_heat = 0.12 + max(cum_height_frac, breadth_height_frac) + gap + 0.06\n",
    "    else:\n",
    "        heat_height_frac = 0.72\n",
    "        cum_height_frac = 0\n",
    "        breadth_height_frac = 0\n",
    "        bottom_heat = 0.15\n",
    "        bottom_cum = 0\n",
    "        bottom_breadth = 0\n",
    "    \n",
    "    # Define consistent widths for alignment\n",
    "    if show_dendrogram and linkage_by_category:\n",
    "        dendro_left = 0.02\n",
    "        dendro_width_frac = 0.08\n",
    "        heat_left = dendro_left + dendro_width_frac + 0.05\n",
    "        heat_width = 0.72\n",
    "    else:\n",
    "        heat_left = 0.10\n",
    "        heat_width = 0.78\n",
    "    \n",
    "    # Create axes\n",
    "    if show_dendrogram and linkage_by_category:\n",
    "        ax_dendro = fig.add_axes([dendro_left, bottom_heat, dendro_width_frac, heat_height_frac])\n",
    "        ax_dendro.set_facecolor(BG_COLOR)\n",
    "    else:\n",
    "        ax_dendro = None\n",
    "    \n",
    "    ax_heat = fig.add_axes([heat_left, bottom_heat, heat_width, heat_height_frac])\n",
    "    ax_heat.set_facecolor(BG_COLOR)\n",
    "    \n",
    "    ax_cum = None\n",
    "    ax_breadth = None\n",
    "    \n",
    "    if n_subplots == 2:\n",
    "        ax_cum = fig.add_axes([heat_left, bottom_cum, heat_width, cum_height_frac])\n",
    "        ax_cum.set_facecolor(BG_COLOR)\n",
    "        ax_breadth = fig.add_axes([heat_left, bottom_breadth, heat_width, breadth_height_frac])\n",
    "        ax_breadth.set_facecolor(BG_COLOR)\n",
    "    elif show_cumulative:\n",
    "        ax_cum = fig.add_axes([heat_left, bottom_cum, heat_width, cum_height_frac])\n",
    "        ax_cum.set_facecolor(BG_COLOR)\n",
    "    elif show_breadth:\n",
    "        ax_breadth = fig.add_axes([heat_left, bottom_breadth, heat_width, breadth_height_frac])\n",
    "        ax_breadth.set_facecolor(BG_COLOR)\n",
    "    \n",
    "    # Plot heatmap - use aspect='auto' to match subplot widths\n",
    "    im = ax_heat.imshow(\n",
    "        normalized_data.values,\n",
    "        aspect='auto',\n",
    "        cmap=Z_CMAP,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        interpolation='nearest'\n",
    "    )\n",
    "    \n",
    "    ax_heat.set_yticks(np.arange(n_tickers))\n",
    "    \n",
    "    # Build category boundaries\n",
    "    category_boundaries = []\n",
    "    prev_category = None\n",
    "    y_labels = []\n",
    "    \n",
    "    for i, ticker in enumerate(data.index):\n",
    "        meta = tickers_meta.get(ticker, {})\n",
    "        category = meta.get('category', '')\n",
    "        parent = meta.get('parent', None)\n",
    "        is_parent = meta.get('is_parent', False)\n",
    "        \n",
    "        if category != prev_category:\n",
    "            category_boundaries.append(i - 0.5)\n",
    "            prev_category = category\n",
    "        \n",
    "        if view_mode == \"constituents\":\n",
    "            if is_parent:\n",
    "                y_labels.append(f\">> {ticker}\")\n",
    "            elif parent:\n",
    "                y_labels.append(f\"   {ticker}\")\n",
    "            else:\n",
    "                y_labels.append(ticker)\n",
    "        else:\n",
    "            y_labels.append(ticker)\n",
    "    \n",
    "    ax_heat.set_yticklabels(y_labels, fontsize=7 if view_mode == \"constituents\" else 8, \n",
    "                            color=TEXT_COLOR, fontfamily='monospace' if view_mode == \"constituents\" else None)\n",
    "    \n",
    "    for boundary in category_boundaries[1:]:\n",
    "        ax_heat.axhline(y=boundary, color=GRID_COLOR, linewidth=1.5, linestyle='-')\n",
    "    \n",
    "    # X-axis setup\n",
    "    dates = data.columns\n",
    "    if n_days > 100:\n",
    "        step = n_days // 20\n",
    "    elif n_days > 50:\n",
    "        step = n_days // 10\n",
    "    else:\n",
    "        step = max(1, n_days // 10)\n",
    "    \n",
    "    x_ticks = np.arange(0, n_days, step)\n",
    "    date_labels = []\n",
    "    for idx in x_ticks:\n",
    "        if idx < len(dates):\n",
    "            d = dates[idx]\n",
    "            if hasattr(d, 'strftime'):\n",
    "                date_labels.append(d.strftime('%m/%d'))\n",
    "            else:\n",
    "                date_labels.append(str(d)[-5:])\n",
    "        else:\n",
    "            date_labels.append('')\n",
    "    \n",
    "    ax_heat.set_xticks(x_ticks)\n",
    "    ax_heat.set_xticklabels(date_labels, rotation=45, ha='right', fontsize=7, color=TEXT_COLOR)\n",
    "    ax_heat.set_xlabel('Date', color=TEXT_COLOR, fontsize=10)\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar_ax = fig.add_axes([heat_left + heat_width + 0.02, bottom_heat, 0.015, heat_height_frac])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "    cbar.ax.yaxis.set_tick_params(color=TEXT_COLOR)\n",
    "    cbar.outline.set_edgecolor(GRID_COLOR)\n",
    "    plt.setp(cbar.ax.yaxis.get_ticklabels(), color=TEXT_COLOR, fontsize=8)\n",
    "    \n",
    "    if z_scale_mode == \"global\":\n",
    "        cbar.set_label(f'Z', color=TEXT_COLOR, fontsize=9)\n",
    "    else:\n",
    "        cbar.set_label('Z (per-row)', color=TEXT_COLOR, fontsize=9)\n",
    "    \n",
    "    # Draw dendrograms\n",
    "    if ax_dendro is not None and linkage_by_category:\n",
    "        ax_dendro.set_ylim(n_tickers - 0.5, -0.5)\n",
    "        draw_stacked_dendrograms(ax_dendro, linkage_by_category, tickers_meta, data.index)\n",
    "        \n",
    "        for boundary in category_boundaries[1:]:\n",
    "            ax_dendro.axhline(y=boundary, color=GRID_COLOR, linewidth=1.5, linestyle='-')\n",
    "    \n",
    "    # Draw cumulative flow subplot\n",
    "    if ax_cum is not None:\n",
    "        cum_positive, cum_negative, cum_net = calculate_cumulative_flow(data)\n",
    "        \n",
    "        x_vals = np.arange(len(cum_positive))\n",
    "        \n",
    "        # Plot positive cumulative (green)\n",
    "        ax_cum.plot(x_vals, cum_positive.values, color='#39FF14', linewidth=1.5, label='Cum +Z')\n",
    "        \n",
    "        # Plot negative cumulative (red) - absolute value\n",
    "        ax_cum.plot(x_vals, np.abs(cum_negative.values), color='#FF3939', linewidth=1.5, label='Cum -Z')\n",
    "        \n",
    "        # Plot net as filled area around zero (light grey)\n",
    "        ax_cum.fill_between(x_vals, 0, cum_net.values, alpha=0.3, color='#888888', label='Net')\n",
    "        ax_cum.axhline(y=0, color=GRID_COLOR, linewidth=0.8, linestyle='--')\n",
    "        \n",
    "        ax_cum.set_xlim(-0.5, n_days - 0.5)\n",
    "        \n",
    "        ax_cum.set_xticks(x_ticks)\n",
    "        ax_cum.set_xticklabels([])  # Remove labels, keep tick marks\n",
    "        \n",
    "        ax_cum.tick_params(axis='both', colors=TEXT_COLOR, labelsize=7)\n",
    "        ax_cum.set_ylabel('Flow', color=TEXT_COLOR, fontsize=8)\n",
    "        \n",
    "        ax_cum.legend(loc='upper left', fontsize=6, facecolor=BG_COLOR, edgecolor=GRID_COLOR,\n",
    "                      labelcolor=TEXT_COLOR, framealpha=0.9)\n",
    "        \n",
    "        for spine in ax_cum.spines.values():\n",
    "            spine.set_color(GRID_COLOR)\n",
    "    \n",
    "    # Draw breadth subplot\n",
    "    if ax_breadth is not None:\n",
    "        breadth_positive, breadth_negative, breadth_net = calculate_cumulative_breadth(data)\n",
    "        \n",
    "        x_vals = np.arange(len(breadth_positive))\n",
    "        \n",
    "        # Plot positive breadth (green)\n",
    "        ax_breadth.plot(x_vals, breadth_positive.values, color='#39FF14', linewidth=1.5, label='Cum +Count')\n",
    "        \n",
    "        # Plot negative breadth (red)\n",
    "        ax_breadth.plot(x_vals, breadth_negative.values, color='#FF3939', linewidth=1.5, label='Cum -Count')\n",
    "        \n",
    "        # Plot net as filled area around zero (light grey)\n",
    "        ax_breadth.fill_between(x_vals, 0, breadth_net.values, alpha=0.3, color='#888888', label='Net')\n",
    "        ax_breadth.axhline(y=0, color=GRID_COLOR, linewidth=0.8, linestyle='--')\n",
    "        \n",
    "        ax_breadth.set_xlim(-0.5, n_days - 0.5)\n",
    "        \n",
    "        ax_breadth.set_xticks(x_ticks)\n",
    "        ax_breadth.set_xticklabels(date_labels, rotation=45, ha='right', fontsize=7, color=TEXT_COLOR)\n",
    "        ax_breadth.set_xlabel('Date', color=TEXT_COLOR, fontsize=10)\n",
    "        \n",
    "        ax_breadth.tick_params(axis='both', colors=TEXT_COLOR, labelsize=7)\n",
    "        ax_breadth.set_ylabel('Breadth', color=TEXT_COLOR, fontsize=8)\n",
    "        \n",
    "        ax_breadth.legend(loc='upper left', fontsize=6, facecolor=BG_COLOR, edgecolor=GRID_COLOR,\n",
    "                          labelcolor=TEXT_COLOR, framealpha=0.9)\n",
    "        \n",
    "        for spine in ax_breadth.spines.values():\n",
    "            spine.set_color(GRID_COLOR)\n",
    "    \n",
    "    # Title - format dates without time\n",
    "    clustering_str = \" (clustered)\" if show_dendrogram else \"\"\n",
    "    start_date = pd.to_datetime(data.columns.min()).strftime('%Y-%m-%d')\n",
    "    end_date = pd.to_datetime(data.columns.max()).strftime('%Y-%m-%d')\n",
    "    title = f'VWBR Z-Score Heatmap{clustering_str} | {start_date} to {end_date}'\n",
    "    fig.suptitle(title, color=TEXT_COLOR, fontsize=12, y=0.91)\n",
    "    \n",
    "    # ax_heat.set_ylabel('Ticker', color=TEXT_COLOR, fontsize=10)\n",
    "    \n",
    "    for spine in ax_heat.spines.values():\n",
    "        spine.set_color(GRID_COLOR)\n",
    "    \n",
    "    ax_heat.tick_params(axis='both', colors=TEXT_COLOR)\n",
    "    \n",
    "    # Add watermark in top right corner\n",
    "    try:\n",
    "        watermark_img = plt.imread(WATERMARK_PATH)\n",
    "        # Create small axes in top right corner [left, bottom, width, height]\n",
    "        ax_watermark = fig.add_axes([0.0, 0.87, 0.15, 0.075])\n",
    "        ax_watermark.imshow(watermark_img)\n",
    "        ax_watermark.axis('off')\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load watermark: {e}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"Heatmap function defined with cumulative flow and breadth subplots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "# Build ticker list\n",
    "tickers, tickers_meta = build_ticker_list(TICKER_TYPES, VIEW_MODE)\n",
    "\n",
    "# Connect and fetch data\n",
    "with connect_db() as conn:\n",
    "    data = fetch_heatmap_data(conn, tickers, START_DATE, END_DATE)\n",
    "\n",
    "# Apply clustering if enabled\n",
    "linkage_by_category = None\n",
    "if ENABLE_CLUSTERING and not data.empty:\n",
    "    print(\"Applying hierarchical clustering within categories...\")\n",
    "    clustered_order, linkage_by_category = cluster_within_categories(data, tickers_meta)\n",
    "    \n",
    "    # Reorder data by clustered order\n",
    "    data = data.reindex(clustered_order)\n",
    "    \n",
    "    # Update tickers_meta order to match new ordering\n",
    "    for i, ticker in enumerate(clustered_order):\n",
    "        if ticker in tickers_meta:\n",
    "            tickers_meta[ticker]['order'] = i\n",
    "    \n",
    "    print(f\"Clustering complete. Categories clustered: {len([k for k,v in linkage_by_category.items() if v is not None])}\")\n",
    "\n",
    "# Plot heatmap\n",
    "if not data.empty:\n",
    "    fig = plot_vwbr_heatmap(\n",
    "        data, \n",
    "        tickers_meta, \n",
    "        Z_SCALE_MODE, \n",
    "        VIEW_MODE, \n",
    "        CELL_SIZE,\n",
    "        show_dendrogram=SHOW_DENDROGRAM and ENABLE_CLUSTERING,\n",
    "        linkage_by_category=linkage_by_category,\n",
    "        show_cumulative=SHOW_CUMULATIVE,\n",
    "        show_breadth=SHOW_BREADTH\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for the selected parameters.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

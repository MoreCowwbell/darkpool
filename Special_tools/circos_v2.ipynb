{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eeaa72d",
   "metadata": {},
   "source": [
    "# Cell 1\n",
    "# Circos-Style Money Flow Chord Diagram\n",
    "\n",
    "This notebook builds a self-contained Circos-style chord diagram from Accumulation Score time series in a local database.\n",
    "\n",
    "Constraints honored:\n",
    "- No project modules are imported or modified.\n",
    "- Database access is read-only (SELECT-only).\n",
    "- No files are written or mutated; all outputs are in-notebook only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c759775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticker groups (from ticker_dictionary.py)\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "def _load_ticker_dictionary():\n",
    "    candidates = [\n",
    "        Path.cwd() / \"ticker_dictionary.py\",\n",
    "        Path.cwd() / \"Special_tools\" / \"ticker_dictionary.py\",\n",
    "        Path.cwd() / \"darkpool_analysis\" / \"ticker_dictionary.py\",\n",
    "        Path.cwd().parent / \"darkpool_analysis\" / \"ticker_dictionary.py\",  # Parent directory for Special_tools\n",
    "    ]\n",
    "    for path in candidates:\n",
    "        if path.exists():\n",
    "            spec = importlib.util.spec_from_file_location(\"ticker_dictionary\", path)\n",
    "            if spec and spec.loader:\n",
    "                module = importlib.util.module_from_spec(spec)\n",
    "                spec.loader.exec_module(module)\n",
    "                return module\n",
    "    raise FileNotFoundError(\"ticker_dictionary.py not found in current, Special_tools, or darkpool_analysis directory\")\n",
    "\n",
    "\n",
    "ticker_dict = _load_ticker_dictionary()\n",
    "\n",
    "# Load flat dictionaries directly (no SECTOR_ZOOM_MAP wrapper)\n",
    "SECTOR_CORE = ticker_dict.SECTOR_CORE\n",
    "THEMATIC_SECTORS = ticker_dict.THEMATIC_SECTORS\n",
    "GLOBAL_MACRO = ticker_dict.GLOBAL_MACRO\n",
    "COMMODITIES = ticker_dict.COMMODITIES\n",
    "MAG8 = ticker_dict.MAG8\n",
    "RATES_CREDIT = ticker_dict.RATES_CREDIT\n",
    "\n",
    "# Extract ticker lists\n",
    "SECTOR_CORE_TICKERS = list(SECTOR_CORE.keys())\n",
    "THEMATIC_SECTORS_TICKERS = list(THEMATIC_SECTORS.keys())\n",
    "GLOBAL_MACRO_TICKERS = list(GLOBAL_MACRO.keys())\n",
    "COMMODITIES_TICKERS = list(COMMODITIES.keys())\n",
    "MAG8_TICKERS = list(MAG8[\"MAG8\"])\n",
    "RATES_CREDIT_TICKERS = list(RATES_CREDIT.keys())\n",
    "SPECULATIVE_TICKERS = list(getattr(ticker_dict, \"SPECULATIVE_TICKERS\", []))\n",
    "CRYPTO_TICKERS = list(getattr(ticker_dict, \"CRYPTO_TICKERS\", []))\n",
    "\n",
    "# --- User options ---\n",
    "# Universe selection (ticker groups)\n",
    "TICKER_TYPE = [\"SECTOR\", \"THEMATIC\", \"GLOBAL\", \"COMMODITIES\", \"MAG8\", \"CRYPTO\", \"SPECULATIVE\"]\n",
    "TICKER_TYPE_OPTIONS = [\"SECTOR\", \"THEMATIC\", \"GLOBAL\", \"COMMODITIES\", \"MAG8\", \"RATES\", \"SPECULATIVE\", \"CRYPTO\", \"ALL\"]\n",
    "\n",
    "# Time window\n",
    "END_DATE = None                         # End of window; None = auto-detect max date in DB\n",
    "FLOW_PERIOD_DAYS = 5                   # Trading-day window used for flows (start->end inside this window)\n",
    "\n",
    "# Flow selection (who connects to whom)\n",
    "TOP_K_WINNERS = 15                        # Max winners by demand (positive delta) to include\n",
    "TOP_K_LOSERS = 15                         # Max losers by supply (negative delta) to include\n",
    "MIN_EDGE_FLOW = 0.0                      # Drop edges smaller than this flow (pre-strand)\n",
    "DISTRIBUTION_MODE = \"demand_weighted\"   # \"equal\" or \"demand_weighted\" split from sources to winners\n",
    "\n",
    "# Chord density + band layout\n",
    "METRIC_BAND_MODE = \"proportional\"       # \"equal\" = same band width per metric; \"proportional\" = band width by flow\n",
    "MAX_EDGES_PER_METRIC = 80               # Cap edges per metric to keep plot readable\n",
    "EDGE_RIBBON_SPLITS = 1                  # Flow-based widths (no artificial splitting)\n",
    "EDGE_RIBBON_MAX = 100                    # Cap ribbons per edge (perf)\n",
    "CHORD_ARC_FRACTION = 1.0                # BASELINE: full arc\n",
    "CHORD_RADIUS = 0.78                     # Inner radius for chord ribbons\n",
    "BAND_GAP_FRAC = 0.0                    # BASELINE: no gap\n",
    "DIR_GAP_FRAC = 0.0                     # BASELINE: no gap\n",
    "CATEGORY_GAP_DEG = 3                    # Degrees; category block gap (0 = uniform spacing)\n",
    "\n",
    "# Ring layout\n",
    "TIME_SLICE_BINS = 30                    # Number of time slices per outer ring\n",
    "RING_BASE_THICKNESS = 0.005             # Minimum ring thickness\n",
    "RING_THICKNESS_SCALE = 0.1              # Added thickness scaled by magnitude\n",
    "RING1_THICKNESS_MULT = 0.8              # Ring 1 (accum) sizing boost\n",
    "RING2_THICKNESS_MULT = 0.8               # Ring 2 (lit) sizing boost\n",
    "RING3_THICKNESS_MULT = 0.8              # Ring 3 (finra_buy) sizing boost\n",
    "RING_GAP = 0.01                         # Gap between metric rings\n",
    "\n",
    "# Fanned chord layout\n",
    "RIBBON_MIN_WIDTH_RAD = {                # Minimum ribbon arc width in radians (per-metric)\n",
    "    'accum': 0.001,\n",
    "    'short': 0.001,\n",
    "    'lit': 0.001,\n",
    "    'finra_buy': 0.001,\n",
    "    'vwbr_z': 0.001,\n",
    "}\n",
    "RIBBON_GAP_RAD = 0.001                  # Small gap between ribbons\n",
    "RIBBON_WIDTH_SCALE_BY_FLOW = True       # Scale ribbon widths by flow\n",
    "RIBBON_CENTERED = True                  # If True, ribbons fan out from center; if False, from edge\n",
    "RIBBON_CONVERGE_TO_POINT = False        # Enable fanning\n",
    "RIBBON_ANCHOR_TO_CENTER = True      # Anchor fan to band midpoint (ignore dir gap)\n",
    "RIBBON_CENTER_OFFSET = {                # Per-metric center offset in radians (positive = clockwise)\n",
    "    'accum': 0.0,                       # accumulation stays centered\n",
    "    'short': 0.0,                       # short offset (adjust as needed)\n",
    "    'lit': 0.0,                        # lit offset (adjust as needed)\n",
    "    'finra_buy': 0.0,                   # finra buy offset\n",
    "    'vwbr_z': 0.0,                      # finra buy z offset\n",
    "}\n",
    "\n",
    "# Render quality (performance vs fidelity)\n",
    "RENDER_MODE = \"balanced\"  # \"fast\", \"balanced\", \"quality\"\n",
    "CHORD_FILL_ALPHA = 0.55\n",
    "CHORD_LINE_ALPHA = 0.8\n",
    "CHORD_COLOR_SOFTEN = 0.25  # blend toward background to reduce saturation\n",
    "if RENDER_MODE == \"fast\":\n",
    "    USE_GRADIENT_FILL = False\n",
    "    CHORD_GRADIENT_STEPS = 8\n",
    "    CHORD_ARC_POINTS = 8\n",
    "    CHORD_CURVE_POINTS = 30\n",
    "elif RENDER_MODE == \"quality\":\n",
    "    USE_GRADIENT_FILL = True\n",
    "    CHORD_GRADIENT_STEPS = 32\n",
    "    CHORD_ARC_POINTS = 18\n",
    "    CHORD_CURVE_POINTS = 70\n",
    "else:\n",
    "    USE_GRADIENT_FILL = True\n",
    "    CHORD_GRADIENT_STEPS = 18\n",
    "    CHORD_ARC_POINTS = 12\n",
    "    CHORD_CURVE_POINTS = 50\n",
    "\n",
    "# Layer toggles (enable/disable chord and ring layers)\n",
    "SHOW_ACCUM_FLOW = False         # Accumulation score chords\n",
    "SHOW_LIT_FLOW = False               # Lit buy/sell chords\n",
    "SHOW_SHORT_NET_FLOW = False         # Net = short_buy_sum - short_sell_sum (raw volumes)\n",
    "SHOW_VWBR_Z = True                 # Finra buy-volume z-score chords (negative/positive)\n",
    "SHOW_FINRA_FLOW = False              # Finra buy-volume chords\n",
    "SHOW_VOLUME_RING = True             # Outer metric rings\n",
    "RING3_COLOR_MODE = \"VWBR_Z\"       # \"SHORT_RATIO\", \"VWBR\", \"VWBR_Z\"\n",
    "RING3_ZSCORE_SPAN = 2.0         # VWBR Z range mapped to full brightness\n",
    "RING3_VWBR_SPAN = 1          # VWBR +/- span mapped to full brightness\n",
    "RING3_BRIGHTNESS_MIN = 0.2    # Darkest category tint\n",
    "RING3_BRIGHTNESS_MAX = 0.9    # Brightest category tint\n",
    "\n",
    "# Optional: time-fade chords\n",
    "SHOW_TIME_FADE_CHORDS = False            # Fade chord alpha by recency\n",
    "TIME_FADE_USE_DAILY_EDGES = True       # Use per-day edges (more ribbons)\n",
    "TIME_FADE_MIN_ALPHA = 0.2               # Oldest ribbon alpha\n",
    "TIME_FADE_MAX_ALPHA = 1.0               # Newest ribbon alpha\n",
    "TIME_FADE_POWER = 10                     # Curve shaping (1=linear)\n",
    "\n",
    "# Adjust density when time-fade is enabled (halve values to reduce visual clutter)\n",
    "if SHOW_TIME_FADE_CHORDS:\n",
    "    TOP_K_WINNERS = TOP_K_WINNERS // 2\n",
    "    TOP_K_LOSERS = TOP_K_LOSERS // 2\n",
    "    MAX_EDGES_PER_METRIC = MAX_EDGES_PER_METRIC // 2\n",
    "\n",
    "# Figure layout (rarely changed)\n",
    "FIGURE_SIZE = (20, 20)          # Width, height in inches\n",
    "PLOT_CENTER_X = 0.55            # Plot center (figure fraction)\n",
    "PLOT_CENTER_Y = 0.52\n",
    "MAIN_AX_SIZE = 0.82             # Square axes size (figure fraction)\n",
    "\n",
    "# Watermark\n",
    "WATERMARK_PATH = r'C:\\Users\\fvign\\Dropbox\\Vscode\\darkpool\\cowwbell_waterrmark.png'\n",
    "WATERMARK_WIDTH = 0.15        # Figure fraction\n",
    "WATERMARK_ALPHA = 0.6\n",
    "\n",
    "# Typography + table sizing\n",
    "FIGURE_SCALE = min(FIGURE_SIZE) / 12.0\n",
    "TITLE_FONTSIZE = 14 * FIGURE_SCALE\n",
    "SUBTITLE_FONTSIZE = 10 * FIGURE_SCALE\n",
    "TICKER_FONTSIZE = 9 * FIGURE_SCALE\n",
    "LEGEND_TITLE_FONTSIZE = 10 * FIGURE_SCALE\n",
    "LEGEND_LABEL_FONTSIZE = 8 * FIGURE_SCALE\n",
    "RING_TITLE_FONTSIZE = 9 * FIGURE_SCALE\n",
    "RING_LABEL_FONTSIZE = 7 * FIGURE_SCALE\n",
    "TABLE_FONTSIZE = 9 * FIGURE_SCALE\n",
    "LEGEND_LINEWIDTH = 4 * FIGURE_SCALE\n",
    "TABLE_COL1_WIDTH = 25         # Table column widths (characters)\n",
    "TABLE_COL2_WIDTH = 20\n",
    "TABLE_COL3_WIDTH = 20\n",
    "\n",
    "\n",
    "def _normalize_ticker_types(value):\n",
    "    if value is None:\n",
    "        return []\n",
    "    if isinstance(value, (list, tuple, set)):\n",
    "        return [str(v).upper() for v in value]\n",
    "    return [str(value).upper()]\n",
    "\n",
    "_selected_types = _normalize_ticker_types(TICKER_TYPE)\n",
    "if not _selected_types:\n",
    "    _selected_types = [\"ALL\"]\n",
    "unknown_types = [t for t in _selected_types if t not in TICKER_TYPE_OPTIONS]\n",
    "if unknown_types:\n",
    "    raise ValueError(f\"Unknown TICKER_TYPE: {unknown_types}\")\n",
    "if \"ALL\" in _selected_types:\n",
    "    _selected_types = [t for t in TICKER_TYPE_OPTIONS if t != \"ALL\"]\n",
    "\n",
    "ENABLED_GROUPS = {\n",
    "    \"GLOBAL_MACRO\": \"GLOBAL\" in _selected_types,\n",
    "    \"MAG8\": \"MAG8\" in _selected_types,\n",
    "    \"THEMATIC_SECTORS\": \"THEMATIC\" in _selected_types,\n",
    "    \"SECTOR_CORE\": \"SECTOR\" in _selected_types,\n",
    "    \"COMMODITIES\": \"COMMODITIES\" in _selected_types,\n",
    "    \"RATES_CREDIT\": \"RATES\" in _selected_types,\n",
    "    \"SPECULATIVE\": \"SPECULATIVE\" in _selected_types,\n",
    "    \"CRYPTO\": \"CRYPTO\" in _selected_types,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b232f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 â€“ DB connection via centralized config\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Use centralized database path from config (DATA_ROOT convention)\n",
    "sys.path.insert(0, str(Path('.').resolve().parent / 'darkpool_analysis'))\n",
    "try:\n",
    "    from config import load_config\n",
    "    config = load_config()\n",
    "    DB_PATH = config.db_path\n",
    "except ImportError:\n",
    "    # Fallback: try direct import of db_path helper\n",
    "    from db_path import get_db_path\n",
    "    DB_PATH = get_db_path()\n",
    "\n",
    "def connect_db(db_path):\n",
    "    \"\"\"Connect to DuckDB database.\"\"\"\n",
    "    db_path = Path(db_path)\n",
    "    return duckdb.connect(database=str(db_path), read_only=True), 'duckdb'\n",
    "\n",
    "def get_tables(conn, db_type):\n",
    "    if db_type == 'duckdb':\n",
    "        return [r[0] for r in conn.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema='main'\").fetchall()]\n",
    "    else:\n",
    "        return [r[0] for r in conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()]\n",
    "\n",
    "def get_columns(conn, db_type, table):\n",
    "    if db_type == 'duckdb':\n",
    "        return [r[0] for r in conn.execute(f\"SELECT column_name FROM information_schema.columns WHERE table_name='{table}'\").fetchall()]\n",
    "    else:\n",
    "        return [r[1] for r in conn.execute(f\"PRAGMA table_info('{table}')\").fetchall()]\n",
    "\n",
    "def pick_column(columns, candidates):\n",
    "    cols_lower = {c.lower(): c for c in columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def quote_ident(name):\n",
    "    return f'\"{name}\"'\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Column name candidates for auto-detection\n",
    "# ---------------------------------------------------------------------------\n",
    "TABLE_CANDIDATES = ['daily_metrics', 'scanner_daily_metrics', 'darkpool_metrics', 'metrics', 'darkpool', 'accumulation']\n",
    "TICKER_COL_CANDIDATES = ['ticker', 'symbol', 'stock', 'name']\n",
    "DATE_COL_CANDIDATES = ['date', 'trade_date', 'dt', 'timestamp']\n",
    "ACCUM_COL_CANDIDATES = ['accumulation_score_display', 'accum_score_display', \n",
    "                        'accumulation_score', 'accum_score', 'accumulation', 'accum']\n",
    "\n",
    "# Volume column candidates\n",
    "SHORT_BUY_CANDIDATES = ['finra_buy_volume', 'short_buy_volume', 'short_buy', 'short_buy_vol']\n",
    "SHORT_SELL_CANDIDATES = ['short_sell_volume', 'short_sell', 'short_sell_vol']\n",
    "LIT_BUY_CANDIDATES = ['lit_buy_volume', 'lit_buy', 'lit_buy_vol']\n",
    "LIT_SELL_CANDIDATES = ['lit_sell_volume', 'lit_sell', 'lit_sell_vol']\n",
    "OTC_VOLUME_CANDIDATES = ['otc_off_exchange_volume', 'otc_volume', 'dark_volume']\n",
    "LIT_TOTAL_CANDIDATES = ['lit_total_volume', 'lit_volume', 'lit_total']\n",
    "FINRA_BUY_CANDIDATES = ['finra_buy_volume', 'finra_buy', 'finra_buy_vol']\n",
    "FINRA_BUY_Z_CANDIDATES = ['finra_buy_volume_z', 'finra_buy_z', 'finra_buy_vol_z']\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Connect to database (using centralized path)\n",
    "# ---------------------------------------------------------------------------\n",
    "print(f'Using DB: {DB_PATH}')\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    print(f'ERROR: Database not found at {DB_PATH}')\n",
    "    print('Make sure DATA_ROOT is set in .env or the database exists at the fallback location.')\n",
    "    raise SystemExit\n",
    "\n",
    "conn, DB_TYPE = connect_db(DB_PATH)\n",
    "print(f'Connected via {DB_TYPE}')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Auto-detect table and column names\n",
    "# ---------------------------------------------------------------------------\n",
    "tables = get_tables(conn, DB_TYPE)\n",
    "print(f'Tables found: {tables}')\n",
    "\n",
    "SELECT_TABLE = None\n",
    "for cand in TABLE_CANDIDATES:\n",
    "    matching = [t for t in tables if t.lower() == cand.lower()]\n",
    "    if matching:\n",
    "        SELECT_TABLE = matching[0]\n",
    "        break\n",
    "if not SELECT_TABLE and tables:\n",
    "    SELECT_TABLE = tables[0]\n",
    "\n",
    "if not SELECT_TABLE:\n",
    "    print('No suitable table found in database.')\n",
    "    conn.close()\n",
    "    raise SystemExit\n",
    "\n",
    "print(f'Using table: {SELECT_TABLE}')\n",
    "\n",
    "columns = get_columns(conn, DB_TYPE, SELECT_TABLE)\n",
    "print(f'Columns in {SELECT_TABLE}: {columns}')\n",
    "\n",
    "TICKER_COL = pick_column(columns, TICKER_COL_CANDIDATES)\n",
    "DATE_COL = pick_column(columns, DATE_COL_CANDIDATES)\n",
    "ACCUM_COL = pick_column(columns, ACCUM_COL_CANDIDATES)\n",
    "\n",
    "if not TICKER_COL:\n",
    "    print(f'Could not find ticker column. Available: {columns}')\n",
    "    conn.close()\n",
    "    raise SystemExit\n",
    "if not DATE_COL:\n",
    "    print(f'Could not find date column. Available: {columns}')\n",
    "    conn.close()\n",
    "    raise SystemExit\n",
    "if not ACCUM_COL:\n",
    "    print(f'Could not find accumulation column. Available: {columns}')\n",
    "    conn.close()\n",
    "    raise SystemExit\n",
    "\n",
    "print(f'Column mapping: ticker={TICKER_COL}, date={DATE_COL}, accum={ACCUM_COL}')\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "def build_ticker_universe():\n",
    "    seen = set()\n",
    "    ordered = []\n",
    "    categories = {}\n",
    "    enabled = globals().get(\"ENABLED_GROUPS\", {})\n",
    "    group_defs = [\n",
    "        (\"GLOBAL_MACRO\", globals().get(\"GLOBAL_MACRO_TICKERS\", [])),\n",
    "        (\"MAG8\", globals().get(\"MAG8_TICKERS\", [])),\n",
    "        (\"THEMATIC_SECTORS\", globals().get(\"THEMATIC_SECTORS_TICKERS\", [])),\n",
    "        (\"SECTOR_CORE\", globals().get(\"SECTOR_CORE_TICKERS\", [])),\n",
    "        (\"COMMODITIES\", globals().get(\"COMMODITIES_TICKERS\", [])),\n",
    "        (\"RATES_CREDIT\", globals().get(\"RATES_CREDIT_TICKERS\", [])),\n",
    "        (\"SPECULATIVE\", globals().get(\"SPECULATIVE_TICKERS\", [])),\n",
    "        (\"CRYPTO\", globals().get(\"CRYPTO_TICKERS\", [])),\n",
    "    ]\n",
    "    for group_name, tickers in group_defs:\n",
    "        if not enabled.get(group_name, True):\n",
    "            continue\n",
    "        for t in tickers:\n",
    "            if t not in seen:\n",
    "                seen.add(t)\n",
    "                ordered.append(t)\n",
    "                categories[t] = group_name\n",
    "    return ordered, categories\n",
    "\n",
    "\n",
    "ticker_order, ticker_category = build_ticker_universe()\n",
    "print(\"Ticker universe size:\", len(ticker_order))\n",
    "print(\"Enabled groups:\", {k: v for k, v in ENABLED_GROUPS.items()})\n",
    "ticker_list = [t.upper() for t in ticker_order]\n",
    "import re\n",
    "\n",
    "conn, DB_TYPE = connect_db(DB_PATH)\n",
    "\n",
    "if DB_TYPE == 'duckdb':\n",
    "    date_expr = f\"CAST({quote_ident(DATE_COL)} AS DATE)\"\n",
    "    accum_expr = f\"{quote_ident(ACCUM_COL)}\"\n",
    "else:\n",
    "    date_expr = f\"DATE({quote_ident(DATE_COL)})\"\n",
    "    accum_expr = f\"{quote_ident(ACCUM_COL)}\"\n",
    "\n",
    "placeholders = ','.join(['?'] * len(ticker_list))\n",
    "query = (\n",
    "    f\"SELECT UPPER({quote_ident(TICKER_COL)}) AS ticker, \"\n",
    "    f\"{date_expr} AS date, {accum_expr} AS accumulation_score \"\n",
    "    f\"FROM {quote_ident(SELECT_TABLE)} \"\n",
    "    f\"WHERE UPPER({quote_ident(TICKER_COL)}) IN ({placeholders})\"\n",
    ")\n",
    "\n",
    "print(f\"Using accumulation table: {SELECT_TABLE} (ticker={TICKER_COL}, date={DATE_COL}, accum={ACCUM_COL})\")\n",
    "\n",
    "try:\n",
    "    if DB_TYPE == 'duckdb':\n",
    "        df_raw = conn.execute(query, ticker_list).df()\n",
    "    else:\n",
    "        df_raw = pd.read_sql_query(query, conn, params=ticker_list)\n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "if df_raw.empty:\n",
    "    print('No data returned for the specified tickers.')\n",
    "    raise SystemExit\n",
    "\n",
    "print(\"Accumulation rows loaded:\", len(df_raw))\n",
    "print(\"Accumulation table date range:\", df_raw[\"date\"].min(), \"->\", df_raw[\"date\"].max())\n",
    "print(\"Accumulation raw sample:\", df_raw[\"accumulation_score\"].head(5).tolist())\n",
    "df_raw['date'] = pd.to_datetime(df_raw['date'], errors='coerce').dt.date\n",
    "df_raw['ticker'] = df_raw['ticker'].str.upper()\n",
    "\n",
    "def fetch_accum_column(accum_col_name):\n",
    "    conn, db_type = connect_db(DB_PATH)\n",
    "    try:\n",
    "        if db_type == 'duckdb':\n",
    "            date_expr = f\"CAST({quote_ident(DATE_COL)} AS DATE)\"\n",
    "        else:\n",
    "            date_expr = f\"DATE({quote_ident(DATE_COL)})\"\n",
    "        query = (\n",
    "            f\"SELECT UPPER({quote_ident(TICKER_COL)}) AS ticker, \"\n",
    "            f\"{date_expr} AS date, {quote_ident(accum_col_name)} AS accumulation_score \"\n",
    "            f\"FROM {quote_ident(SELECT_TABLE)} \"\n",
    "            f\"WHERE UPPER({quote_ident(TICKER_COL)}) IN ({placeholders})\"\n",
    "        )\n",
    "        if db_type == 'duckdb':\n",
    "            return conn.execute(query, ticker_list).df()\n",
    "        return pd.read_sql_query(query, conn, params=ticker_list)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def normalize_accum_df(df):\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.date\n",
    "    df['ticker'] = df['ticker'].str.upper()\n",
    "    df['accumulation_score'] = df['accumulation_score'].apply(parse_accum)\n",
    "    return df.dropna(subset=['date', 'accumulation_score'])\n",
    "\n",
    "def parse_accum(value):\n",
    "    if value is None:\n",
    "        return np.nan\n",
    "    if isinstance(value, (int, float, np.number)):\n",
    "        if np.isnan(value):\n",
    "            return np.nan\n",
    "        return float(value)\n",
    "    s = str(value).strip()\n",
    "    if not s:\n",
    "        return np.nan\n",
    "    s = s.replace('%', '').replace(',', '')\n",
    "    m = re.search(r'[-+]?\\d+\\.?\\d*', s)\n",
    "    return float(m.group(0)) if m else np.nan\n",
    "\n",
    "df_raw = normalize_accum_df(df_raw)\n",
    "ACCUM_COL_SELECTED = ACCUM_COL\n",
    "if df_raw.empty:\n",
    "    print('Primary accumulation column returned no usable values; trying fallbacks...')\n",
    "    conn, db_type = connect_db(DB_PATH)\n",
    "    try:\n",
    "        cols = get_columns(conn, db_type, SELECT_TABLE)\n",
    "    finally:\n",
    "        conn.close()\n",
    "    candidates = []\n",
    "    for cand in [\n",
    "        'accumulation_score_display', 'accum_score_display',\n",
    "        'accumulation_score', 'accum_score', 'accumulation', 'accum'\n",
    "    ]:\n",
    "        col = pick_column(cols, [cand])\n",
    "        if col and col not in candidates:\n",
    "            candidates.append(col)\n",
    "    candidates = [c for c in candidates if c != ACCUM_COL]\n",
    "    print('Accumulation fallback candidates:', candidates)\n",
    "    for col in candidates:\n",
    "        print('Trying accumulation column:', col)\n",
    "        df_try = fetch_accum_column(col)\n",
    "        print('Fallback rows loaded:', len(df_try))\n",
    "        print('Fallback raw sample:', df_try['accumulation_score'].head(5).tolist())\n",
    "        df_try = normalize_accum_df(df_try)\n",
    "        if not df_try.empty:\n",
    "            ACCUM_COL_SELECTED = col\n",
    "            df_raw = df_try\n",
    "            break\n",
    "\n",
    "if df_raw.empty:\n",
    "    print('All accumulation score rows are null after parsing.')\n",
    "    raise SystemExit\n",
    "print('Using accumulation column:', ACCUM_COL_SELECTED)\n",
    "\n",
    "df_raw_full = df_raw.copy()\n",
    "\n",
    "max_date = df_raw['date'].max()\n",
    "if END_DATE is None or str(END_DATE).strip() == '':\n",
    "    END_DATE_RESOLVED = max_date\n",
    "else:\n",
    "    END_DATE_RESOLVED = pd.to_datetime(END_DATE).date()\n",
    "    if END_DATE_RESOLVED > max_date:\n",
    "        print(f'END_DATE {END_DATE_RESOLVED} exceeds DB max date {max_date}; using max date.')\n",
    "        END_DATE_RESOLVED = max_date\n",
    "\n",
    "flow_days = int(FLOW_PERIOD_DAYS) if FLOW_PERIOD_DAYS and int(FLOW_PERIOD_DAYS) > 0 else 1\n",
    "print(\"Flow period days:\", flow_days, \"END_DATE:\", END_DATE_RESOLVED)\n",
    "\n",
    "all_dates = sorted([d for d in df_raw[\"date\"].unique() if pd.notna(d)])\n",
    "end_dates = [d for d in all_dates if d <= END_DATE_RESOLVED]\n",
    "window_dates = end_dates[-flow_days:]\n",
    "if not window_dates:\n",
    "    print(\"No dates available within FLOW_PERIOD_DAYS window.\")\n",
    "    raise SystemExit\n",
    "print(\"Window date range used:\", window_dates[0], \"->\", window_dates[-1], \"count:\", len(window_dates))\n",
    "df_accum_daily = df_raw_full.sort_values([\"ticker\", \"date\"]).copy()\n",
    "df_accum_daily[\"accum_net\"] = df_accum_daily.groupby(\"ticker\")[\"accumulation_score\"].diff()\n",
    "df_accum_daily[\"accum_net\"] = df_accum_daily[\"accum_net\"].fillna(0.0)\n",
    "df_accum_daily = df_accum_daily[df_accum_daily[\"date\"].isin(window_dates)][[\"ticker\", \"date\", \"accum_net\"]]\n",
    "df_raw = df_raw[df_raw[\"date\"].isin(window_dates)]\n",
    "print(\"Accumulation window rows:\", len(df_raw))\n",
    "missing = sorted(set(ticker_list) - set(df_raw[\"ticker\"].unique()))\n",
    "if missing:\n",
    "    print(\"Missing tickers in accumulation window:\", missing)\n",
    "if df_raw.empty:\n",
    "    print(\"No accumulation data within the selected window.\")\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "def tail_n(df_ticker, n, end_date):\n",
    "    df = df_ticker[df_ticker['date'] <= end_date].sort_values('date')\n",
    "    if df.empty:\n",
    "        return df\n",
    "    return df.tail(n)\n",
    "\n",
    "\n",
    "rows = []\n",
    "for ticker in ticker_order:\n",
    "    df_t = df_raw[df_raw['ticker'] == ticker]\n",
    "    tail = tail_n(df_t, flow_days, END_DATE_RESOLVED)\n",
    "    if len(tail) < 2:\n",
    "        a_start = None\n",
    "        a_end = None\n",
    "    else:\n",
    "        a_start = float(tail['accumulation_score'].iloc[0])\n",
    "        a_end = float(tail['accumulation_score'].iloc[-1])\n",
    "    rows.append({\n",
    "        'ticker': ticker,\n",
    "        'category': ticker_category.get(ticker, 'UNKNOWN'),\n",
    "        'A_end': a_end,\n",
    "        'A_start': a_start,\n",
    "        'end_date': END_DATE_RESOLVED,\n",
    "        'start_date': tail['date'].iloc[0] if len(tail) else None,\n",
    "        'samples': len(tail),\n",
    "    })\n",
    "\n",
    "df_scores = pd.DataFrame(rows)\n",
    "\n",
    "# --- Volume data discovery (lit/short buy/sell + new columns for rings) ---\n",
    "LIT_BUY_CANDIDATES = ['lit_buy_volume', 'lit_buy_vol', 'lit_buy']\n",
    "LIT_SELL_CANDIDATES = ['lit_sell_volume', 'lit_sell_vol', 'lit_sell']\n",
    "SHORT_BUY_CANDIDATES = ['finra_buy_volume', 'short_buy_volume', 'short_buy_vol', 'short_buy']\n",
    "SHORT_SELL_CANDIDATES = ['short_sell_volume', 'short_sell_vol', 'short_sell']\n",
    "# New columns for rings\n",
    "OTC_VOLUME_CANDIDATES = ['otc_off_exchange_volume', 'otc_volume', 'dark_volume']\n",
    "LIT_TOTAL_CANDIDATES = ['lit_total_volume', 'lit_volume', 'lit_total']\n",
    "FINRA_BUY_CANDIDATES = ['finra_buy_volume', 'finra_buy', 'finra_buy_vol']\n",
    "FINRA_BUY_Z_CANDIDATES = ['finra_buy_volume_z', 'finra_buy_z', 'finra_buy_vol_z']\n",
    "SHORT_RATIO_CANDIDATES = ['short_ratio']\n",
    "SHORT_BUY_SELL_RATIO_CANDIDATES = ['short_buy_sell_ratio', 'short_buy_sell_ratio_z']\n",
    "VWBR_CANDIDATES = ['vw_flow', 'vwbr', 'vw_buy_ratio', 'vw_buy_sell_ratio']\n",
    "VWBR_Z_CANDIDATES = ['vw_flow_z', 'vwbr_z', 'vw_buy_ratio_z']\n",
    "\n",
    "\n",
    "def find_volume_table(conn, db_type):\n",
    "    tables = get_tables(conn, db_type)  # Fixed: was list_tables\n",
    "    candidates = []\n",
    "    for table in tables:\n",
    "        try:\n",
    "            cols = get_columns(conn, db_type, table)\n",
    "        except Exception:\n",
    "            continue\n",
    "        ticker_col = pick_column(cols, TICKER_COL_CANDIDATES)\n",
    "        date_col = pick_column(cols, DATE_COL_CANDIDATES)\n",
    "        lit_buy_col = pick_column(cols, LIT_BUY_CANDIDATES)\n",
    "        lit_sell_col = pick_column(cols, LIT_SELL_CANDIDATES)\n",
    "        short_buy_col = pick_column(cols, SHORT_BUY_CANDIDATES)\n",
    "        short_sell_col = pick_column(cols, SHORT_SELL_CANDIDATES)\n",
    "        # New columns (optional but preferred)\n",
    "        otc_vol_col = pick_column(cols, OTC_VOLUME_CANDIDATES)\n",
    "        lit_total_col = pick_column(cols, LIT_TOTAL_CANDIDATES)\n",
    "        finra_buy_col = pick_column(cols, FINRA_BUY_CANDIDATES)\n",
    "        finra_buy_z_col = pick_column(cols, FINRA_BUY_Z_CANDIDATES)\n",
    "        short_ratio_col = pick_column(cols, SHORT_RATIO_CANDIDATES)\n",
    "        vwbr_col = pick_column(cols, VWBR_CANDIDATES)\n",
    "        vwbr_z_col = pick_column(cols, VWBR_Z_CANDIDATES)\n",
    "        if not (ticker_col and date_col and lit_buy_col and lit_sell_col and short_buy_col and short_sell_col):\n",
    "            continue\n",
    "        row_count = None\n",
    "        distinct_tickers = None\n",
    "        try:\n",
    "            row_count = conn.execute(\n",
    "                f'SELECT COUNT(*) FROM {quote_ident(table)}'\n",
    "            ).fetchone()[0]\n",
    "            distinct_tickers = conn.execute(\n",
    "                f'SELECT COUNT(DISTINCT {quote_ident(ticker_col)}) FROM {quote_ident(table)}'\n",
    "            ).fetchone()[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "        candidates.append({\n",
    "            'table': table,\n",
    "            'ticker_col': ticker_col,\n",
    "            'date_col': date_col,\n",
    "            'lit_buy_col': lit_buy_col,\n",
    "            'lit_sell_col': lit_sell_col,\n",
    "            'short_buy_col': short_buy_col,\n",
    "            'short_sell_col': short_sell_col,\n",
    "            'otc_vol_col': otc_vol_col,\n",
    "            'lit_total_col': lit_total_col,\n",
    "            'finra_buy_col': finra_buy_col,\n",
    "            'finra_buy_z_col': finra_buy_z_col,\n",
    "            'short_ratio_col': short_ratio_col,\n",
    "            'vwbr_col': vwbr_col,\n",
    "            'vwbr_z_col': vwbr_z_col,\n",
    "            'short_buy_sell_ratio_col': pick_column(cols, SHORT_BUY_SELL_RATIO_CANDIDATES),\n",
    "            'row_count': row_count,\n",
    "            'distinct_tickers': distinct_tickers,\n",
    "        })\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates.sort(key=lambda r: (r['row_count'] or 0, r['distinct_tickers'] or 0), reverse=True)\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "conn, DB_TYPE = connect_db(DB_PATH)\n",
    "try:\n",
    "    volume_info = find_volume_table(conn, DB_TYPE)\n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "VOLUME_DATA_AVAILABLE = volume_info is not None\n",
    "if VOLUME_DATA_AVAILABLE:\n",
    "    print(\"Volume table selected:\", volume_info[\"table\"])\n",
    "    print(\"Volume columns:\", {k: volume_info[k] for k in [\"ticker_col\", \"date_col\", \"lit_buy_col\", \"lit_sell_col\", \"short_buy_col\", \"short_sell_col\"]})\n",
    "    print(\"Ring columns:\", {k: volume_info.get(k) for k in [\"otc_vol_col\", \"lit_total_col\", \"finra_buy_col\", \"vwbr_col\", \"vwbr_z_col\"]})\n",
    "    print(\"Finra buy z column:\", volume_info.get(\"finra_buy_z_col\"))\n",
    "else:\n",
    "    print(\"No volume table found with lit/short buy/sell columns.\")\n",
    "\n",
    "if VOLUME_DATA_AVAILABLE:\n",
    "    conn, DB_TYPE = connect_db(DB_PATH)\n",
    "    try:\n",
    "        if DB_TYPE == 'duckdb':\n",
    "            date_expr = f\"CAST({quote_ident(volume_info['date_col'])} AS DATE)\"\n",
    "            num_cast = \"TRY_CAST\"\n",
    "        else:\n",
    "            date_expr = f\"DATE({quote_ident(volume_info['date_col'])})\"\n",
    "            num_cast = \"CAST\"\n",
    "\n",
    "        # Build query with all columns including new ring columns\n",
    "        select_cols = [\n",
    "            f\"UPPER({quote_ident(volume_info['ticker_col'])}) AS ticker\",\n",
    "            f\"{date_expr} AS date\",\n",
    "            f\"{num_cast}({quote_ident(volume_info['lit_buy_col'])} AS DOUBLE) AS lit_buy\",\n",
    "            f\"{num_cast}({quote_ident(volume_info['lit_sell_col'])} AS DOUBLE) AS lit_sell\",\n",
    "            f\"{num_cast}({quote_ident(volume_info['short_buy_col'])} AS DOUBLE) AS short_buy\",\n",
    "            f\"{num_cast}({quote_ident(volume_info['short_sell_col'])} AS DOUBLE) AS short_sell\",\n",
    "        ]\n",
    "        # Add optional ring columns if available\n",
    "        if volume_info.get('otc_vol_col'):\n",
    "            select_cols.append(f\"{num_cast}({quote_ident(volume_info['otc_vol_col'])} AS DOUBLE) AS otc_volume\")\n",
    "        if volume_info.get('lit_total_col'):\n",
    "            select_cols.append(f\"{num_cast}({quote_ident(volume_info['lit_total_col'])} AS DOUBLE) AS lit_total\")\n",
    "        # Add lit buy/sell for Ring 2 coloring (lit buy ratio)\n",
    "        select_cols.append(f\"{num_cast}({quote_ident(volume_info['lit_buy_col'])} AS DOUBLE) AS lit_buy_volume\")\n",
    "        select_cols.append(f\"{num_cast}({quote_ident(volume_info['lit_sell_col'])} AS DOUBLE) AS lit_sell_volume\")\n",
    "        if volume_info.get('finra_buy_col'):\n",
    "            select_cols.append(f\"{num_cast}({quote_ident(volume_info['finra_buy_col'])} AS DOUBLE) AS finra_buy\")\n",
    "        if volume_info.get('finra_buy_z_col'):\n",
    "            select_cols.append(f\"{num_cast}({quote_ident(volume_info['finra_buy_z_col'])} AS DOUBLE) AS finra_buy_z\")\n",
    "        if volume_info.get('vwbr_col'):\n",
    "            select_cols.append(f\"{num_cast}({quote_ident(volume_info['vwbr_col'])} AS DOUBLE) AS vwbr\")\n",
    "        if volume_info.get('vwbr_z_col'):\n",
    "            select_cols.append(f\"{num_cast}({quote_ident(volume_info['vwbr_z_col'])} AS DOUBLE) AS vwbr_z\")\n",
    "\n",
    "        # Add short_ratio for Ring 3 coloring\n",
    "        if volume_info.get('short_ratio_col'):\n",
    "            select_cols.append(f\"{num_cast}({quote_ident(volume_info['short_ratio_col'])} AS DOUBLE) AS short_ratio\")\n",
    "        # Add short_buy_sell_ratio for Ring 3 coloring (FINRA short sale buy/sell ratio)\n",
    "        if volume_info.get('short_buy_sell_ratio_col'):\n",
    "            select_cols.append(f\"{num_cast}({quote_ident(volume_info['short_buy_sell_ratio_col'])} AS DOUBLE) AS short_buy_sell_ratio\")\n",
    "\n",
    "        query = (\n",
    "            f\"SELECT {', '.join(select_cols)} \"\n",
    "            f\"FROM {quote_ident(volume_info['table'])} \"\n",
    "            f\"WHERE UPPER({quote_ident(volume_info['ticker_col'])}) IN ({placeholders})\"\n",
    "        )\n",
    "\n",
    "        if DB_TYPE == 'duckdb':\n",
    "            df_vol_raw = conn.execute(query, ticker_list).df()\n",
    "        else:\n",
    "            df_vol_raw = pd.read_sql_query(query, conn, params=ticker_list)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    df_vol_raw['date'] = pd.to_datetime(df_vol_raw['date'], errors='coerce').dt.date\n",
    "    print('Volume table rows loaded:', len(df_vol_raw))\n",
    "    print('Volume table date range:', df_vol_raw['date'].min(), '->', df_vol_raw['date'].max())\n",
    "    df_vol_raw = df_vol_raw[df_vol_raw['date'].isin(window_dates)]\n",
    "    print('Volume window date range:', df_vol_raw['date'].min(), '->', df_vol_raw['date'].max(), 'rows:', len(df_vol_raw))\n",
    "    df_vol_raw['ticker'] = df_vol_raw['ticker'].str.upper()\n",
    "    for col in ['lit_buy', 'lit_sell', 'short_buy', 'short_sell']:\n",
    "        df_vol_raw[col] = pd.to_numeric(df_vol_raw[col], errors='coerce')\n",
    "    # Also convert new columns if present\n",
    "    for col in ['otc_volume', 'lit_total', 'finra_buy', 'finra_buy_z', 'vwbr', 'vwbr_z']:\n",
    "        if col in df_vol_raw.columns:\n",
    "            df_vol_raw[col] = pd.to_numeric(df_vol_raw[col], errors='coerce')\n",
    "    df_vol_raw = df_vol_raw.dropna(subset=['date'])\n",
    "    \n",
    "    df_lit_daily = (\n",
    "        df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n",
    "        .agg({'lit_buy': 'sum', 'lit_sell': 'sum'})\n",
    "    )\n",
    "    df_lit_daily['lit_net'] = df_lit_daily['lit_buy'] - df_lit_daily['lit_sell']\n",
    "    df_lit_daily['lit_total'] = df_lit_daily['lit_buy'] + df_lit_daily['lit_sell']\n",
    "    df_lit_daily['lit_buy_ratio'] = df_lit_daily['lit_buy'] / df_lit_daily['lit_total'].replace(0, np.nan)\n",
    "    df_lit_daily['lit_buy_ratio'] = df_lit_daily['lit_buy_ratio'].fillna(0.5)\n",
    "    df_short_daily = (\n",
    "        df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n",
    "        .agg({'short_buy': 'sum', 'short_sell': 'sum'})\n",
    "    )\n",
    "    df_short_daily['short_net'] = df_short_daily['short_buy'] - df_short_daily['short_sell']\n",
    "\n",
    "    # Create dark/lit daily data for Ring 2\n",
    "    if 'otc_volume' in df_vol_raw.columns and 'lit_total' in df_vol_raw.columns:\n",
    "        df_dark_lit_daily = (\n",
    "            df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n",
    "            .agg({'otc_volume': 'sum', 'lit_total': 'sum'})\n",
    "        )\n",
    "        df_dark_lit_daily['total_volume'] = df_dark_lit_daily['otc_volume'] + df_dark_lit_daily['lit_total']\n",
    "        df_dark_lit_daily['dark_ratio'] = df_dark_lit_daily['otc_volume'] / df_dark_lit_daily['total_volume'].replace(0, np.nan)\n",
    "        df_dark_lit_daily['dark_ratio'] = df_dark_lit_daily['dark_ratio'].fillna(0.5)\n",
    "        print('Dark/Lit daily data created:', len(df_dark_lit_daily), 'rows')\n",
    "\n",
    "        # Query 20-day lookback for dark_ratio z-score normalization\n",
    "        DARK_LIT_LOOKBACK_DAYS = 20\n",
    "        latest_date = pd.Timestamp(max(window_dates))\n",
    "        lookback_start = latest_date - pd.Timedelta(days=DARK_LIT_LOOKBACK_DAYS + 10)  # buffer for weekends/holidays\n",
    "\n",
    "        try:\n",
    "            conn, db_type = connect_db(DB_PATH)\n",
    "            query_lookback = (\n",
    "                f\"SELECT UPPER({quote_ident(volume_info['ticker_col'])}) AS ticker, \"\n",
    "                f\"{date_expr} AS date, \"\n",
    "                f\"{num_cast}({quote_ident(volume_info['otc_vol_col'])} AS DOUBLE) AS otc_volume, \"\n",
    "                f\"{num_cast}({quote_ident(volume_info['lit_total_col'])} AS DOUBLE) AS lit_total \"\n",
    "                f\"FROM {quote_ident(volume_info['table'])} \"\n",
    "                f\"WHERE UPPER({quote_ident(volume_info['ticker_col'])}) IN ({placeholders}) \"\n",
    "                f\"AND {date_expr} >= '{lookback_start.strftime('%Y-%m-%d')}'\"\n",
    "            )\n",
    "            if db_type == 'duckdb':\n",
    "                df_dark_lit_lookback = conn.execute(query_lookback, ticker_list).df()\n",
    "            else:\n",
    "                df_dark_lit_lookback = pd.read_sql_query(query_lookback, conn, params=ticker_list)\n",
    "            conn.close()\n",
    "\n",
    "            df_dark_lit_lookback['total_volume'] = df_dark_lit_lookback['otc_volume'] + df_dark_lit_lookback['lit_total']\n",
    "            df_dark_lit_lookback['dark_ratio'] = df_dark_lit_lookback['otc_volume'] / df_dark_lit_lookback['total_volume'].replace(0, np.nan)\n",
    "            df_dark_lit_lookback['dark_ratio'] = df_dark_lit_lookback['dark_ratio'].fillna(0.5)\n",
    "\n",
    "            # Calculate per-ticker z-score stats from 20-day lookback\n",
    "            dark_ratio_stats = df_dark_lit_lookback.groupby('ticker')['dark_ratio'].agg(['mean', 'std']).reset_index()\n",
    "            dark_ratio_stats.columns = ['ticker', 'dark_ratio_mean', 'dark_ratio_std']\n",
    "            print(f'Dark/Lit 20-day lookback stats calculated for {len(dark_ratio_stats)} tickers')\n",
    "        except Exception as e:\n",
    "            print(f'Warning: Could not query 20-day lookback: {e}')\n",
    "            dark_ratio_stats = pd.DataFrame(columns=['ticker', 'dark_ratio_mean', 'dark_ratio_std'])\n",
    "    else:\n",
    "        df_dark_lit_daily = pd.DataFrame(columns=['ticker', 'date', 'otc_volume', 'lit_total', 'total_volume', 'dark_ratio'])\n",
    "        dark_ratio_stats = pd.DataFrame(columns=['ticker', 'dark_ratio_mean', 'dark_ratio_std'])\n",
    "        print('Dark/Lit columns not available - using empty dataframe')\n",
    "\n",
    "    # Create finra_buy daily data for Ring 3\n",
    "    if 'finra_buy' in df_vol_raw.columns:\n",
    "        df_finra_daily = (\n",
    "            df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n",
    "            .agg({'finra_buy': 'sum'})\n",
    "        )\n",
    "        print('Finra buy daily data created:', len(df_finra_daily), 'rows')\n",
    "\n",
    "        # Add short_ratio to df_finra_daily for Ring 3 coloring\n",
    "        if 'short_ratio' in df_vol_raw.columns:\n",
    "            df_short_ratio_daily = (\n",
    "                df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n",
    "                .agg({'short_ratio': 'mean'})\n",
    "            )\n",
    "            df_finra_daily = df_finra_daily.merge(df_short_ratio_daily, on=['ticker', 'date'], how='left')\n",
    "            df_finra_daily['short_ratio'] = df_finra_daily['short_ratio'].fillna(0.5)\n",
    "            print('Short ratio added to finra daily data')\n",
    "    else:\n",
    "        df_finra_daily = pd.DataFrame(columns=['ticker', 'date', 'finra_buy'])\n",
    "        print('Finra buy column not available - using empty dataframe')\n",
    "\n",
    "    vol_rows = []\n",
    "    for ticker in ticker_order:\n",
    "        df_t = df_vol_raw[df_vol_raw['ticker'] == ticker]\n",
    "        tail = tail_n(df_t, flow_days, END_DATE_RESOLVED)\n",
    "        if tail.empty:\n",
    "            vol_rows.append({\n",
    "                'ticker': ticker,\n",
    "                'lit_buy_sum': None,\n",
    "                'lit_sell_sum': None,\n",
    "                'short_buy_sum': None,\n",
    "                'short_sell_sum': None,\n",
    "                'finra_buy_sum': None,\n",
    "                'finra_buy_z_mean': None,\n",
    "                'volume_samples': 0,\n",
    "            })\n",
    "            continue\n",
    "        vol_rows.append({\n",
    "            'ticker': ticker,\n",
    "            'lit_buy_sum': float(tail['lit_buy'].sum(skipna=True)),\n",
    "            'lit_sell_sum': float(tail['lit_sell'].sum(skipna=True)),\n",
    "            'short_buy_sum': float(tail['short_buy'].sum(skipna=True)),\n",
    "            'short_sell_sum': float(tail['short_sell'].sum(skipna=True)),\n",
    "            'finra_buy_sum': float(tail['finra_buy'].sum(skipna=True)) if 'finra_buy' in tail.columns else None,\n",
    "            'finra_buy_z_mean': float(tail['finra_buy_z'].mean(skipna=True)) if 'finra_buy_z' in tail.columns else None,\n",
    "            'volume_samples': len(tail),\n",
    "        })\n",
    "\n",
    "    df_volume = pd.DataFrame(vol_rows)\n",
    "    total_samples = int(df_volume['volume_samples'].sum()) if not df_volume.empty else 0\n",
    "    lit_total = float(df_volume['lit_buy_sum'].fillna(0).sum() + df_volume['lit_sell_sum'].fillna(0).sum()) if not df_volume.empty else 0.0\n",
    "    short_total = float(df_volume['short_buy_sum'].fillna(0).sum() + df_volume['short_sell_sum'].fillna(0).sum()) if not df_volume.empty else 0.0\n",
    "    if total_samples == 0 or (lit_total == 0 and short_total == 0):\n",
    "        print('Volume data missing for selected period. Lit/short chords require non-zero buy/sell volume data.')\n",
    "        raise SystemExit\n",
    "else:\n",
    "    df_volume = pd.DataFrame(columns=[\n",
    "        'ticker', 'lit_buy_sum', 'lit_sell_sum', 'short_buy_sum', 'short_sell_sum', 'finra_buy_sum', 'finra_buy_z_mean', 'volume_samples'\n",
    "    ])\n",
    "    df_dark_lit_daily = pd.DataFrame(columns=['ticker', 'date', 'otc_volume', 'lit_total', 'total_volume', 'dark_ratio'])\n",
    "    df_finra_daily = pd.DataFrame(columns=['ticker', 'date', 'finra_buy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "def build_edges_from_value(df, value_col, top_k_winners, top_k_losers):\n",
    "    df = df[['ticker', value_col]].dropna().copy()\n",
    "    winners = df[df[value_col] > 0].nlargest(top_k_winners, value_col)\n",
    "    losers = df[df[value_col] < 0].copy()\n",
    "    losers['supply'] = -losers[value_col]\n",
    "    losers = losers.nlargest(top_k_losers, 'supply')\n",
    "\n",
    "    edges = []\n",
    "    total_demand = winners[value_col].sum() if not winners.empty else 0.0\n",
    "    total_supply = losers['supply'].sum() if not losers.empty else 0.0\n",
    "\n",
    "    if winners.empty or losers.empty:\n",
    "        return pd.DataFrame(edges), winners, losers, total_demand, total_supply\n",
    "\n",
    "    if DISTRIBUTION_MODE not in {'equal', 'demand_weighted'}:\n",
    "        raise ValueError('DISTRIBUTION_MODE must be \"equal\" or \"demand_weighted\"')\n",
    "\n",
    "    if DISTRIBUTION_MODE == 'equal':\n",
    "        for _, loser in losers.iterrows():\n",
    "            flow_each = loser['supply'] / len(winners)\n",
    "            for _, winner in winners.iterrows():\n",
    "                if flow_each >= MIN_EDGE_FLOW:\n",
    "                    edges.append({\n",
    "                        'source': loser['ticker'],\n",
    "                        'dest': winner['ticker'],\n",
    "                        'flow': float(flow_each),\n",
    "                    })\n",
    "    else:\n",
    "        if total_demand > 0:\n",
    "            for _, loser in losers.iterrows():\n",
    "                for _, winner in winners.iterrows():\n",
    "                    flow = loser['supply'] * (winner[value_col] / total_demand)\n",
    "                    if flow >= MIN_EDGE_FLOW:\n",
    "                        edges.append({\n",
    "                            'source': loser['ticker'],\n",
    "                            'dest': winner['ticker'],\n",
    "                            'flow': float(flow),\n",
    "                        })\n",
    "\n",
    "    edges_df = pd.DataFrame(edges)\n",
    "    if not edges_df.empty:\n",
    "        edges_df = edges_df.sort_values('flow', ascending=False).reset_index(drop=True)\n",
    "    return edges_df, winners, losers, total_demand, total_supply\n",
    "\n",
    "\n",
    "def build_edges_from_positive_value(df, value_col, top_k_high, top_k_low):\n",
    "    \"\"\"Build edges for metrics that are always positive (like finra_buy_volume).\n",
    "    Flow goes from low-value tickers to high-value tickers.\"\"\"\n",
    "    df = df[['ticker', value_col]].dropna().copy()\n",
    "    df = df[df[value_col] > 0]  # Only positive values\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), 0.0, 0.0\n",
    "    \n",
    "    median_val = df[value_col].median()\n",
    "    high_tickers = df[df[value_col] >= median_val].nlargest(top_k_high, value_col)\n",
    "    low_tickers = df[df[value_col] < median_val].nsmallest(top_k_low, value_col)\n",
    "    \n",
    "    if high_tickers.empty or low_tickers.empty:\n",
    "        return pd.DataFrame(), high_tickers, low_tickers, 0.0, 0.0\n",
    "    \n",
    "    total_high = high_tickers[value_col].sum()\n",
    "    total_low = low_tickers[value_col].sum()\n",
    "    \n",
    "    edges = []\n",
    "    for _, low_row in low_tickers.iterrows():\n",
    "        for _, high_row in high_tickers.iterrows():\n",
    "            # Flow proportional to the difference\n",
    "            flow = (high_row[value_col] - low_row[value_col]) * (low_row[value_col] / total_low)\n",
    "            if flow > MIN_EDGE_FLOW:\n",
    "                edges.append({\n",
    "                    'source': low_row['ticker'],\n",
    "                    'dest': high_row['ticker'],\n",
    "                    'flow': float(flow),\n",
    "                })\n",
    "    \n",
    "    edges_df = pd.DataFrame(edges)\n",
    "    if not edges_df.empty:\n",
    "        edges_df = edges_df.sort_values('flow', ascending=False).reset_index(drop=True)\n",
    "    return edges_df, high_tickers, low_tickers, total_high, total_low\n",
    "\n",
    "\n",
    "def build_edges_by_date(df, date_col, value_col, top_k_winners, top_k_losers, positive_only=False):\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=['source', 'dest', 'flow', 'date'])\n",
    "    if date_col not in df.columns or value_col not in df.columns:\n",
    "        return pd.DataFrame(columns=['source', 'dest', 'flow', 'date'])\n",
    "    edges_all = []\n",
    "    for dt, df_day in df.groupby(date_col):\n",
    "        if df_day.empty:\n",
    "            continue\n",
    "        if positive_only:\n",
    "            edges_df, _, _, _, _ = build_edges_from_positive_value(df_day, value_col, top_k_winners, top_k_losers)\n",
    "        else:\n",
    "            edges_df, _, _, _, _ = build_edges_from_value(df_day, value_col, top_k_winners, top_k_losers)\n",
    "        if edges_df is None or edges_df.empty:\n",
    "            continue\n",
    "        edges_df = edges_df.copy()\n",
    "        edges_df['date'] = dt\n",
    "        edges_all.append(edges_df)\n",
    "    if not edges_all:\n",
    "        return pd.DataFrame(columns=['source', 'dest', 'flow', 'date'])\n",
    "    return pd.concat(edges_all, ignore_index=True)\n",
    "\n",
    "\n",
    "# Accumulation flow (average level over window, centered by mean)\n",
    "if df_scores['A_end'].notna().any() and df_scores['A_start'].notna().any():\n",
    "    df_scores['delta'] = df_scores['A_end'] - df_scores['A_start']\n",
    "else:\n",
    "    df_scores['delta'] = np.nan\n",
    "\n",
    "df_accum_level = df_raw_full[df_raw_full['date'].isin(window_dates)][['ticker', 'date', 'accumulation_score']].copy()\n",
    "if 'accum_avg' in df_scores.columns:\n",
    "    df_scores = df_scores.drop(columns=['accum_avg', 'accum_centered'], errors='ignore')\n",
    "if not df_accum_level.empty:\n",
    "    df_accum_avg = df_accum_level.groupby('ticker')['accumulation_score'].mean()\n",
    "    df_scores = df_scores.merge(df_accum_avg.rename('accum_avg'), on='ticker', how='left')\n",
    "else:\n",
    "    df_scores['accum_avg'] = np.nan\n",
    "\n",
    "if df_scores['accum_avg'].notna().any():\n",
    "    mean_level = df_scores['accum_avg'].mean()\n",
    "    df_scores['accum_centered'] = df_scores['accum_avg'] - mean_level\n",
    "else:\n",
    "    df_scores['accum_centered'] = np.nan\n",
    "\n",
    "accum_for_flow = df_scores['accum_centered']\n",
    "\n",
    "df_scores['role'] = np.where(\n",
    "    accum_for_flow > 0,\n",
    "    'winner',\n",
    "    np.where(accum_for_flow < 0, 'loser', 'neutral')\n",
    ")\n",
    "\n",
    "df_scores_sorted = df_scores.sort_values(\n",
    "    by='accum_centered', key=lambda s: s.abs(), ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "accum_edges_df, accum_winners, accum_losers, accum_demand, accum_supply = build_edges_from_value(\n",
    "    df_scores, 'accum_centered', TOP_K_WINNERS, TOP_K_LOSERS\n",
    ")\n",
    "print(\"Accum winners/losers:\", len(accum_winners), len(accum_losers), \"edges:\", len(accum_edges_df))\n",
    "\n",
    "# Lit and Short flows (net buy - sell)\n",
    "if VOLUME_DATA_AVAILABLE and not df_volume.empty:\n",
    "    df_volume = df_volume.copy()\n",
    "    df_volume['lit_net'] = df_volume['lit_buy_sum'] - df_volume['lit_sell_sum']\n",
    "    df_volume['short_net'] = df_volume['short_buy_sum'] - df_volume['short_sell_sum']\n",
    "\n",
    "    lit_edges_df, lit_winners, lit_losers, lit_demand, lit_supply = build_edges_from_value(\n",
    "        df_volume, 'lit_net', TOP_K_WINNERS, TOP_K_LOSERS\n",
    "    )\n",
    "    short_edges_df, short_winners, short_losers, short_demand, short_supply = build_edges_from_value(\n",
    "        df_volume, 'short_net', TOP_K_WINNERS, TOP_K_LOSERS\n",
    "    )\n",
    "\n",
    "    print(\"Lit winners/losers:\", len(lit_winners), len(lit_losers), \"edges:\", len(lit_edges_df))\n",
    "    print(\"Short winners/losers:\", len(short_winners), len(short_losers), \"edges:\", len(short_edges_df))\n",
    "    \n",
    "    # Finra buy flow (positive values - flow from low to high)\n",
    "    if 'finra_buy_sum' in df_volume.columns and df_volume['finra_buy_sum'].notna().any():\n",
    "        finra_edges_df, finra_high, finra_low, finra_high_total, finra_low_total = build_edges_from_positive_value(\n",
    "            df_volume, 'finra_buy_sum', TOP_K_WINNERS, TOP_K_LOSERS\n",
    "        )\n",
    "        print(\"Finra buy high/low:\", len(finra_high), len(finra_low), \"edges:\", len(finra_edges_df))\n",
    "    else:\n",
    "        finra_edges_df = pd.DataFrame()\n",
    "        finra_high = pd.DataFrame()\n",
    "        finra_low = pd.DataFrame()\n",
    "        finra_high_total = finra_low_total = 0.0\n",
    "        print(\"Finra buy data not available\")\n",
    "    # Finra buy z-score flow (negative/positive z)\n",
    "    if 'finra_buy_z_mean' in df_volume.columns and df_volume['finra_buy_z_mean'].notna().any():\n",
    "        vwbr_z_edges_df, vwbr_z_winners, vwbr_z_losers, vwbr_z_demand, vwbr_z_supply = build_edges_from_value(\n",
    "            df_volume, 'finra_buy_z_mean', TOP_K_WINNERS, TOP_K_LOSERS\n",
    "        )\n",
    "        print(\"Finra buy z winners/losers:\", len(vwbr_z_winners), len(vwbr_z_losers), \"edges:\", len(vwbr_z_edges_df))\n",
    "    else:\n",
    "        vwbr_z_edges_df = pd.DataFrame()\n",
    "        vwbr_z_winners = pd.DataFrame()\n",
    "        vwbr_z_losers = pd.DataFrame()\n",
    "        vwbr_z_demand = vwbr_z_supply = 0.0\n",
    "        print(\"Finra buy z data not available\")\n",
    "else:\n",
    "    lit_edges_df = pd.DataFrame()\n",
    "    short_edges_df = pd.DataFrame()\n",
    "    finra_edges_df = pd.DataFrame()\n",
    "    lit_winners = pd.DataFrame()\n",
    "    short_winners = pd.DataFrame()\n",
    "    finra_high = pd.DataFrame()\n",
    "    lit_losers = pd.DataFrame()\n",
    "    short_losers = pd.DataFrame()\n",
    "    finra_low = pd.DataFrame()\n",
    "    lit_demand = lit_supply = 0.0\n",
    "    short_demand = short_supply = 0.0\n",
    "    finra_high_total = finra_low_total = 0.0\n",
    "    vwbr_z_edges_df = pd.DataFrame()\n",
    "    vwbr_z_winners = pd.DataFrame()\n",
    "    vwbr_z_losers = pd.DataFrame()\n",
    "    vwbr_z_demand = vwbr_z_supply = 0.0\n",
    "\n",
    "# Time-faded edges per day (optional)\n",
    "accum_time_edges_df = pd.DataFrame()\n",
    "lit_time_edges_df = pd.DataFrame()\n",
    "short_time_edges_df = pd.DataFrame()\n",
    "finra_time_edges_df = pd.DataFrame()\n",
    "vwbr_z_time_edges_df = pd.DataFrame()\n",
    "if SHOW_TIME_FADE_CHORDS and TIME_FADE_USE_DAILY_EDGES:\n",
    "    if not df_accum_level.empty:\n",
    "        df_accum_daily_centered = df_accum_level.copy()\n",
    "        df_accum_daily_centered['date'] = pd.to_datetime(df_accum_daily_centered['date'], errors='coerce').dt.date\n",
    "        daily_means = df_accum_daily_centered.groupby('date')['accumulation_score'].mean()\n",
    "        df_accum_daily_centered['accum_centered'] = df_accum_daily_centered['accumulation_score'] - df_accum_daily_centered['date'].map(daily_means)\n",
    "        accum_time_edges_df = build_edges_by_date(df_accum_daily_centered, 'date', 'accum_centered', TOP_K_WINNERS, TOP_K_LOSERS)\n",
    "    if 'df_lit_daily' in dir() and not df_lit_daily.empty:\n",
    "        lit_time_edges_df = build_edges_by_date(df_lit_daily, 'date', 'lit_net', TOP_K_WINNERS, TOP_K_LOSERS)\n",
    "    if 'df_short_daily' in dir() and not df_short_daily.empty:\n",
    "        short_time_edges_df = build_edges_by_date(df_short_daily, 'date', 'short_net', TOP_K_WINNERS, TOP_K_LOSERS)\n",
    "    if 'df_finra_daily' in dir() and not df_finra_daily.empty and 'finra_buy' in df_finra_daily.columns:\n",
    "        finra_time_edges_df = build_edges_by_date(df_finra_daily, 'date', 'finra_buy', TOP_K_WINNERS, TOP_K_LOSERS, positive_only=True)\n",
    "    if 'df_vol_raw' in dir() and not df_vol_raw.empty and 'finra_buy_z' in df_vol_raw.columns:\n",
    "        df_vwbr_z_daily = df_vol_raw.groupby(['ticker', 'date'], as_index=False).agg({'finra_buy_z': 'mean'})\n",
    "        vwbr_z_time_edges_df = build_edges_by_date(df_vwbr_z_daily, 'date', 'finra_buy_z', TOP_K_WINNERS, TOP_K_LOSERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "EDGE_MULTIPLIER = 1000\n",
    "\n",
    "display(df_scores_sorted[['ticker', 'category', 'A_end', 'A_start', 'accum_avg', 'accum_centered', 'role']])\n",
    "\n",
    "if SHOW_ACCUM_FLOW:\n",
    "    if accum_edges_df.empty:\n",
    "        display(pd.DataFrame(columns=['source', 'dest', 'flow']))\n",
    "    else:\n",
    "        display(accum_edges_df[['source', 'dest', 'flow']].assign(flow=lambda d: d['flow'] * EDGE_MULTIPLIER))\n",
    "\n",
    "\n",
    "def chord_counts(edges_df):\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return pd.DataFrame(columns=['out', 'in'])\n",
    "    out = edges_df.groupby('source')['flow'].sum().rename('out')\n",
    "    inn = edges_df.groupby('dest')['flow'].sum().rename('in')\n",
    "    df = out.to_frame().join(inn, how='outer').fillna(0)\n",
    "    return (df * EDGE_MULTIPLIER).round(0).astype(int)\n",
    "\n",
    "\n",
    "if SHOW_ACCUM_FLOW:\n",
    "    print('Accum chord counts (out/in):')\n",
    "    display(chord_counts(accum_edges_df))\n",
    "if SHOW_LIT_FLOW:\n",
    "    print('Lit chord counts (out/in):')\n",
    "    display(chord_counts(lit_edges_df))\n",
    "if SHOW_SHORT_NET_FLOW:\n",
    "    print('Short chord counts (out/in):')\n",
    "    display(chord_counts(short_edges_df))\n",
    "if SHOW_VWBR_Z:\n",
    "    print('Finra buy z chord counts (out/in):')\n",
    "    display(chord_counts(vwbr_z_edges_df))\n",
    "\n",
    "print('Volume availability:', VOLUME_DATA_AVAILABLE, 'rows:', len(df_volume))\n",
    "if VOLUME_DATA_AVAILABLE and not df_volume.empty:\n",
    "    display(df_volume[['ticker', 'lit_buy_sum', 'lit_sell_sum', 'short_buy_sum', 'short_sell_sum']])\n",
    "    if (df_volume['lit_buy_sum'].fillna(0).sum() == 0 and df_volume['lit_sell_sum'].fillna(0).sum() == 0):\n",
    "        print('Note: lit volumes sum to zero across selected period.')\n",
    "else:\n",
    "    print('Volume data not available for lit/short buy/sell flows.')\n",
    "\n",
    "summary_parts = [\n",
    "    f'END_DATE={END_DATE_RESOLVED}',\n",
    "    f'FLOW_PERIOD_DAYS={FLOW_PERIOD_DAYS}',\n",
    "    f'accum_supply={accum_supply:.2f}',\n",
    "    f'accum_demand={accum_demand:.2f}',\n",
    "]\n",
    "if SHOW_LIT_FLOW:\n",
    "    summary_parts.append(f'lit_supply={lit_supply:.2f}')\n",
    "    summary_parts.append(f'lit_demand={lit_demand:.2f}')\n",
    "if SHOW_SHORT_NET_FLOW:\n",
    "    summary_parts.append(f'short_supply={short_supply:.2f}')\n",
    "    summary_parts.append(f'short_demand={short_demand:.2f}')\n",
    "if SHOW_VWBR_Z:\n",
    "    summary_parts.append(f'vwbr_z_supply={vwbr_z_supply:.2f}')\n",
    "    summary_parts.append(f'vwbr_z_demand={vwbr_z_demand:.2f}')\n",
    "\n",
    "summary_parts.append(f'accum_edges={len(accum_edges_df)}')\n",
    "summary_parts.append(f'lit_edges={len(lit_edges_df)}')\n",
    "summary_parts.append(f'short_edges={len(short_edges_df)}')\n",
    "if SHOW_VWBR_Z:\n",
    "    summary_parts.append(f'vwbr_z_edges={len(vwbr_z_edges_df)}')\n",
    "\n",
    "print(' | '.join(summary_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "vwbr_net = {}\n",
    "if 'df_finra_daily' in dir() and not df_finra_daily.empty and 'vwbr' in df_finra_daily.columns:\n",
    "    vwbr_series = df_finra_daily.groupby('ticker')['vwbr'].mean()\n",
    "    vwbr_net = (vwbr_series - 0.5).to_dict()\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection, PolyCollection\n",
    "from matplotlib.colors import to_rgba\n",
    "from matplotlib.patches import Wedge, PathPatch, Rectangle\n",
    "from matplotlib.path import Path as MplPath\n",
    "import os\n",
    "\n",
    "BG_COLOR = '#0b0f1a'\n",
    "\n",
    "CATEGORY_LABELS = {\n",
    "    'GLOBAL_MACRO': 'GLOBAL MACRO',\n",
    "    'MAG8': 'MAG8',\n",
    "    'THEMATIC_SECTORS': 'THEMATIC SECTORS',\n",
    "    'SECTOR_CORE': 'SECTORS',\n",
    "    'COMMODITIES': 'COMMODITIES',\n",
    "    'RATES_CREDIT': 'RATES/CREDIT',\n",
    "    'SPECULATIVE': 'SPECULATIVE',\n",
    "    'CRYPTO': 'CRYPTO',\n",
    "}\n",
    "CATEGORY_PALETTE = {\n",
    "    'GLOBAL_MACRO': \"#0059FF\",\n",
    "    'MAG8': \"#D38CFA\",\n",
    "    'THEMATIC_SECTORS': \"#00E4C5\",\n",
    "    'SECTOR_CORE': \"#FAAF00F2\",\n",
    "    'COMMODITIES': \"#FFFC2F\",\n",
    "    'RATES_CREDIT': \"#FF9966\",\n",
    "    'SPECULATIVE': \"#FF7A45\",\n",
    "    'CRYPTO': \"#4CC9F0\",\n",
    "    'UNKNOWN': \"#8F8E8E\",\n",
    "}\n",
    "\n",
    "# Updated metric orders to include finra_buy for chords\n",
    "CHORD_METRIC_ORDER = ['accum', 'short', 'lit', 'finra_buy', 'vwbr_z']\n",
    "BAND_ORDER = ['lit', 'accum', 'short', 'finra_buy', 'vwbr_z']\n",
    "\n",
    "# Ring metrics (different from chord metrics)\n",
    "RING_METRIC_ORDER = ['accum', 'dark_lit', 'finra_buy']\n",
    "\n",
    "METRIC_LABELS = {\n",
    "    'accum': 'Accumulation',\n",
    "    'short': 'Daily Short',\n",
    "    'lit': 'Lit',\n",
    "    'finra_buy': 'Finra Buy',\n",
    "    'vwbr_z': 'VWBR Z',\n",
    "    'dark_lit': 'Dark/Lit Ratio',\n",
    "}\n",
    "\n",
    "# Chord colors\n",
    "METRIC_COLORS = {\n",
    "    'accum': {'sell': \"#8304B9\", 'buy': \"#26FF00\"},\n",
    "    'short': {'sell': \"#280042\", 'buy': \"#00AEFF\"},\n",
    "    'lit': {'sell': \"#FF0B0B\", 'buy': \"#1E7237\"},\n",
    "    'finra_buy': {'low': \"#63238D\", 'high': \"#00E4C5\"},  # Purple gradient for finra\n",
    "    'vwbr_z': {'sell': \"#63238D\", 'buy': \"#00E4C5\"},\n",
    "}\n",
    "\n",
    "# Ring colors (new encodings)\n",
    "RING_COLORS = {\n",
    "    'accum': {'negative': \"#8304B9\", 'positive': \"#26FF00\"},  # Purple (-) to Green (+)\n",
    "    'dark_lit': {'lit': \"#4488FF\", 'neutral': \"#888888\", 'dark': \"#FF4444\"},  # Blue-Gray-Red\n",
    "    'finra_buy': {},  # Uses category colors dynamically\n",
    "}\n",
    "\n",
    "\n",
    "def blend_color(c1, c2, t=0.5):\n",
    "    a = np.array(to_rgba(c1))\n",
    "    b = np.array(to_rgba(c2))\n",
    "    return a * (1 - t) + b * t\n",
    "\n",
    "\n",
    "def soften_color(color, amount, base=BG_COLOR):\n",
    "    return blend_color(color, base, max(0.0, min(1.0, amount)))\n",
    "\n",
    "\n",
    "def darken_color(color, factor):\n",
    "    \"\"\"Darken a color by factor (0=black, 1=original)\"\"\"\n",
    "    rgba = np.array(to_rgba(color))\n",
    "    rgba[:3] = rgba[:3] * max(0.0, min(1.0, factor))\n",
    "    return rgba\n",
    "\n",
    "\n",
    "def add_gradient_curve(ax, points, color_start, color_end, lw, alpha):\n",
    "    if len(points) < 2:\n",
    "        return\n",
    "    segments = np.stack([points[:-1], points[1:]], axis=1)\n",
    "    c0 = np.array(to_rgba(color_start))\n",
    "    c1 = np.array(to_rgba(color_end))\n",
    "    t = np.linspace(0, 1, len(segments))[:, None]\n",
    "    colors = c0 * (1 - t) + c1 * t\n",
    "    colors[:, 3] = colors[:, 3] * alpha\n",
    "    lc = LineCollection(segments, colors=colors, linewidths=lw, capstyle='round')\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "\n",
    "def arc_points(a0, a1, r, n=None):\n",
    "    count = n or CHORD_ARC_POINTS\n",
    "    angles = np.linspace(a0, a1, count)\n",
    "    return np.column_stack([r * np.cos(angles), r * np.sin(angles)])\n",
    "\n",
    "\n",
    "def bezier_curve(p0, p1, p2, n=None):\n",
    "    count = n or CHORD_CURVE_POINTS\n",
    "    t = np.linspace(0, 1, count)[:, None]\n",
    "    return (1 - t) ** 2 * p0 + 2 * (1 - t) * t * p1 + t ** 2 * p2\n",
    "\n",
    "\n",
    "def ribbon_patch(a0, a1, b0, b1, r, color_start, color_end, alpha):\n",
    "    arc1 = arc_points(a0, a1, r, n=16)\n",
    "    arc2 = arc_points(b0, b1, r, n=16)\n",
    "    curve1 = bezier_curve(arc1[-1], np.array([0.0, 0.0]), arc2[0], n=24)\n",
    "    curve2 = bezier_curve(arc2[-1], np.array([0.0, 0.0]), arc1[0], n=24)\n",
    "    poly = np.vstack([arc1, curve1, arc2, curve2])\n",
    "    codes = [MplPath.MOVETO] + [MplPath.LINETO] * (len(poly) - 1)\n",
    "    path = MplPath(poly, codes)\n",
    "    mid = blend_color(color_start, color_end, 0.5)\n",
    "    return PathPatch(path, facecolor=mid, edgecolor='none', alpha=alpha)\n",
    "\n",
    "\n",
    "def gradient_fill_collection(a0, a1, b0, b1, r, color_start, color_end, alpha, steps=None):\n",
    "    steps = steps or CHORD_GRADIENT_STEPS\n",
    "    arc1 = arc_points(a0, a1, r, n=4)\n",
    "    arc2 = arc_points(b0, b1, r, n=4)\n",
    "    p_a0, p_a1 = arc1[0], arc1[-1]\n",
    "    p_b0, p_b1 = arc2[0], arc2[-1]\n",
    "    left = bezier_curve(p_a0, np.array([0.0, 0.0]), p_b0, n=steps + 1)\n",
    "    right = bezier_curve(p_a1, np.array([0.0, 0.0]), p_b1, n=steps + 1)\n",
    "    polys = []\n",
    "    colors = []\n",
    "    for i in range(steps):\n",
    "        quad = np.vstack([left[i], left[i + 1], right[i + 1], right[i]])\n",
    "        t = (i + 0.5) / steps\n",
    "        color = blend_color(color_start, color_end, t)\n",
    "        color[3] = color[3] * alpha\n",
    "        polys.append(quad)\n",
    "        colors.append(color)\n",
    "    return PolyCollection(polys, facecolors=colors, edgecolors='none')\n",
    "\n",
    "\n",
    "def draw_ribbon(ax, a0, a1, b0, b1, r, color_start, color_end, fill_alpha, line_alpha, lw):\n",
    "    if USE_GRADIENT_FILL:\n",
    "        clip = ribbon_patch(a0, a1, b0, b1, r, color_start, color_end, alpha=1.0)\n",
    "        clip.set_facecolor('none')\n",
    "        clip.set_edgecolor('none')\n",
    "        ax.add_patch(clip)\n",
    "        fill = gradient_fill_collection(a0, a1, b0, b1, r, color_start, color_end, fill_alpha)\n",
    "        fill.set_clip_path(clip)\n",
    "        ax.add_collection(fill)\n",
    "    else:\n",
    "        patch = ribbon_patch(a0, a1, b0, b1, r, color_start, color_end, alpha=fill_alpha)\n",
    "        ax.add_patch(patch)\n",
    "    mid_a = (a0 + a1) / 2\n",
    "    mid_b = (b0 + b1) / 2\n",
    "    p0 = np.array([r * np.cos(mid_a), r * np.sin(mid_a)])\n",
    "    p2 = np.array([r * np.cos(mid_b), r * np.sin(mid_b)])\n",
    "    curve = bezier_curve(p0, np.array([0.0, 0.0]), p2)\n",
    "    add_gradient_curve(ax, curve, color_start, color_end, lw=lw, alpha=line_alpha)\n",
    "\n",
    "\n",
    "def make_time_bins(dates, bins):\n",
    "    if not dates:\n",
    "        return []\n",
    "    if bins is None or bins <= 0:\n",
    "        return [dates]\n",
    "    bins = min(bins, len(dates))\n",
    "    split = np.array_split(dates, bins)\n",
    "    return [list(s) for s in split if len(s)]\n",
    "\n",
    "\n",
    "def compute_metric_totals(edges_df):\n",
    "    totals = {}\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return totals\n",
    "    for row in edges_df.itertuples():\n",
    "        totals[row.source] = totals.get(row.source, 0.0) + row.flow\n",
    "        totals[row.dest] = totals.get(row.dest, 0.0) + row.flow\n",
    "    return totals\n",
    "\n",
    "\n",
    "def filter_edges(edges_df):\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return edges_df\n",
    "    df = edges_df.copy()\n",
    "    if MAX_EDGES_PER_METRIC and MAX_EDGES_PER_METRIC > 0:\n",
    "        df = df.nlargest(MAX_EDGES_PER_METRIC, 'flow')\n",
    "    return df\n",
    "\n",
    "\n",
    "def expand_edges(edges_df, splits):\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return edges_df\n",
    "    if not splits or splits <= 1:\n",
    "        return edges_df\n",
    "    max_flow = edges_df['flow'].max() if 'flow' in edges_df.columns and not edges_df.empty else 0.0\n",
    "    if max_flow <= 0:\n",
    "        return edges_df\n",
    "    rows = []\n",
    "    for row in edges_df.itertuples():\n",
    "        date_val = getattr(row, 'date', None)\n",
    "        n = max(1, int(round(splits * (row.flow / max_flow))))\n",
    "        if EDGE_RIBBON_MAX and EDGE_RIBBON_MAX > 0:\n",
    "            n = min(n, EDGE_RIBBON_MAX)\n",
    "        flow = row.flow / n\n",
    "        for _ in range(n):\n",
    "            rows.append({'source': row.source, 'dest': row.dest, 'flow': flow, 'date': date_val})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def allocate_intervals(edges_df, band_map, metric_key, centered=True, center_offset=0.0):\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return []\n",
    "    \n",
    "    min_width_rad = RIBBON_MIN_WIDTH_RAD.get(metric_key, 0.003)\n",
    "    \n",
    "    out_counts = edges_df.groupby('source').size().to_dict()\n",
    "    in_counts = edges_df.groupby('dest').size().to_dict()\n",
    "    max_flow = edges_df['flow'].max() if not edges_df.empty else 1.0\n",
    "    \n",
    "    out_flow_totals = edges_df.groupby('source')['flow'].apply(lambda s: s.abs().sum()).to_dict()\n",
    "    in_flow_totals = edges_df.groupby('dest')['flow'].apply(lambda s: s.abs().sum()).to_dict()\n",
    "    total_flow_by_ticker = {t: out_flow_totals.get(t, 0.0) + in_flow_totals.get(t, 0.0) for t in ticker_order}\n",
    "    max_total_flow = max(total_flow_by_ticker.values()) if total_flow_by_ticker else 0.0\n",
    "    \n",
    "    use_time_order = SHOW_TIME_FADE_CHORDS and TIME_FADE_USE_DAILY_EDGES and 'date' in edges_df.columns\n",
    "    out_order = {}\n",
    "    in_order = {}\n",
    "    edges_with_id = edges_df\n",
    "    if use_time_order:\n",
    "        edges_with_id = edges_df.copy()\n",
    "        edges_with_id['_row_id'] = range(len(edges_with_id))\n",
    "        for src, group in edges_with_id.groupby('source'):\n",
    "            group_sorted = group.sort_values(['date', 'flow'], ascending=[False, False], na_position='first')\n",
    "            for idx, row_id in enumerate(group_sorted['_row_id'].tolist()):\n",
    "                out_order[row_id] = idx\n",
    "        for dst, group in edges_with_id.groupby('dest'):\n",
    "            group_sorted = group.sort_values(['date', 'flow'], ascending=[False, False], na_position='first')\n",
    "            for idx, row_id in enumerate(group_sorted['_row_id'].tolist()):\n",
    "                in_order[row_id] = idx\n",
    "    \n",
    "    out_slot_info = {}\n",
    "    in_slot_info = {}\n",
    "\n",
    "    def build_slot_info(range_start, range_end, n, anchor=None, flow_scale=None):\n",
    "        arc = range_end - range_start\n",
    "        if n <= 0 or arc <= 0:\n",
    "            return None\n",
    "        anchor_mode = centered and RIBBON_ANCHOR_TO_CENTER and anchor is not None\n",
    "        effective_arc = arc\n",
    "        if anchor_mode:\n",
    "            left_cap = max(0.0, anchor - range_start)\n",
    "            right_cap = max(0.0, range_end - anchor)\n",
    "            effective_arc = 2 * min(left_cap, right_cap)\n",
    "        if flow_scale is not None:\n",
    "            flow_scale = max(0.0, min(1.0, flow_scale))\n",
    "            effective_arc = effective_arc * flow_scale\n",
    "        if effective_arc <= 0:\n",
    "            return None\n",
    "        \n",
    "        total_gap = RIBBON_GAP_RAD * (n - 1)\n",
    "        usable = max(0.0, effective_arc - total_gap)\n",
    "        slot_width = usable / n if n > 0 else 0.0\n",
    "        if slot_width < min_width_rad and n > 1:\n",
    "            slot_width = min_width_rad\n",
    "            total_gap = max(0.0, effective_arc - n * slot_width)\n",
    "        if n > 0 and slot_width * n > effective_arc:\n",
    "            slot_width = effective_arc / n if n else 0.0\n",
    "            total_gap = 0.0\n",
    "        \n",
    "        actual_gap = total_gap / max(1, n - 1) if n > 1 else 0.0\n",
    "        total_width = n * slot_width + (n - 1) * actual_gap\n",
    "        \n",
    "        if centered:\n",
    "            if anchor_mode:\n",
    "                start_pos = anchor - total_width / 2\n",
    "            else:\n",
    "                arc_center = (range_start + range_end) / 2 + center_offset\n",
    "                start_pos = arc_center - total_width / 2\n",
    "        else:\n",
    "            start_pos = range_start\n",
    "        \n",
    "        if not anchor_mode:\n",
    "            start_pos = max(range_start, min(start_pos, range_end - total_width))\n",
    "        \n",
    "        return {\n",
    "            'slot_width': slot_width,\n",
    "            'cursor': start_pos,\n",
    "            'gap': actual_gap,\n",
    "            'anchor': anchor if anchor_mode else None,\n",
    "            'range': (range_start, range_end),\n",
    "        }\n",
    "    \n",
    "    for ticker in ticker_order:\n",
    "        spans = band_map.get(ticker, {}).get(metric_key)\n",
    "        if not spans:\n",
    "            continue\n",
    "        \n",
    "        out_range = spans['out']\n",
    "        in_range = spans['in']\n",
    "        band_center = spans.get('center')\n",
    "        if band_center is None:\n",
    "            band_center = (out_range[0] + in_range[1]) / 2\n",
    "        band_center = band_center + center_offset\n",
    "        \n",
    "        n_out = out_counts.get(ticker, 0)\n",
    "        n_in = in_counts.get(ticker, 0)\n",
    "        total_flow = total_flow_by_ticker.get(ticker, 0.0)\n",
    "        flow_scale = (total_flow / max_total_flow) if max_total_flow > 0 else 0.0\n",
    "        \n",
    "        if centered and RIBBON_ANCHOR_TO_CENTER:\n",
    "            full_start = out_range[0]\n",
    "            full_end = in_range[1]\n",
    "            anchor = max(full_start, min(band_center, full_end))\n",
    "            out_info = build_slot_info(full_start, full_end, n_out, anchor, flow_scale)\n",
    "            in_info = build_slot_info(full_start, full_end, n_in, anchor, flow_scale)\n",
    "        else:\n",
    "            out_info = build_slot_info(out_range[0], out_range[1], n_out, None, flow_scale)\n",
    "            in_info = build_slot_info(in_range[0], in_range[1], n_in, None, flow_scale)\n",
    "        \n",
    "        if out_info:\n",
    "            out_slot_info[ticker] = out_info\n",
    "        if in_info:\n",
    "            in_slot_info[ticker] = in_info\n",
    "    \n",
    "    intervals = []\n",
    "    edge_iter = edges_with_id.itertuples() if use_time_order else edges_df.sort_values('flow', ascending=False).itertuples()\n",
    "    for row in edge_iter:\n",
    "        date_val = getattr(row, 'date', None)\n",
    "        row_id = getattr(row, '_row_id', None)\n",
    "        src, dst, flow = row.source, row.dest, row.flow\n",
    "        if src not in out_slot_info or dst not in in_slot_info:\n",
    "            continue\n",
    "        \n",
    "        out_info = out_slot_info[src]\n",
    "        in_info = in_slot_info[dst]\n",
    "        \n",
    "        if RIBBON_WIDTH_SCALE_BY_FLOW:\n",
    "            flow_scale = (flow / max_flow) ** 0.5 if max_flow > 0 else 1.0\n",
    "            out_width = max(min_width_rad * 0.5, out_info['slot_width'] * flow_scale)\n",
    "            in_width = max(min_width_rad * 0.5, in_info['slot_width'] * flow_scale)\n",
    "        else:\n",
    "            out_width = out_info['slot_width']\n",
    "            in_width = in_info['slot_width']\n",
    "        \n",
    "        out_width = min(out_width, out_info['slot_width'])\n",
    "        in_width = min(in_width, in_info['slot_width'])\n",
    "        \n",
    "        if RIBBON_CONVERGE_TO_POINT:\n",
    "            if out_info.get('anchor') is not None:\n",
    "                a0 = out_info['anchor'] - out_width / 2\n",
    "                a1 = out_info['anchor'] + out_width / 2\n",
    "            else:\n",
    "                a0 = out_info['cursor']\n",
    "                a1 = a0 + out_width\n",
    "            if in_info.get('anchor') is not None:\n",
    "                b0 = in_info['anchor'] - in_width / 2\n",
    "                b1 = in_info['anchor'] + in_width / 2\n",
    "            else:\n",
    "                b0 = in_info['cursor']\n",
    "                b1 = b0 + in_width\n",
    "        else:\n",
    "            if use_time_order and row_id is not None:\n",
    "                out_idx = out_order.get(row_id, 0)\n",
    "                in_idx = in_order.get(row_id, 0)\n",
    "                out_step = out_info['slot_width'] + out_info['gap']\n",
    "                in_step = in_info['slot_width'] + in_info['gap']\n",
    "                a0 = out_info['cursor'] + out_idx * out_step\n",
    "                a1 = a0 + out_width\n",
    "                b0 = in_info['cursor'] + in_idx * in_step\n",
    "                b1 = b0 + in_width\n",
    "            else:\n",
    "                a0 = out_info['cursor']\n",
    "                a1 = a0 + out_width\n",
    "                b0 = in_info['cursor']\n",
    "                b1 = b0 + in_width\n",
    "                out_info['cursor'] = a0 + out_info['slot_width'] + out_info['gap']\n",
    "                in_info['cursor'] = b0 + in_info['slot_width'] + in_info['gap']\n",
    "        \n",
    "        if a1 > a0 and b1 > b0:\n",
    "            intervals.append({'source': src, 'dest': dst, 'flow': flow, 'a0': a0, 'a1': a1, 'b0': b0, 'b1': b1, 'date': date_val})\n",
    "    \n",
    "    return intervals\n",
    "def metric_visible(metric_key):\n",
    "    if metric_key == \"accum\":\n",
    "        return SHOW_ACCUM_FLOW\n",
    "    if metric_key == \"short\":\n",
    "        return SHOW_SHORT_NET_FLOW\n",
    "    if metric_key == \"lit\":\n",
    "        return SHOW_LIT_FLOW\n",
    "    if metric_key == \"finra_buy\":\n",
    "        return SHOW_FINRA_FLOW\n",
    "    if metric_key == \"vwbr_z\":\n",
    "        return SHOW_VWBR_Z\n",
    "    return True\n",
    "\n",
    "\n",
    "# Prepare chord metric datasets\n",
    "accum_edges_plot = filter_edges(accum_edges_df)\n",
    "short_edges_plot = filter_edges(short_edges_df)\n",
    "lit_edges_plot = filter_edges(lit_edges_df)\n",
    "finra_edges_plot = filter_edges(finra_edges_df) if 'finra_edges_df' in dir() else pd.DataFrame()\n",
    "vwbr_z_edges_plot = filter_edges(vwbr_z_edges_df) if 'vwbr_z_edges_df' in dir() else pd.DataFrame()\n",
    "\n",
    "def choose_time_edges(time_df, base_df):\n",
    "    if SHOW_TIME_FADE_CHORDS and TIME_FADE_USE_DAILY_EDGES and time_df is not None and not time_df.empty:\n",
    "        return time_df\n",
    "    return base_df\n",
    "\n",
    "metric_edges = {\n",
    "    'accum': choose_time_edges(accum_time_edges_df, accum_edges_plot),\n",
    "    'short': choose_time_edges(short_time_edges_df, short_edges_plot),\n",
    "    'lit': choose_time_edges(lit_time_edges_df, lit_edges_plot),\n",
    "    'finra_buy': choose_time_edges(finra_time_edges_df, finra_edges_plot),\n",
    "    'vwbr_z': choose_time_edges(vwbr_z_time_edges_df, vwbr_z_edges_plot),\n",
    "}\n",
    "\n",
    "metric_edges_draw = {m: expand_edges(metric_edges[m], EDGE_RIBBON_SPLITS) for m in CHORD_METRIC_ORDER}\n",
    "\n",
    "metric_nets = {\n",
    "    'accum': df_scores.set_index('ticker')['accum_centered'].to_dict() if 'accum_centered' in df_scores.columns else df_scores.set_index('ticker')['delta'].to_dict(),\n",
    "    'short': df_volume.set_index('ticker')['short_net'].to_dict() if 'short_net' in df_volume.columns else {},\n",
    "    'lit': df_volume.set_index('ticker')['lit_net'].to_dict() if 'lit_net' in df_volume.columns else {},\n",
    "    'finra_buy': df_volume.set_index('ticker')['finra_buy_sum'].to_dict() if 'finra_buy_sum' in df_volume.columns else {},\n",
    "    'vwbr_z': df_volume.set_index('ticker')['finra_buy_z_mean'].to_dict() if 'finra_buy_z_mean' in df_volume.columns else {},\n",
    "    'vwbr': vwbr_net,\n",
    "}\n",
    "\n",
    "df_accum_level = df_raw_full[df_raw_full['date'].isin(window_dates)][['ticker', 'date', 'accumulation_score']].copy()\n",
    "if not df_accum_level.empty:\n",
    "    df_accum_level = df_accum_level.rename(columns={'accumulation_score': 'value'})\n",
    "\n",
    "# Reopen database connection for lookback queries\n",
    "conn, db_type = connect_db(DB_PATH)\n",
    "\n",
    "# Ring metric daily data\n",
    "# Query 20-day lookback for z-score normalization (all ring metrics)\n",
    "RING_LOOKBACK_DAYS = 20\n",
    "latest_date = pd.Timestamp(max(window_dates)) if window_dates else pd.Timestamp.now()\n",
    "lookback_start = latest_date - pd.Timedelta(days=RING_LOOKBACK_DAYS + 10)\n",
    "\n",
    "# 20-day stats for accumulation score\n",
    "accum_stats = pd.DataFrame()\n",
    "try:\n",
    "    query_accum_lookback = f'''\n",
    "        SELECT UPPER({quote_ident(TICKER_COL)}) AS ticker,\n",
    "               {date_expr} AS date,\n",
    "               CAST({quote_ident(ACCUM_COL_SELECTED)} AS DOUBLE) AS accumulation_score\n",
    "        FROM {quote_ident(SELECT_TABLE)}\n",
    "        WHERE UPPER({quote_ident(TICKER_COL)}) IN ({placeholders})\n",
    "        AND {date_expr} >= '{lookback_start.strftime('%Y-%m-%d')}'\n",
    "    '''\n",
    "    df_accum_lookback = pd.read_sql(query_accum_lookback, conn, params=ticker_list)\n",
    "    if not df_accum_lookback.empty:\n",
    "        accum_stats = df_accum_lookback.groupby('ticker')['accumulation_score'].agg(['mean', 'std']).reset_index()\n",
    "        accum_stats.columns = ['ticker', 'accum_mean', 'accum_std']\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load accum lookback: {e}\")\n",
    "\n",
    "# 20-day stats for dark_ratio (kept for backward compatibility)\n",
    "dark_lit_stats = pd.DataFrame()\n",
    "try:\n",
    "    query_dark_lookback = f'''\n",
    "        SELECT UPPER({quote_ident(volume_info['ticker_col'])}) AS ticker,\n",
    "               {date_expr} AS date,\n",
    "               {num_cast}({quote_ident(volume_info['otc_vol_col'])} AS DOUBLE) AS otc_volume,\n",
    "               {num_cast}({quote_ident(volume_info['lit_total_col'])} AS DOUBLE) AS lit_total\n",
    "        FROM {quote_ident(volume_info['table'])}\n",
    "        WHERE UPPER({quote_ident(volume_info['ticker_col'])}) IN ({placeholders})\n",
    "        AND {date_expr} >= '{lookback_start.strftime('%Y-%m-%d')}'\n",
    "    '''\n",
    "    df_dark_lookback = pd.read_sql(query_dark_lookback, conn, params=ticker_list)\n",
    "    if not df_dark_lookback.empty:\n",
    "        df_dark_lookback['total_volume'] = df_dark_lookback['otc_volume'] + df_dark_lookback['lit_total']\n",
    "        df_dark_lookback['dark_ratio'] = df_dark_lookback['otc_volume'] / df_dark_lookback['total_volume'].replace(0, np.nan)\n",
    "        df_dark_lookback['dark_ratio'] = df_dark_lookback['dark_ratio'].fillna(0.5)\n",
    "        dark_lit_stats = df_dark_lookback.groupby('ticker')['dark_ratio'].agg(['mean', 'std']).reset_index()\n",
    "        dark_lit_stats.columns = ['ticker', 'dark_ratio_mean', 'dark_ratio_std']\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load dark_lit lookback: {e}\")\n",
    "\n",
    "# 20-day stats for lit volume (Ring 2 sizing)\n",
    "lit_stats = pd.DataFrame()\n",
    "try:\n",
    "    query_lit_lookback = f'''\n",
    "        SELECT UPPER({quote_ident(volume_info['ticker_col'])}) AS ticker,\n",
    "               {date_expr} AS date,\n",
    "               {num_cast}({quote_ident(volume_info['lit_total_col'])} AS DOUBLE) AS lit_total\n",
    "        FROM {quote_ident(volume_info['table'])}\n",
    "        WHERE UPPER({quote_ident(volume_info['ticker_col'])}) IN ({placeholders})\n",
    "        AND {date_expr} >= '{lookback_start.strftime('%Y-%m-%d')}'\n",
    "    '''\n",
    "    df_lit_lookback = pd.read_sql(query_lit_lookback, conn, params=ticker_list)\n",
    "    if not df_lit_lookback.empty:\n",
    "        lit_stats = df_lit_lookback.groupby('ticker')['lit_total'].agg(['mean', 'std']).reset_index()\n",
    "        lit_stats.columns = ['ticker', 'lit_vol_mean', 'lit_vol_std']\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load lit lookback: {e}\")\n",
    "\n",
    "# 20-day stats for finra_buy and short_buy_sell_ratio\n",
    "finra_stats = pd.DataFrame()\n",
    "try:\n",
    "    finra_cols = [f\"{num_cast}({quote_ident(volume_info['finra_buy_col'])} AS DOUBLE) AS finra_buy\"]\n",
    "    if volume_info.get('short_buy_sell_ratio_col'):\n",
    "        finra_cols.append(f\"{num_cast}({quote_ident(volume_info['short_buy_sell_ratio_col'])} AS DOUBLE) AS short_buy_sell_ratio\")\n",
    "    query_finra_lookback = f'''\n",
    "        SELECT UPPER({quote_ident(volume_info['ticker_col'])}) AS ticker,\n",
    "               {date_expr} AS date,\n",
    "               {', '.join(finra_cols)}\n",
    "        FROM {quote_ident(volume_info['table'])}\n",
    "        WHERE UPPER({quote_ident(volume_info['ticker_col'])}) IN ({placeholders})\n",
    "        AND {date_expr} >= '{lookback_start.strftime('%Y-%m-%d')}'\n",
    "    '''\n",
    "    df_finra_lookback = pd.read_sql(query_finra_lookback, conn, params=ticker_list)\n",
    "    if not df_finra_lookback.empty:\n",
    "        agg_dict = {'finra_buy': ['mean', 'std']}\n",
    "        if 'short_buy_sell_ratio' in df_finra_lookback.columns:\n",
    "            agg_dict['short_buy_sell_ratio'] = ['mean', 'std']\n",
    "        finra_stats = df_finra_lookback.groupby('ticker').agg(agg_dict).reset_index()\n",
    "        finra_stats.columns = ['ticker'] + [f'{col}_{stat}' for col, stat in [('finra_buy', 'mean'), ('finra_buy', 'std')] + ([('sbsr', 'mean'), ('sbsr', 'std')] if 'short_buy_sell_ratio' in df_finra_lookback.columns else [])]\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load finra lookback: {e}\")\n",
    "\n",
    "# Close database connection after lookback queries\n",
    "conn.close()\n",
    "\n",
    "# Add short_buy_sell_ratio to df_finra_daily if available\n",
    "if 'df_finra_daily' in dir() and 'df_vol_raw' in dir() and 'short_buy_sell_ratio' in df_vol_raw.columns:\n",
    "    df_sbsr = df_vol_raw.groupby(['ticker', 'date'])['short_buy_sell_ratio'].mean().reset_index()\n",
    "    df_finra_daily = df_finra_daily.merge(df_sbsr, on=['ticker', 'date'], how='left')\n",
    "\n",
    "# Add VWBR/VWBR Z to df_finra_daily if available\n",
    "if 'df_finra_daily' in dir() and 'df_vol_raw' in dir() and 'vwbr' in df_vol_raw.columns:\n",
    "    df_vwbr = df_vol_raw.groupby(['ticker', 'date'])['vwbr'].mean().reset_index()\n",
    "    df_finra_daily = df_finra_daily.merge(df_vwbr, on=['ticker', 'date'], how='left')\n",
    "if 'df_finra_daily' in dir() and 'df_vol_raw' in dir() and 'vwbr_z' in df_vol_raw.columns:\n",
    "    df_vwbr_z = df_vol_raw.groupby(['ticker', 'date'])['vwbr_z'].mean().reset_index()\n",
    "    df_finra_daily = df_finra_daily.merge(df_vwbr_z, on=['ticker', 'date'], how='left')\n",
    "\n",
    "\n",
    "ring_metric_daily = {\n",
    "    'accum': df_accum_level,\n",
    "    'accum_stats': accum_stats,\n",
    "    'dark_lit': df_dark_lit_daily if 'df_dark_lit_daily' in dir() else pd.DataFrame(),\n",
    "    'dark_lit_stats': dark_lit_stats,\n",
    "    'lit': df_lit_daily if 'df_lit_daily' in dir() else pd.DataFrame(),\n",
    "    'lit_stats': lit_stats if 'lit_stats' in dir() else pd.DataFrame(),\n",
    "    'finra_buy': df_finra_daily if 'df_finra_daily' in dir() else pd.DataFrame(),\n",
    "    'finra_stats': finra_stats,\n",
    "}\n",
    "\n",
    "# Build ticker layout\n",
    "grouped = {\n",
    "    'GLOBAL_MACRO': [t for t in ticker_order if ticker_category.get(t) == 'GLOBAL_MACRO'],\n",
    "    'MAG8': [t for t in ticker_order if ticker_category.get(t) == 'MAG8'],\n",
    "    'THEMATIC_SECTORS': [t for t in ticker_order if ticker_category.get(t) == 'THEMATIC_SECTORS'],\n",
    "    'SECTOR_CORE': [t for t in ticker_order if ticker_category.get(t) == 'SECTOR_CORE'],\n",
    "    'COMMODITIES': [t for t in ticker_order if ticker_category.get(t) == 'COMMODITIES'],\n",
    "    'RATES_CREDIT': [t for t in ticker_order if ticker_category.get(t) == 'RATES_CREDIT'],\n",
    "    'SPECULATIVE': [t for t in ticker_order if ticker_category.get(t) == 'SPECULATIVE'],\n",
    "    'CRYPTO': [t for t in ticker_order if ticker_category.get(t) == 'CRYPTO'],\n",
    "}\n",
    "\n",
    "metric_totals = {m: compute_metric_totals(metric_edges[m]) for m in CHORD_METRIC_ORDER}\n",
    "\n",
    "total_nodes = sum(len(v) for v in grouped.values())\n",
    "if total_nodes == 0:\n",
    "    print('No nodes to plot.')\n",
    "else:\n",
    "    gap = math.radians(CATEGORY_GAP_DEG)\n",
    "    total_gap = gap * len([g for g in grouped.values() if g])\n",
    "    usable = 2 * math.pi - total_gap\n",
    "    if usable <= 0:\n",
    "        usable = 2 * math.pi\n",
    "    step = usable / total_nodes\n",
    "    arc_span = step * 0.85\n",
    "\n",
    "    angles = {}\n",
    "    spans = {}\n",
    "    angle = 0.0\n",
    "    for cat in ['GLOBAL_MACRO', 'MAG8', 'THEMATIC_SECTORS', 'SECTOR_CORE', 'COMMODITIES', 'RATES_CREDIT', 'SPECULATIVE', 'CRYPTO']:\n",
    "        if not grouped[cat]:\n",
    "            continue\n",
    "        angle += gap / 2\n",
    "        for t in grouped[cat]:\n",
    "            angles[t] = angle\n",
    "            spans[t] = (angle - arc_span / 2, angle + arc_span / 2)\n",
    "            angle += step\n",
    "        angle += gap / 2\n",
    "\n",
    "    # Filter BAND_ORDER to only visible metrics for centered layout\n",
    "    visible_band_order = [m for m in BAND_ORDER if metric_visible(m)]\n",
    "    if not visible_band_order:\n",
    "        visible_band_order = BAND_ORDER  # Fallback to all if none visible\n",
    "\n",
    "    # Metric bands per ticker (for chords)\n",
    "    band_map = {}\n",
    "    for t, (a0, a1) in spans.items():\n",
    "        max_span = (a1 - a0)\n",
    "        chord_span = min(max_span, max_span * CHORD_ARC_FRACTION)\n",
    "        chord_center = (a0 + a1) / 2\n",
    "        band_gap = chord_span * BAND_GAP_FRAC\n",
    "\n",
    "        if METRIC_BAND_MODE == 'proportional':\n",
    "            weights = {m: metric_totals[m].get(t, 0.0) for m in visible_band_order}\n",
    "            if sum(weights.values()) <= 0:\n",
    "                weights = {m: 1.0 for m in visible_band_order}\n",
    "        else:\n",
    "            weights = {m: 1.0 for m in visible_band_order}\n",
    "\n",
    "        total_w = sum(weights.values())\n",
    "        if total_w <= 0:\n",
    "            weights = {m: 1.0 for m in visible_band_order}\n",
    "            total_w = sum(weights.values())\n",
    "\n",
    "        available = chord_span - band_gap * max(len(visible_band_order) - 1, 0)\n",
    "        if available <= 0:\n",
    "            band_gap = 0.0\n",
    "            available = chord_span\n",
    "\n",
    "        lengths = {m: max(0.0, available * (weights[m] / total_w)) for m in visible_band_order}\n",
    "\n",
    "        # Layout: follow BAND_ORDER\n",
    "        total_len = sum(lengths.values()) + band_gap * max(len(visible_band_order) - 1, 0)\n",
    "        start = chord_center - total_len / 2\n",
    "\n",
    "        metric_spans = {}\n",
    "        cursor = start\n",
    "        for idx, m in enumerate(visible_band_order):\n",
    "            m_len = lengths.get(m, 0.0)\n",
    "            m_start = cursor\n",
    "            m_end = m_start + m_len\n",
    "            metric_spans[m] = (m_start, m_end)\n",
    "            if idx < len(visible_band_order) - 1:\n",
    "                cursor = m_end + band_gap\n",
    "            else:\n",
    "                cursor = m_end\n",
    "\n",
    "        def band_slices(start, end):\n",
    "            span = end - start\n",
    "            dir_gap = span * DIR_GAP_FRAC\n",
    "            if RIBBON_ANCHOR_TO_CENTER:\n",
    "                dir_gap = 0.0\n",
    "            dir_gap = min(dir_gap, span * 0.5)\n",
    "            if dir_gap < 0:\n",
    "                dir_gap = 0.0\n",
    "            mid = (start + end) / 2\n",
    "            out_span = (start, mid - dir_gap / 2)\n",
    "            in_span = (mid + dir_gap / 2, end)\n",
    "            return {'out': out_span, 'in': in_span, 'center': mid}\n",
    "\n",
    "        band_map[t] = {m: band_slices(*metric_spans[m]) for m in visible_band_order}\n",
    "\n",
    "    # Prepare intervals per metric with per-metric center offset\n",
    "    metric_intervals = {}\n",
    "    for m in CHORD_METRIC_ORDER:\n",
    "        if not metric_visible(m):\n",
    "            metric_intervals[m] = []\n",
    "            continue\n",
    "        offset = RIBBON_CENTER_OFFSET.get(m, 0.0)\n",
    "        metric_intervals[m] = allocate_intervals(\n",
    "            metric_edges_draw[m], band_map, m, \n",
    "            centered=RIBBON_CENTERED, \n",
    "            center_offset=offset\n",
    "        )\n",
    "    \n",
    "    for m in CHORD_METRIC_ORDER:\n",
    "        if metric_visible(m):\n",
    "            offset = RIBBON_CENTER_OFFSET.get(m, 0.0)\n",
    "            min_w = RIBBON_MIN_WIDTH_RAD.get(m, 0.003)\n",
    "            print(f\"{m}: {len(metric_intervals[m])} ribbons (centered={RIBBON_CENTERED}, offset={offset:.3f}, min_width={min_w:.4f})\")\n",
    "\n",
    "    # Time bins for outer ring\n",
    "    window_dates_sorted = sorted(window_dates)\n",
    "    time_bins = make_time_bins(window_dates_sorted, TIME_SLICE_BINS)\n",
    "    time_bins = list(reversed(time_bins))\n",
    "\n",
    "    time_fade_alpha = {}\n",
    "    time_fade_min_date = None\n",
    "    vwbr_z_brightness = {}\n",
    "    if SHOW_TIME_FADE_CHORDS and TIME_FADE_USE_DAILY_EDGES and SHOW_VWBR_Z:\n",
    "        if 'df_finra_daily' in dir() and not df_finra_daily.empty and 'vwbr_z' in df_finra_daily.columns:\n",
    "            df_vwbr_z_bright = df_finra_daily[['ticker', 'date', 'vwbr_z']].dropna()\n",
    "            df_vwbr_z_bright['date'] = pd.to_datetime(df_vwbr_z_bright['date'], errors='coerce').dt.date\n",
    "            for row in df_vwbr_z_bright.itertuples():\n",
    "                z_val = getattr(row, \"vwbr_z\", None)\n",
    "                if z_val is None or not np.isfinite(z_val):\n",
    "                    continue\n",
    "                normalized = 0.5 + (z_val / RING3_ZSCORE_SPAN)\n",
    "                normalized = max(0.0, min(1.0, normalized))\n",
    "                vwbr_z_brightness[(row.ticker, row.date)] = normalized\n",
    "\n",
    "    if SHOW_TIME_FADE_CHORDS and TIME_FADE_USE_DAILY_EDGES and window_dates_sorted:\n",
    "        time_fade_min_date = window_dates_sorted[0]\n",
    "        count = len(window_dates_sorted)\n",
    "        for idx, dt in enumerate(window_dates_sorted):\n",
    "            t = idx / max(1, count - 1)\n",
    "            t = t ** TIME_FADE_POWER\n",
    "            time_fade_alpha[dt] = TIME_FADE_MIN_ALPHA + t * (TIME_FADE_MAX_ALPHA - TIME_FADE_MIN_ALPHA)\n",
    "\n",
    "    # Pre-compute normalization for ring rendering\n",
    "    ring_max_mag = {}\n",
    "    ring_min_val = {}\n",
    "    for m in RING_METRIC_ORDER:\n",
    "        df_m = ring_metric_daily.get(m, pd.DataFrame())\n",
    "        if df_m is None or df_m.empty:\n",
    "            ring_max_mag[m] = 1.0\n",
    "            ring_min_val[m] = 0.0\n",
    "            continue\n",
    "        if m == 'accum' and 'value' in df_m.columns and df_m['value'].notna().any():\n",
    "            ring_max_mag[m] = float(df_m['value'].abs().max())\n",
    "            ring_min_val[m] = 0.0  # For magnitude-based thickness\n",
    "        elif m == 'dark_lit' and 'total_volume' in df_m.columns:\n",
    "            ring_max_mag[m] = float(df_m['total_volume'].max()) if df_m['total_volume'].notna().any() else 1.0\n",
    "            ring_min_val[m] = 0.0\n",
    "        elif m == 'finra_buy' and 'finra_buy' in df_m.columns:\n",
    "            vals = df_m['finra_buy'].dropna()\n",
    "            if len(vals) > 0:\n",
    "                ring_max_mag[m] = float(np.log10(vals.max() + 1)) if vals.max() > 0 else 1.0\n",
    "                ring_min_val[m] = float(np.log10(vals.min() + 1)) if vals.min() > 0 else 0.0\n",
    "            else:\n",
    "                ring_max_mag[m] = 1.0\n",
    "                ring_min_val[m] = 0.0\n",
    "        else:\n",
    "            ring_max_mag[m] = 1.0\n",
    "            ring_min_val[m] = 0.0\n",
    "\n",
    "    # Build bin value lookup for rings\n",
    "    ring_bin_data = {m: {} for m in RING_METRIC_ORDER}\n",
    "    for m in RING_METRIC_ORDER:\n",
    "        df_m = ring_metric_daily.get(m, pd.DataFrame())\n",
    "        if df_m is None or df_m.empty:\n",
    "            for t in ticker_order:\n",
    "                ring_bin_data[m][t] = [{'value': 0.0, 'extra': 0.0} for _ in time_bins]\n",
    "            continue\n",
    "        for t in ticker_order:\n",
    "            data_by_bin = []\n",
    "            for bin_dates in time_bins:\n",
    "                mask = (df_m['ticker'] == t) & (df_m['date'].isin(bin_dates))\n",
    "                if m == 'accum' and 'value' in df_m.columns:\n",
    "                    val = float(df_m.loc[mask, 'value'].mean()) if mask.any() else 0.0\n",
    "                    data_by_bin.append({'value': val, 'extra': 0.0})\n",
    "                elif m == 'dark_lit':\n",
    "                    # Ring 2 now shows Lit data: sizing=lit volume z-score, coloring=lit buy ratio\n",
    "                    df_lit = ring_metric_daily.get('lit', pd.DataFrame())\n",
    "                    if not df_lit.empty:\n",
    "                        lit_mask = (df_lit['ticker'] == t) & (df_lit['date'].isin(bin_dates))\n",
    "                        if lit_mask.any():\n",
    "                            lit_vol = float(df_lit.loc[lit_mask, 'lit_total'].sum())\n",
    "                            lit_buy_ratio = float(df_lit.loc[lit_mask, 'lit_buy_ratio'].mean())\n",
    "                        else:\n",
    "                            lit_vol = 0.0\n",
    "                            lit_buy_ratio = 0.5\n",
    "                    else:\n",
    "                        lit_vol = 0.0\n",
    "                        lit_buy_ratio = 0.5\n",
    "                    data_by_bin.append({'value': lit_vol, 'extra': lit_buy_ratio})\n",
    "                elif m == 'finra_buy' and 'finra_buy' in df_m.columns:\n",
    "                    val = float(df_m.loc[mask, 'finra_buy'].sum()) if mask.any() else 0.0\n",
    "                    # Pass ring 3 coloring inputs as extra\n",
    "                    sbsr_val = float(df_m.loc[mask, 'short_buy_sell_ratio'].mean()) if 'short_buy_sell_ratio' in df_m.columns and mask.any() else None\n",
    "                    vwbr_val = float(df_m.loc[mask, 'vwbr'].mean()) if 'vwbr' in df_m.columns and mask.any() else None\n",
    "                    vwbr_z_val = float(df_m.loc[mask, 'vwbr_z'].mean()) if 'vwbr_z' in df_m.columns and mask.any() else None\n",
    "                    data_by_bin.append({'value': val, 'extra': {'short_buy_sell_ratio': sbsr_val, 'vwbr': vwbr_val, 'vwbr_z': vwbr_z_val}})\n",
    "                else:\n",
    "                    data_by_bin.append({'value': 0.0, 'extra': 0.0})\n",
    "            ring_bin_data[m][t] = data_by_bin\n",
    "\n",
    "    # Start plot\n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZE, subplot_kw={'aspect': 'equal'})\n",
    "    fig.patch.set_facecolor(BG_COLOR)\n",
    "    ax.set_facecolor(BG_COLOR)\n",
    "    ax.axis('off')\n",
    "    main_left = PLOT_CENTER_X - MAIN_AX_SIZE / 2\n",
    "    main_bottom = PLOT_CENTER_Y - MAIN_AX_SIZE / 2\n",
    "    main_left = max(0.0, min(main_left, 1.0 - MAIN_AX_SIZE))\n",
    "    main_bottom = max(0.0, min(main_bottom, 1.0 - MAIN_AX_SIZE))\n",
    "    ax.set_position([main_left, main_bottom, MAIN_AX_SIZE, MAIN_AX_SIZE])\n",
    "\n",
    "    theta = np.linspace(0, 2 * math.pi, 400)\n",
    "    ax.plot(np.cos(theta), np.sin(theta), color='#39424e', lw=1.0, alpha=0.6)\n",
    "\n",
    "    # Ring outer edge (ticker arc removed - Ring 3 shows category colors)\n",
    "    ticker_outer = 1.02\n",
    "\n",
    "    # Outer rings (NEW ENCODING: accum, dark_lit, finra_buy)\n",
    "    if SHOW_VOLUME_RING:\n",
    "        track_span = RING_BASE_THICKNESS + RING_THICKNESS_SCALE + RING_GAP\n",
    "        for idx, m in enumerate(RING_METRIC_ORDER):\n",
    "            inner_base = ticker_outer + 0.02 + idx * track_span\n",
    "            max_mag = ring_max_mag.get(m, 1.0)\n",
    "            \n",
    "            for t, (a0, a1) in spans.items():\n",
    "                bin_data = ring_bin_data[m].get(t, [])\n",
    "                if not bin_data:\n",
    "                    continue\n",
    "                n_bins = len(bin_data)\n",
    "                arc_len = a1 - a0\n",
    "                slice_gap = arc_len * 0.02 / max(1, n_bins)\n",
    "                slice_len = (arc_len - slice_gap * (n_bins - 1)) / n_bins if n_bins > 0 else arc_len\n",
    "                cursor = a0\n",
    "                for bd in bin_data:\n",
    "                    val = bd['value']\n",
    "                    extra = bd['extra']\n",
    "                    \n",
    "                    if m == 'accum':\n",
    "                        # Ring 1: Sizing = 20-day z-score deviation, Coloring = accumulation thresholds\n",
    "                        accum_scale = RING_THICKNESS_SCALE * RING1_THICKNESS_MULT\n",
    "                        # Get 20-day stats for this ticker\n",
    "                        df_astats = ring_metric_daily.get('accum_stats', pd.DataFrame())\n",
    "                        if not df_astats.empty and t in df_astats['ticker'].values:\n",
    "                            stats = df_astats[df_astats['ticker'] == t].iloc[0]\n",
    "                            mean_val = stats['accum_mean']\n",
    "                            std_val = stats['accum_std']\n",
    "                            if std_val > 0:\n",
    "                                z = abs(val - mean_val) / std_val\n",
    "                                thickness = RING_BASE_THICKNESS + accum_scale * min(z / 2.0, 1.0)\n",
    "                            else:\n",
    "                                thickness = RING_BASE_THICKNESS\n",
    "                        else:\n",
    "                            # Fallback: deviation from 50\n",
    "                            deviation = abs(val - 50)\n",
    "                            thickness = RING_BASE_THICKNESS + accum_scale * (deviation / 50.0)\n",
    "\n",
    "                        # Coloring: <=30 = max purple, >=70 = max green, 30-70 = blend\n",
    "                        if val >= 70:\n",
    "                            color = RING_COLORS['accum']['positive']  # Max green\n",
    "                        elif val <= 30:\n",
    "                            color = RING_COLORS['accum']['negative']  # Max purple\n",
    "                        else:\n",
    "                            # Blend based on position between 30-70\n",
    "                            t_blend = (val - 30) / 40.0  # 0 at 30, 1 at 70\n",
    "                            color = blend_color(RING_COLORS['accum']['negative'], RING_COLORS['accum']['positive'], t_blend)\n",
    "                    \n",
    "                    elif m == 'dark_lit':\n",
    "                        # Ring 2: Sizing = Lit volume 20-day z-score, Coloring = Lit buy ratio\n",
    "                        lit_scale = RING_THICKNESS_SCALE * RING2_THICKNESS_MULT\n",
    "                        lit_vol = val  # lit_total from data\n",
    "                        lit_buy_ratio = extra  # lit_buy_ratio from data\n",
    "\n",
    "                        # Get 20-day lit volume stats for this ticker\n",
    "                        df_lstats = ring_metric_daily.get('lit_stats', pd.DataFrame())\n",
    "                        if not df_lstats.empty and t in df_lstats['ticker'].values:\n",
    "                            stats = df_lstats[df_lstats['ticker'] == t].iloc[0]\n",
    "                            mean_vol = stats['lit_vol_mean']\n",
    "                            std_vol = stats['lit_vol_std']\n",
    "                            if std_vol > 0 and lit_vol > 0:\n",
    "                                z = (lit_vol - mean_vol) / std_vol\n",
    "                                # Sizing: abs(z-score) deviation\n",
    "                                thickness = RING_BASE_THICKNESS + lit_scale * min(abs(z) / 2.0, 1.0)\n",
    "                            else:\n",
    "                                thickness = RING_BASE_THICKNESS\n",
    "                        else:\n",
    "                            # No per-ticker z-score stats available - use minimal thickness\n",
    "                            thickness = RING_BASE_THICKNESS\n",
    "\n",
    "                        # Coloring: Lit buy ratio (0=all sells/red, 0.5=neutral/gray, 1=all buys/green)\n",
    "                        # Typical range is 0.4-0.6, so use tighter scaling\n",
    "                        normalized = (lit_buy_ratio - 0.4) / 0.2  # 0.4->0, 0.5->0.5, 0.6->1\n",
    "                        normalized = max(0.0, min(1.0, normalized))\n",
    "\n",
    "                        # Color: sell (red) to neutral (gray) to buy (green)\n",
    "                        if normalized < 0.5:\n",
    "                            color = blend_color('#FF6666', '#888888', normalized * 2)  # Red to gray\n",
    "                        else:\n",
    "                            color = blend_color('#888888', '#66FF66', (normalized - 0.5) * 2)  # Gray to green\n",
    "                    \n",
    "                    elif m == 'finra_buy':\n",
    "                        # Ring 3: Sizing = 20-day z-score deviation, Coloring = RING3_COLOR_MODE\n",
    "                        finra_scale = RING_THICKNESS_SCALE * RING3_THICKNESS_MULT\n",
    "                        extra_vals = extra if isinstance(extra, dict) else {}\n",
    "                        short_buy_sell_ratio = extra_vals.get('short_buy_sell_ratio')\n",
    "                        vwbr_val = extra_vals.get('vwbr')\n",
    "                        vwbr_z_val = extra_vals.get('vwbr_z')\n",
    "\n",
    "                        # Sizing: 20-day z-score deviation of finra_buy\n",
    "                        df_fstats = ring_metric_daily.get('finra_stats', pd.DataFrame())\n",
    "                        if not df_fstats.empty and t in df_fstats['ticker'].values:\n",
    "                            stats = df_fstats[df_fstats['ticker'] == t].iloc[0]\n",
    "                            mean_fb = stats.get('finra_buy_mean', 0)\n",
    "                            std_fb = stats.get('finra_buy_std', 0)\n",
    "                            if std_fb > 0 and val > 0:\n",
    "                                z = abs(val - mean_fb) / std_fb\n",
    "                                thickness = RING_BASE_THICKNESS + finra_scale * min(z / 2.0, 1.0)\n",
    "                            else:\n",
    "                                thickness = RING_BASE_THICKNESS\n",
    "                        else:\n",
    "                            # No per-ticker z-score stats available - use minimal thickness\n",
    "                            thickness = RING_BASE_THICKNESS\n",
    "\n",
    "                        # Coloring: use category tint with brightness driven by selected mode\n",
    "                        color_mode = str(RING3_COLOR_MODE).upper()\n",
    "                        if color_mode == 'VWBR_Z':\n",
    "                            if vwbr_z_val is None or not np.isfinite(vwbr_z_val):\n",
    "                                vwbr_z_val = 0.0\n",
    "                            normalized = 0.5 + (vwbr_z_val / RING3_ZSCORE_SPAN)\n",
    "                            normalized = max(0.0, min(1.0, normalized))\n",
    "                        elif color_mode == 'VWBR':\n",
    "                            if vwbr_val is None or not np.isfinite(vwbr_val):\n",
    "                                vwbr_val = 0.5\n",
    "                            normalized = 0.5 + ((vwbr_val - 0.5) / RING3_VWBR_SPAN)\n",
    "                            normalized = max(0.0, min(1.0, normalized))\n",
    "                        else:\n",
    "                            # FINRA short buy/sell ratio (buy/sell ratio from FINRA short sales)\n",
    "                            if short_buy_sell_ratio and short_buy_sell_ratio > 0:\n",
    "                                log_ratio = np.log(short_buy_sell_ratio)\n",
    "                                normalized = 0.5 + (log_ratio / 1.0)  # +/-0.5 log units = 0.6x to 1.6x\n",
    "                                normalized = max(0.0, min(1.0, normalized))\n",
    "                            else:\n",
    "                                normalized = 0.5\n",
    "\n",
    "                        # Color: category tint (dark -> bright)\n",
    "                        cat_color = CATEGORY_PALETTE.get(ticker_category.get(t, 'UNKNOWN'), '#A0A0A0')\n",
    "                        brightness = RING3_BRIGHTNESS_MIN + (RING3_BRIGHTNESS_MAX - RING3_BRIGHTNESS_MIN) * normalized\n",
    "                        color = darken_color(cat_color, brightness)\n",
    "                    \n",
    "                    else:\n",
    "                        thickness = RING_BASE_THICKNESS\n",
    "                        color = '#666666'\n",
    "                    \n",
    "                    wedge = Wedge(\n",
    "                        (0, 0), inner_base + thickness, math.degrees(cursor), math.degrees(cursor + slice_len),\n",
    "                        width=thickness,\n",
    "                        facecolor=color, edgecolor='none', alpha=0.85,\n",
    "                    )\n",
    "                    ax.add_patch(wedge)\n",
    "                    cursor += slice_len + slice_gap\n",
    "\n",
    "    # Draw chords per metric\n",
    "    for m in CHORD_METRIC_ORDER:\n",
    "        if not metric_visible(m):\n",
    "            continue\n",
    "        intervals = metric_intervals[m]\n",
    "        if not intervals:\n",
    "            continue\n",
    "        \n",
    "        if m == 'finra_buy':\n",
    "            raw_start = METRIC_COLORS[m]['low']\n",
    "            raw_end = METRIC_COLORS[m]['high']\n",
    "        else:\n",
    "            raw_start = METRIC_COLORS[m]['sell']\n",
    "            raw_end = METRIC_COLORS[m]['buy']\n",
    "        max_flow = metric_edges[m]['flow'].max() if metric_edges[m] is not None and not metric_edges[m].empty else 1.0\n",
    "        intervals_to_draw = intervals\n",
    "        if SHOW_TIME_FADE_CHORDS and TIME_FADE_USE_DAILY_EDGES and time_fade_alpha:\n",
    "            if m == 'vwbr_z' and vwbr_z_brightness:\n",
    "                def _draw_key(edge):\n",
    "                    edge_date = edge.get('date')\n",
    "                    src_val = vwbr_z_brightness.get((edge['source'], edge_date))\n",
    "                    dst_val = vwbr_z_brightness.get((edge['dest'], edge_date))\n",
    "                    vals = [v for v in (src_val, dst_val) if v is not None]\n",
    "                    if vals:\n",
    "                        return sum(vals) / len(vals)\n",
    "                    return time_fade_alpha.get(edge_date, TIME_FADE_MIN_ALPHA)\n",
    "                intervals_to_draw = sorted(intervals, key=_draw_key)\n",
    "            else:\n",
    "                intervals_to_draw = sorted(intervals, key=lambda e: time_fade_alpha.get(e.get('date'), TIME_FADE_MIN_ALPHA))\n",
    "        for edge in intervals_to_draw:\n",
    "            flow = edge['flow']\n",
    "            lw = 0.6 + 2.2 * ((flow / max_flow) ** 0.6) if max_flow > 0 else 1.0\n",
    "            edge_date = edge.get('date')\n",
    "            alpha_factor = 1.0\n",
    "            start_brightness = 1.0\n",
    "            end_brightness = 1.0\n",
    "            if SHOW_TIME_FADE_CHORDS and TIME_FADE_USE_DAILY_EDGES and time_fade_alpha:\n",
    "                if edge_date in time_fade_alpha:\n",
    "                    alpha_factor = time_fade_alpha[edge_date]\n",
    "                    start_brightness = alpha_factor\n",
    "                    end_brightness = alpha_factor\n",
    "            if m == 'vwbr_z' and vwbr_z_brightness and edge_date is not None:\n",
    "                src_val = vwbr_z_brightness.get((edge['source'], edge_date))\n",
    "                dst_val = vwbr_z_brightness.get((edge['dest'], edge_date))\n",
    "                if src_val is not None:\n",
    "                    start_brightness = src_val\n",
    "                if dst_val is not None:\n",
    "                    end_brightness = dst_val\n",
    "            start_brightness = max(0.0, min(1.0, start_brightness))\n",
    "            end_brightness = max(0.0, min(1.0, end_brightness))\n",
    "            start_soften = min(1.0, CHORD_COLOR_SOFTEN + (1.0 - start_brightness) * 0.6)\n",
    "            end_soften = min(1.0, CHORD_COLOR_SOFTEN + (1.0 - end_brightness) * 0.6)\n",
    "            edge_color_start = soften_color(raw_start, start_soften)\n",
    "            edge_color_end = soften_color(raw_end, end_soften)\n",
    "            draw_ribbon(ax, edge['a0'], edge['a1'], edge['b0'], edge['b1'], CHORD_RADIUS, edge_color_start, edge_color_end, fill_alpha=CHORD_FILL_ALPHA * alpha_factor, line_alpha=CHORD_LINE_ALPHA * alpha_factor, lw=lw)\n",
    "\n",
    "    # Ticker labels\n",
    "    for t, ang in angles.items():\n",
    "        x, y = math.cos(ang), math.sin(ang)\n",
    "        r = CHORD_RADIUS + (ticker_outer - CHORD_RADIUS) * 0.4\n",
    "        rot = math.degrees(ang)\n",
    "        if math.pi / 2 < ang < 3 * math.pi / 2:\n",
    "            rot += 180\n",
    "        ax.text(r * x, r * y, t, color='#FFFFFF', fontsize=TICKER_FONTSIZE, fontweight='bold',\n",
    "                ha='center', va='center', rotation=rot, rotation_mode='anchor')\n",
    "\n",
    "    # Title and settings\n",
    "    date_range = f\"{window_dates_sorted[0]} -> {window_dates_sorted[-1]}\" if window_dates_sorted else 'n/a'\n",
    "    fig.text(0.55, 0.97, 'Institutional Money Flow Tracker', ha='center', va='top', color='white', fontsize=TITLE_FONTSIZE, fontweight='bold')\n",
    "    fig.text(0.55, 0.945, f'Date Range: {date_range}', ha='center', va='top', color='#C9D1D9', fontsize=SUBTITLE_FONTSIZE)\n",
    "    \n",
    "    # Upper-left Legend (Chords only)\n",
    "    leg = fig.add_axes([0.08, 0.75, 0.30, 0.18])\n",
    "    leg.axis('off')\n",
    "    leg.set_facecolor('none')\n",
    "    leg.set_xlim(0, 1)\n",
    "    leg.set_ylim(0, 1)\n",
    "\n",
    "    y = 0.95\n",
    "    leg.text(0.0, y, 'Chords', color='white', fontsize=LEGEND_TITLE_FONTSIZE, fontweight='bold', va='top')\n",
    "    y -= 0.15\n",
    "\n",
    "    chord_items = [\n",
    "        ('accum', 'Accumulation', METRIC_COLORS['accum']['sell'], METRIC_COLORS['accum']['buy']),\n",
    "        ('short', 'Short', METRIC_COLORS['short']['sell'], METRIC_COLORS['short']['buy']),\n",
    "        ('lit', 'Lit', METRIC_COLORS['lit']['sell'], METRIC_COLORS['lit']['buy']),\n",
    "        ('finra_buy', 'Finra Buy', METRIC_COLORS['finra_buy']['low'], METRIC_COLORS['finra_buy']['high']),\n",
    "        ('vwbr_z', 'VWBR Z (Distribution -> Accumulation)', METRIC_COLORS['vwbr_z']['sell'], METRIC_COLORS['vwbr_z']['buy']),\n",
    "    ]\n",
    "    # Legend gradients run left (min) -> right (max).\n",
    "    for key, label, c_start, c_end in chord_items:\n",
    "        if not metric_visible(key):\n",
    "            continue\n",
    "        xs = np.linspace(0.0, 0.20, 30)\n",
    "        points = np.column_stack([xs, np.full_like(xs, y)])\n",
    "        segments = np.stack([points[:-1], points[1:]], axis=1)\n",
    "        c0 = np.array(to_rgba(c_start))\n",
    "        c1 = np.array(to_rgba(c_end))\n",
    "        t_arr = np.linspace(0, 1, len(segments))[:, None]\n",
    "        colors = c0 * (1 - t_arr) + c1 * t_arr\n",
    "        leg.add_collection(LineCollection(segments, colors=colors, linewidths=LEGEND_LINEWIDTH))\n",
    "        leg.text(0.24, y, label, color='white', fontsize=LEGEND_LABEL_FONTSIZE, va='center')\n",
    "        y -= 0.15\n",
    "\n",
    "    # Bottom-left Ring Legend (Sizing + Coloring sections)\n",
    "    ring_leg = fig.add_axes([0.08, 0.00, 0.6, 0.22])\n",
    "    ring_leg.axis('off')\n",
    "    ring_leg.set_facecolor('none')\n",
    "    ring_leg.set_xlim(0, 1)\n",
    "    ring_leg.set_ylim(0, 1)\n",
    "\n",
    "    y = 0.98\n",
    "    ring_leg.text(0.0, y, 'Ring Sizing - how unusual is today', color='white', fontsize=RING_TITLE_FONTSIZE, fontweight='bold', va='top')\n",
    "    y -= 0.12\n",
    "\n",
    "    sizing_items = [\n",
    "        ('Inner Ring 1', 'Accumulation score vs. 20-day average'),\n",
    "        ('Center Ring 2', 'Lit total volume (buy + sell) vs. 20-day average'),\n",
    "        ('Outer Ring 3', 'FINRA buy volume vs. 20-day average'),\n",
    "    ]\n",
    "    for ring, desc in sizing_items:\n",
    "        ring_leg.text(0.02, y, f'{ring}:', color='#888888', fontsize=RING_LABEL_FONTSIZE, va='center')\n",
    "        ring_leg.text(0.14, y, desc, color='#C9D1D9', fontsize=RING_LABEL_FONTSIZE, va='center')\n",
    "        y -= 0.09\n",
    "\n",
    "    ring_leg.text(0.02, y, 'Each bar = 1 day', color='#C9D1D9', fontsize=RING_LABEL_FONTSIZE, va='center')\n",
    "    y -= 0.09\n",
    "\n",
    "    y -= 0.04\n",
    "    ring_leg.text(0.0, y, 'Ring Coloring - what is the direction of the flow today', color='white', fontsize=RING_TITLE_FONTSIZE, fontweight='bold', va='top')\n",
    "    y -= 0.12\n",
    "\n",
    "    ring3_mode = str(RING3_COLOR_MODE).upper()\n",
    "    if ring3_mode == 'VWBR_Z':\n",
    "        ring3_desc = 'VWBR Z (category tint bright->dark)'\n",
    "    elif ring3_mode == 'VWBR':\n",
    "        ring3_desc = 'VWBR buy ratio (category tint bright->dark)'\n",
    "    else:\n",
    "        ring3_desc = 'FINRA short buy/sell ratio (category tint bright->dark)'\n",
    "    ring3_color_start = '#E6E6E6'\n",
    "    ring3_color_end = '#444444'\n",
    "\n",
    "    coloring_items = [\n",
    "        ('Ring 1', 'Acc score 70 -> 30', RING_COLORS['accum']['positive'], RING_COLORS['accum']['negative']),\n",
    "        ('Ring 2', 'Lit buy ratio (buy -> sell)', '#66FF66', '#FF6666'),\n",
    "        ('Ring 3', ring3_desc, ring3_color_start, ring3_color_end),\n",
    "    ]\n",
    "    for ring, desc, c_start, c_end in coloring_items:\n",
    "        if c_start and c_end:\n",
    "            xs = np.linspace(0.0, 0.10, 30)\n",
    "            points = np.column_stack([xs, np.full_like(xs, y)])\n",
    "            segments = np.stack([points[:-1], points[1:]], axis=1)\n",
    "            c0 = np.array(to_rgba(c_start))\n",
    "            c1 = np.array(to_rgba(c_end))\n",
    "            t_arr = np.linspace(0, 1, len(segments))[:, None]\n",
    "            colors_arr = c0 * (1 - t_arr) + c1 * t_arr\n",
    "            ring_leg.add_collection(LineCollection(segments, colors=colors_arr, linewidths=LEGEND_LINEWIDTH))\n",
    "            ring_leg.text(0.12, y, f'{ring}: {desc}', color='#C9D1D9', fontsize=RING_LABEL_FONTSIZE, va='center')\n",
    "        else:\n",
    "            ring_leg.text(0.02, y, f'{ring}: {desc}', color='#C9D1D9', fontsize=RING_LABEL_FONTSIZE, va='center')\n",
    "        y -= 0.09\n",
    "\n",
    "    # Upper-right Category Legend (dynamic, gradient squares)\n",
    "    present_categories = [cat for cat in ['GLOBAL_MACRO', 'MAG8', 'THEMATIC_SECTORS', 'SECTOR_CORE', 'COMMODITIES', 'RATES_CREDIT', 'SPECULATIVE', 'CRYPTO'] \n",
    "                         if grouped.get(cat)]\n",
    "    \n",
    "    if present_categories:\n",
    "        cat_leg = fig.add_axes([0.85, 0.75, 0.18, 0.18])\n",
    "        cat_leg.axis('off')\n",
    "        cat_leg.set_facecolor('none')\n",
    "        cat_leg.set_xlim(0, 1)\n",
    "        cat_leg.set_ylim(0, 1)\n",
    "        \n",
    "        y = 0.95\n",
    "        cat_leg.text(0.0, y, 'Categories', color='white', fontsize=LEGEND_TITLE_FONTSIZE, fontweight='bold', va='top')\n",
    "        y -= 0.12\n",
    "        \n",
    "        for cat in present_categories:\n",
    "            base_color = CATEGORY_PALETTE.get(cat, '#A0A0A0')\n",
    "            dark_color = darken_color(base_color, 0.2)\n",
    "            \n",
    "            # Draw gradient square\n",
    "            for i in range(10):\n",
    "                t = i / 9.0\n",
    "                color = blend_color(dark_color, base_color, t)\n",
    "                rect = Rectangle((i * 0.008, y - 0.04), 0.008, 0.08, facecolor=color, edgecolor='none')\n",
    "                cat_leg.add_patch(rect)\n",
    "            \n",
    "            cat_leg.text(0.12, y, CATEGORY_LABELS.get(cat, cat), color='white', fontsize=LEGEND_LABEL_FONTSIZE, va='center')\n",
    "            y -= 0.16\n",
    "\n",
    "    # Top inflow/outflow table\n",
    "    def compute_trend_map(df_daily, value_col, dates):\n",
    "        if df_daily is None or df_daily.empty or not dates:\n",
    "            return {}\n",
    "        df = df_daily[[\"ticker\", \"date\", value_col]].dropna()\n",
    "        if df.empty:\n",
    "            return {}\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
    "        df = df[df[\"date\"].isin(dates)]\n",
    "        if df.empty:\n",
    "            return {}\n",
    "        dates_sorted = sorted(set(dates))\n",
    "        split = max(1, len(dates_sorted) // 2)\n",
    "        early_dates = set(dates_sorted[:split])\n",
    "        late_dates = set(dates_sorted[split:])\n",
    "        early = df[df[\"date\"].isin(early_dates)].groupby(\"ticker\")[value_col].mean()\n",
    "        late = df[df[\"date\"].isin(late_dates)].groupby(\"ticker\")[value_col].mean()\n",
    "        trend = (late - early).dropna()\n",
    "        return trend.to_dict()\n",
    "\n",
    "    def compute_anomaly_map_from_stats(df_daily, value_col, stats_df, mean_col, std_col, latest_date):\n",
    "        if df_daily is None or df_daily.empty or stats_df is None or stats_df.empty or latest_date is None:\n",
    "            return {}\n",
    "        df = df_daily[df_daily[\"date\"] == latest_date][[\"ticker\", value_col]].dropna()\n",
    "        if df.empty:\n",
    "            return {}\n",
    "        stats = stats_df.set_index(\"ticker\")\n",
    "        anomaly = {}\n",
    "        for row in df.itertuples():\n",
    "            if row.ticker not in stats.index:\n",
    "                continue\n",
    "            mean_val = stats.loc[row.ticker, mean_col]\n",
    "            std_val = stats.loc[row.ticker, std_col]\n",
    "            if std_val and std_val > 0:\n",
    "                anomaly[row.ticker] = abs((getattr(row, value_col) - mean_val) / std_val)\n",
    "        return anomaly\n",
    "\n",
    "    def compute_abs_latest_map(df_daily, value_col, latest_date):\n",
    "        if df_daily is None or df_daily.empty or latest_date is None:\n",
    "            return {}\n",
    "        df = df_daily[df_daily[\"date\"] == latest_date][[\"ticker\", value_col]].dropna()\n",
    "        if df.empty:\n",
    "            return {}\n",
    "        return df.set_index(\"ticker\")[value_col].abs().to_dict()\n",
    "\n",
    "    def compute_score_maps(flow_map, trend_map, anomaly_map):\n",
    "        if flow_map is None:\n",
    "            return {}, {}\n",
    "        flow_vals = [abs(v) for v in flow_map.values() if v is not None and np.isfinite(v)]\n",
    "        flow_scale = np.std(flow_vals) if len(flow_vals) > 1 else 1.0\n",
    "        if flow_scale == 0:\n",
    "            flow_scale = 1.0\n",
    "        trend_vals = [v for v in trend_map.values() if v is not None and np.isfinite(v)]\n",
    "        trend_scale = np.std(trend_vals) if len(trend_vals) > 1 else 1.0\n",
    "        if trend_scale == 0:\n",
    "            trend_scale = 1.0\n",
    "        buy_scores = {}\n",
    "        sell_scores = {}\n",
    "        for ticker in ticker_order:\n",
    "            flow = flow_map.get(ticker)\n",
    "            if flow is None or not np.isfinite(flow) or flow == 0:\n",
    "                continue\n",
    "            direction = 1 if flow > 0 else -1\n",
    "            flow_norm = abs(flow) / flow_scale\n",
    "            trend_norm = (trend_map.get(ticker, 0.0) / trend_scale) if trend_scale else 0.0\n",
    "            anomaly = anomaly_map.get(ticker, 0.0)\n",
    "            base = flow_norm + anomaly\n",
    "            if direction > 0:\n",
    "                buy_scores[ticker] = base + max(trend_norm, 0.0)\n",
    "            else:\n",
    "                sell_scores[ticker] = base + max(-trend_norm, 0.0)\n",
    "        return buy_scores, sell_scores\n",
    "\n",
    "    def top_tickers_from_scores(score_map, k=3):\n",
    "        if not score_map:\n",
    "            return 'n/a'\n",
    "        items = sorted(score_map.items(), key=lambda x: x[1], reverse=True)\n",
    "        return ', '.join([k for k, _ in items[:k]]) or 'n/a'\n",
    "\n",
    "    def top_tickers(net_map, positive=True, k=3):\n",
    "        if not net_map:\n",
    "            return 'n/a'\n",
    "        items = [(k, v) for k, v in net_map.items() if v is not None and np.isfinite(v)]\n",
    "        if positive:\n",
    "            items = sorted([x for x in items if x[1] > 0], key=lambda x: x[1], reverse=True)\n",
    "        else:\n",
    "            items = sorted([x for x in items if x[1] < 0], key=lambda x: x[1])\n",
    "        return ', '.join([k for k, _ in items[:k]]) or 'n/a'\n",
    "\n",
    "    latest_date = max(window_dates_sorted) if window_dates_sorted else None\n",
    "\n",
    "    # Accumulation signals\n",
    "    df_accum_daily_centered = df_accum_level.copy()\n",
    "    df_accum_daily_centered['date'] = pd.to_datetime(df_accum_daily_centered['date'], errors='coerce').dt.date\n",
    "    if not df_accum_daily_centered.empty:\n",
    "        daily_means = df_accum_daily_centered.groupby('date')['value'].mean()\n",
    "        df_accum_daily_centered['accum_centered'] = df_accum_daily_centered['value'] - df_accum_daily_centered['date'].map(daily_means)\n",
    "    accum_trend_map = compute_trend_map(df_accum_daily_centered, 'accum_centered', window_dates_sorted)\n",
    "    accum_anomaly_map = compute_anomaly_map_from_stats(df_accum_daily_centered, 'value', accum_stats, 'accum_mean', 'accum_std', latest_date)\n",
    "    accum_buy_scores, accum_sell_scores = compute_score_maps(metric_nets.get('accum', {}), accum_trend_map, accum_anomaly_map)\n",
    "\n",
    "    # Lit signals\n",
    "    lit_trend_map = compute_trend_map(df_lit_daily, 'lit_net', window_dates_sorted) if 'df_lit_daily' in dir() else {}\n",
    "    lit_anomaly_map = compute_anomaly_map_from_stats(df_lit_daily, 'lit_total', lit_stats, 'lit_vol_mean', 'lit_vol_std', latest_date) if 'df_lit_daily' in dir() else {}\n",
    "    lit_buy_scores, lit_sell_scores = compute_score_maps(metric_nets.get('lit', {}), lit_trend_map, lit_anomaly_map)\n",
    "\n",
    "    # VWBR Z signals\n",
    "    vwbr_z_trend_map = {}\n",
    "    vwbr_z_anomaly_map = {}\n",
    "    if 'df_finra_daily' in dir() and not df_finra_daily.empty and 'vwbr_z' in df_finra_daily.columns:\n",
    "        vwbr_z_trend_map = compute_trend_map(df_finra_daily, 'vwbr_z', window_dates_sorted)\n",
    "        vwbr_z_anomaly_map = compute_abs_latest_map(df_finra_daily, 'vwbr_z', latest_date)\n",
    "    vwbr_z_buy_scores, vwbr_z_sell_scores = compute_score_maps(metric_nets.get('vwbr_z', {}), vwbr_z_trend_map, vwbr_z_anomaly_map)\n",
    "\n",
    "    table_rows = [\n",
    "        ('Accumulation Score', top_tickers_from_scores(accum_buy_scores), top_tickers_from_scores(accum_sell_scores)),\n",
    "        ('Lit Buy/Sell Ratio', top_tickers_from_scores(lit_buy_scores), top_tickers_from_scores(lit_sell_scores)),\n",
    "        ('VWBR Z', top_tickers_from_scores(vwbr_z_buy_scores), top_tickers_from_scores(vwbr_z_sell_scores)),\n",
    "    ]\n",
    "    col1, col2, col3 = TABLE_COL1_WIDTH, TABLE_COL2_WIDTH, TABLE_COL3_WIDTH\n",
    "    table_lines = [f\"{'Metric':<{col1}}{'Buy (Top)':<{col2}}{'Sell (Top)':<{col3}}\", '-' * (col1 + col2 + col3)]\n",
    "    for label, buy_str, sell_str in table_rows:\n",
    "        table_lines.append(f\"{label:<{col1}}{buy_str:<{col2}}{sell_str:<{col3}}\")\n",
    "    fig.text(0.55, 0.05, '\\n'.join(table_lines), ha='left', va='bottom', color='#C9D1D9', fontsize=TABLE_FONTSIZE, fontfamily='monospace', linespacing=1.2)\n",
    "    \n",
    "    # Watermark\n",
    "    if os.path.exists(WATERMARK_PATH):\n",
    "        watermark_img = plt.imread(WATERMARK_PATH)\n",
    "        wm_h, wm_w = watermark_img.shape[:2]\n",
    "        wm_aspect = (wm_h / wm_w) if wm_w else 1.0\n",
    "        wm_w_frac = WATERMARK_WIDTH\n",
    "        wm_h_frac = wm_w_frac * wm_aspect\n",
    "        wm_left = 1 - wm_w_frac -0.03\n",
    "        wm_bottom = 0.13\n",
    "        wm_ax = fig.add_axes([wm_left, wm_bottom, wm_w_frac, wm_h_frac])\n",
    "        wm_ax.axis('off')\n",
    "        wm_ax.imshow(watermark_img, alpha=WATERMARK_ALPHA)\n",
    "\n",
    "    # Save circos plot to output directory\n",
    "    output_dir = r'C:\\Users\\fvign\\Dropbox\\Vscode\\darkpool\\darkpool_analysis\\output\\circos_plot'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if window_dates_sorted:\n",
    "        start_dt = str(window_dates_sorted[0]).replace('-', '')\n",
    "        end_dt = str(window_dates_sorted[-1]).replace('-', '')\n",
    "        filename = f'circos_{start_dt}_to_{end_dt}.png'\n",
    "    else:\n",
    "        filename = 'circos_plot.png'\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    fig.savefig(output_path, dpi=150, facecolor=fig.get_facecolor(), edgecolor='none', bbox_inches='tight')\n",
    "    print(f'Circos plot saved to: {output_path}')\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eeaa72d",
   "metadata": {},
   "source": [
    "# Cell 1\n",
    "# Circos-Style Money Flow Chord Diagram\n",
    "\n",
    "This notebook builds a self-contained Circos-style chord diagram from Accumulation Score time series in a local database.\n",
    "\n",
    "Constraints honored:\n",
    "- No project modules are imported or modified.\n",
    "- Database access is read-only (SELECT-only).\n",
    "- No files are written or mutated; all outputs are in-notebook only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c759775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticker groups (from ticker_dictionary.py)\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "def _load_ticker_dictionary():\n",
    "    candidates = [\n",
    "        Path.cwd() / \"ticker_dictionary.py\",\n",
    "        Path.cwd() / \"Special_tools\" / \"ticker_dictionary.py\",\n",
    "    ]\n",
    "    for path in candidates:\n",
    "        if path.exists():\n",
    "            spec = importlib.util.spec_from_file_location(\"ticker_dictionary\", path)\n",
    "            if spec and spec.loader:\n",
    "                module = importlib.util.module_from_spec(spec)\n",
    "                spec.loader.exec_module(module)\n",
    "                return module\n",
    "    raise FileNotFoundError(\"ticker_dictionary.py not found in current or Special_tools directory\")\n",
    "\n",
    "\n",
    "ticker_dict = _load_ticker_dictionary()\n",
    "SECTOR_ZOOM_MAP = ticker_dict.SECTOR_ZOOM_MAP\n",
    "SECTOR_CORE_TICKERS = list(SECTOR_ZOOM_MAP[\"SECTOR_CORE\"].keys())\n",
    "SECTOR_SUMMARY_TICKERS = list(SECTOR_ZOOM_MAP[\"SECTOR_SUMMARY\"].keys())\n",
    "GLOBAL_MACRO_TICKERS = list(SECTOR_ZOOM_MAP[\"GLOBAL_MACRO\"].keys())\n",
    "COMMODITIES_TICKERS = list(SECTOR_ZOOM_MAP[\"COMMODITIES\"].keys())\n",
    "MAG8_TICKERS = list(SECTOR_ZOOM_MAP[\"MAG8\"][\"MAG8\"])\n",
    "SPECULATIVE_TICKERS = list(getattr(ticker_dict, \"SPECULATIVE_TICKERS\", []))\n",
    "\n",
    "TICKER_TYPE = [\"SECTOR\", \"SUMMARY\", \"GLOBAL\", \"COMMODITIES\", \"MAG8\"]\n",
    "TICKER_TYPE_OPTIONS = [\"SECTOR\", \"SUMMARY\", \"GLOBAL\", \"COMMODITIES\", \"MAG8\", \"SPECULATIVE\", \"ALL\"]\n",
    "\n",
    "def _normalize_ticker_types(value):\n",
    "    if value is None:\n",
    "        return []\n",
    "    if isinstance(value, (list, tuple, set)):\n",
    "        return [str(v).upper() for v in value]\n",
    "    return [str(value).upper()]\n",
    "\n",
    "_selected_types = _normalize_ticker_types(TICKER_TYPE)\n",
    "if not _selected_types:\n",
    "    _selected_types = [\"ALL\"]\n",
    "unknown_types = [t for t in _selected_types if t not in TICKER_TYPE_OPTIONS]\n",
    "if unknown_types:\n",
    "    raise ValueError(f\"Unknown TICKER_TYPE: {unknown_types}\")\n",
    "if \"ALL\" in _selected_types:\n",
    "    _selected_types = [t for t in TICKER_TYPE_OPTIONS if t != \"ALL\"]\n",
    "\n",
    "ENABLED_GROUPS = {\n",
    "    \"GLOBAL_MACRO\": \"GLOBAL\" in _selected_types,\n",
    "    \"MAG8\": \"MAG8\" in _selected_types,\n",
    "    \"SECTOR_SUMMARY\": \"SUMMARY\" in _selected_types,\n",
    "    \"SECTOR_CORE\": \"SECTOR\" in _selected_types,\n",
    "    \"COMMODITIES\": \"COMMODITIES\" in _selected_types,\n",
    "    \"SPECULATIVE\": \"SPECULATIVE\" in _selected_types,\n",
    "}\n",
    "\n",
    "# Controls\n",
    "END_DATE = None                         # End of window; None = auto-detect max date in DB\n",
    "FLOW_PERIOD_DAYS = 5                   # Trading-day window used for flows (start->end inside this window)\n",
    "TOP_K_WINNERS =12                       # Max winners by demand (positive delta) to include\n",
    "TOP_K_LOSERS = 12                        # Max losers by supply (negative delta) to include\n",
    "MIN_EDGE_FLOW = 0.0                     # Drop edges smaller than this flow (pre-strand)\n",
    "DISTRIBUTION_MODE = \"demand_weighted\"   # \"equal\" or \"demand_weighted\" split from sources to winners\n",
    "\n",
    "# Chord + ring layout\n",
    "METRIC_BAND_MODE = \"proportional\"       # \"equal\" = same band width per metric; \"proportional\" = band width by flow\n",
    "MAX_EDGES_PER_METRIC = 80              # cap edges per metric to keep plot readable\n",
    "EDGE_RIBBON_SPLITS = 25                 # split each edge into N thin ribbons (approx density)\n",
    "EDGE_RIBBON_MAX = 60                    # cap ribbons per edge (perf)\n",
    "CHORD_ARC_FRACTION = 0.5                # fraction of ticker arc reserved for chord endpoints\n",
    "CHORD_RADIUS = 0.78                     # inner radius for chord ribbons\n",
    "TIME_SLICE_BINS = 30                     # number of time slices per outer ring\n",
    "RING_BASE_THICKNESS = 0.005             # minimum ring thickness\n",
    "RING_THICKNESS_SCALE = 0.1             # added thickness scaled by magnitude\n",
    "RING_GAP = 0.01                         # gap between metric rings\n",
    "BAND_GAP_FRAC = 0.02                    # gap between metric bands within a ticker\n",
    "DIR_GAP_FRAC = 0.02                     # gap between outflow/inflow halves inside a band\n",
    "CATEGORY_GAP_DEG = 1                    # degrees; category block gap (0 = uniform spacing)\n",
    "\n",
    "# Fanned chord layout config\n",
    "RIBBON_MIN_WIDTH_RAD = {                # minimum ribbon arc width in radians (per-metric)\n",
    "    'accum': 0.002,\n",
    "    'short': 0.002,\n",
    "    'lit': 0.002,\n",
    "    'finra_buy': 0.002,\n",
    "}\n",
    "RIBBON_GAP_RAD = 0.002                  # gap between ribbons in radians\n",
    "RIBBON_WIDTH_SCALE_BY_FLOW = True       # if True, ribbon width scales with flow; if False, all equal\n",
    "RIBBON_CENTERED = True                  # if True, ribbons fan out from center; if False, from edge\n",
    "RIBBON_CENTER_OFFSET = {                # per-metric center offset in radians (positive = clockwise)\n",
    "    'accum': 0.0,                       # accumulation stays centered\n",
    "    'short': 0.5,                       # short offset (adjust as needed)\n",
    "    'lit': -0.5,                        # lit offset (adjust as needed)\n",
    "    'finra_buy': 0.0,                   # finra buy offset\n",
    "}\n",
    "\n",
    "# Render quality (performance vs fidelity)\n",
    "RENDER_MODE = \"balanced\"  # \"fast\", \"balanced\", \"quality\"\n",
    "CHORD_FILL_ALPHA = 0.55\n",
    "CHORD_LINE_ALPHA = 0.8\n",
    "CHORD_COLOR_SOFTEN = 0.25  # blend toward background to reduce saturation\n",
    "if RENDER_MODE == \"fast\":\n",
    "    USE_GRADIENT_FILL = False\n",
    "    CHORD_GRADIENT_STEPS = 8\n",
    "    CHORD_ARC_POINTS = 8\n",
    "    CHORD_CURVE_POINTS = 30\n",
    "elif RENDER_MODE == \"quality\":\n",
    "    USE_GRADIENT_FILL = True\n",
    "    CHORD_GRADIENT_STEPS = 32\n",
    "    CHORD_ARC_POINTS = 18\n",
    "    CHORD_CURVE_POINTS = 70\n",
    "else:\n",
    "    USE_GRADIENT_FILL = True\n",
    "    CHORD_GRADIENT_STEPS = 18\n",
    "    CHORD_ARC_POINTS = 12\n",
    "    CHORD_CURVE_POINTS = 50\n",
    "\n",
    "# Layer toggles\n",
    "SHOW_ACCUM_FLOW = False\n",
    "SHOW_LIT_FLOW = False\n",
    "SHOW_SHORT_FLOW = True\n",
    "SHOW_FINRA_FLOW = False                  # 4th chord: finra buy volume flow\n",
    "SHOW_VOLUME_RING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b232f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 â€“ DB discovery & connection helpers\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def find_db_candidates():\n",
    "    root = Path('.').resolve()\n",
    "    search_dirs = [\n",
    "        root / 'data',\n",
    "        root / 'darkpool_analysis' / 'data',\n",
    "        root.parent / 'darkpool_analysis' / 'data',  # Parent directory for Special_tools\n",
    "        root,\n",
    "    ]\n",
    "    patterns = ['*.duckdb', '*.db', '*.sqlite', '*.sqlite3']\n",
    "    candidates = []\n",
    "    for d in search_dirs:\n",
    "        if d.is_dir():\n",
    "            for pat in patterns:\n",
    "                candidates.extend(d.glob(pat))\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for p in candidates:\n",
    "        rp = p.resolve()\n",
    "        if rp not in seen:\n",
    "            seen.add(rp)\n",
    "            unique.append(p)\n",
    "    unique.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return unique\n",
    "\n",
    "def connect_db(db_path):\n",
    "    db_path = Path(db_path)\n",
    "    suffix = db_path.suffix.lower()\n",
    "    errors = []\n",
    "\n",
    "    def try_duckdb():\n",
    "        import duckdb\n",
    "        return duckdb.connect(database=str(db_path), read_only=True), 'duckdb'\n",
    "\n",
    "    def try_sqlite():\n",
    "        import sqlite3\n",
    "        con = sqlite3.connect(str(db_path))\n",
    "        con.row_factory = sqlite3.Row\n",
    "        return con, 'sqlite'\n",
    "\n",
    "    if suffix == '.duckdb':\n",
    "        attempts = [try_duckdb, try_sqlite]\n",
    "    else:\n",
    "        attempts = [try_sqlite, try_duckdb]\n",
    "\n",
    "    for fn in attempts:\n",
    "        try:\n",
    "            return fn()\n",
    "        except Exception as e:\n",
    "            errors.append(f'{fn.__name__}: {e}')\n",
    "    raise RuntimeError('Unable to open database. ' + '; '.join(errors))\n",
    "\n",
    "def get_tables(conn, db_type):\n",
    "    if db_type == 'duckdb':\n",
    "        return [r[0] for r in conn.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema='main'\").fetchall()]\n",
    "    else:\n",
    "        return [r[0] for r in conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()]\n",
    "\n",
    "def get_columns(conn, db_type, table):\n",
    "    if db_type == 'duckdb':\n",
    "        return [r[0] for r in conn.execute(f\"SELECT column_name FROM information_schema.columns WHERE table_name='{table}'\").fetchall()]\n",
    "    else:\n",
    "        return [r[1] for r in conn.execute(f\"PRAGMA table_info('{table}')\").fetchall()]\n",
    "\n",
    "def pick_column(columns, candidates):\n",
    "    cols_lower = {c.lower(): c for c in columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def quote_ident(name):\n",
    "    return f'\"{name}\"'\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Column name candidates for auto-detection\n",
    "# ---------------------------------------------------------------------------\n",
    "# Priority order: daily_metrics has accumulation scores\n",
    "TABLE_CANDIDATES = ['daily_metrics', 'scanner_daily_metrics', 'darkpool_metrics', 'metrics', 'darkpool', 'accumulation']\n",
    "TICKER_COL_CANDIDATES = ['ticker', 'symbol', 'stock', 'name']\n",
    "DATE_COL_CANDIDATES = ['date', 'trade_date', 'dt', 'timestamp']\n",
    "ACCUM_COL_CANDIDATES = ['accumulation_score_display', 'accum_score_display', \n",
    "                        'accumulation_score', 'accum_score', 'accumulation', 'accum']\n",
    "\n",
    "# Volume column candidates\n",
    "SHORT_BUY_CANDIDATES = ['finra_buy_volume', 'short_buy_volume', 'short_buy', 'short_buy_vol']\n",
    "SHORT_SELL_CANDIDATES = ['short_sell_volume', 'short_sell', 'short_sell_vol']\n",
    "LIT_BUY_CANDIDATES = ['lit_buy_volume', 'lit_buy', 'lit_buy_vol']\n",
    "LIT_SELL_CANDIDATES = ['lit_sell_volume', 'lit_sell', 'lit_sell_vol']\n",
    "OTC_VOLUME_CANDIDATES = ['otc_off_exchange_volume', 'otc_volume', 'dark_volume']\n",
    "LIT_TOTAL_CANDIDATES = ['lit_total_volume', 'lit_volume', 'lit_total']\n",
    "FINRA_BUY_CANDIDATES = ['finra_buy_volume', 'finra_buy', 'finra_buy_vol']\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Discover & connect\n",
    "# ---------------------------------------------------------------------------\n",
    "db_candidates = find_db_candidates()\n",
    "\n",
    "print('DB candidates (newest first):')\n",
    "for p in db_candidates:\n",
    "    print(' -', p)\n",
    "\n",
    "if not db_candidates:\n",
    "    print('No database files found under ./data or ./darkpool_analysis/data.')\n",
    "    raise SystemExit\n",
    "\n",
    "DB_PATH = db_candidates[0]\n",
    "print(f'Using DB: {DB_PATH}')\n",
    "\n",
    "conn, db_type = connect_db(DB_PATH)\n",
    "print(f'Connected via {db_type}')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Auto-detect table and column names\n",
    "# ---------------------------------------------------------------------------\n",
    "tables = get_tables(conn, db_type)\n",
    "print(f'Tables found: {tables}')\n",
    "\n",
    "SELECT_TABLE = None\n",
    "for cand in TABLE_CANDIDATES:\n",
    "    matching = [t for t in tables if t.lower() == cand.lower()]\n",
    "    if matching:\n",
    "        SELECT_TABLE = matching[0]\n",
    "        break\n",
    "if not SELECT_TABLE and tables:\n",
    "    SELECT_TABLE = tables[0]\n",
    "\n",
    "if not SELECT_TABLE:\n",
    "    print('No suitable table found in database.')\n",
    "    conn.close()\n",
    "    raise SystemExit\n",
    "\n",
    "print(f'Using table: {SELECT_TABLE}')\n",
    "\n",
    "columns = get_columns(conn, db_type, SELECT_TABLE)\n",
    "print(f'Columns in {SELECT_TABLE}: {columns}')\n",
    "\n",
    "TICKER_COL = pick_column(columns, TICKER_COL_CANDIDATES)\n",
    "DATE_COL = pick_column(columns, DATE_COL_CANDIDATES)\n",
    "ACCUM_COL = pick_column(columns, ACCUM_COL_CANDIDATES)\n",
    "\n",
    "if not TICKER_COL:\n",
    "    print(f'Could not find ticker column. Available: {columns}')\n",
    "    conn.close()\n",
    "    raise SystemExit\n",
    "if not DATE_COL:\n",
    "    print(f'Could not find date column. Available: {columns}')\n",
    "    conn.close()\n",
    "    raise SystemExit\n",
    "if not ACCUM_COL:\n",
    "    print(f'Could not find accumulation column. Available: {columns}')\n",
    "    conn.close()\n",
    "    raise SystemExit\n",
    "\n",
    "print(f'Column mapping: ticker={TICKER_COL}, date={DATE_COL}, accum={ACCUM_COL}')\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "def build_ticker_universe():\n",
    "    seen = set()\n",
    "    ordered = []\n",
    "    categories = {}\n",
    "    enabled = globals().get(\"ENABLED_GROUPS\", {})\n",
    "    group_defs = [\n",
    "        (\"GLOBAL_MACRO\", globals().get(\"GLOBAL_MACRO_TICKERS\", [])),\n",
    "        (\"MAG8\", globals().get(\"MAG8_TICKERS\", [])),\n",
    "        (\"SECTOR_SUMMARY\", globals().get(\"SECTOR_SUMMARY_TICKERS\", [])),\n",
    "        (\"SECTOR_CORE\", globals().get(\"SECTOR_CORE_TICKERS\", [])),\n",
    "        (\"COMMODITIES\", globals().get(\"COMMODITIES_TICKERS\", [])),\n",
    "        (\"SPECULATIVE\", globals().get(\"SPECULATIVE_TICKERS\", [])),\n",
    "    ]\n",
    "    for group_name, tickers in group_defs:\n",
    "        if not enabled.get(group_name, True):\n",
    "            continue\n",
    "        for t in tickers:\n",
    "            if t not in seen:\n",
    "                seen.add(t)\n",
    "                ordered.append(t)\n",
    "                categories[t] = group_name\n",
    "    return ordered, categories\n",
    "\n",
    "\n",
    "ticker_order, ticker_category = build_ticker_universe()\n",
    "print(\"Ticker universe size:\", len(ticker_order))\n",
    "print(\"Enabled groups:\", {k: v for k, v in ENABLED_GROUPS.items()})\n",
    "ticker_list = [t.upper() for t in ticker_order]\n",
    "import re\n",
    "\n",
    "conn, DB_TYPE = connect_db(DB_PATH)\n",
    "\n",
    "if DB_TYPE == 'duckdb':\n",
    "    date_expr = f\"CAST({quote_ident(DATE_COL)} AS DATE)\"\n",
    "    accum_expr = f\"{quote_ident(ACCUM_COL)}\"\n",
    "else:\n",
    "    date_expr = f\"DATE({quote_ident(DATE_COL)})\"\n",
    "    accum_expr = f\"{quote_ident(ACCUM_COL)}\"\n",
    "\n",
    "placeholders = ','.join(['?'] * len(ticker_list))\n",
    "query = (\n",
    "    f\"SELECT UPPER({quote_ident(TICKER_COL)}) AS ticker, \"\n",
    "    f\"{date_expr} AS date, {accum_expr} AS accumulation_score \"\n",
    "    f\"FROM {quote_ident(SELECT_TABLE)} \"\n",
    "    f\"WHERE UPPER({quote_ident(TICKER_COL)}) IN ({placeholders})\"\n",
    ")\n",
    "\n",
    "print(f\"Using accumulation table: {SELECT_TABLE} (ticker={TICKER_COL}, date={DATE_COL}, accum={ACCUM_COL})\")\n",
    "\n",
    "try:\n",
    "    if DB_TYPE == 'duckdb':\n",
    "        df_raw = conn.execute(query, ticker_list).df()\n",
    "    else:\n",
    "        df_raw = pd.read_sql_query(query, conn, params=ticker_list)\n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "if df_raw.empty:\n",
    "    print('No data returned for the specified tickers.')\n",
    "    raise SystemExit\n",
    "\n",
    "print(\"Accumulation rows loaded:\", len(df_raw))\n",
    "print(\"Accumulation table date range:\", df_raw[\"date\"].min(), \"->\", df_raw[\"date\"].max())\n",
    "print(\"Accumulation raw sample:\", df_raw[\"accumulation_score\"].head(5).tolist())\n",
    "df_raw['date'] = pd.to_datetime(df_raw['date'], errors='coerce').dt.date\n",
    "df_raw['ticker'] = df_raw['ticker'].str.upper()\n",
    "\n",
    "def fetch_accum_column(accum_col_name):\n",
    "    conn, db_type = connect_db(DB_PATH)\n",
    "    try:\n",
    "        if db_type == 'duckdb':\n",
    "            date_expr = f\"CAST({quote_ident(DATE_COL)} AS DATE)\"\n",
    "        else:\n",
    "            date_expr = f\"DATE({quote_ident(DATE_COL)})\"\n",
    "        query = (\n",
    "            f\"SELECT UPPER({quote_ident(TICKER_COL)}) AS ticker, \"\n",
    "            f\"{date_expr} AS date, {quote_ident(accum_col_name)} AS accumulation_score \"\n",
    "            f\"FROM {quote_ident(SELECT_TABLE)} \"\n",
    "            f\"WHERE UPPER({quote_ident(TICKER_COL)}) IN ({placeholders})\"\n",
    "        )\n",
    "        if db_type == 'duckdb':\n",
    "            return conn.execute(query, ticker_list).df()\n",
    "        return pd.read_sql_query(query, conn, params=ticker_list)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def normalize_accum_df(df):\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.date\n",
    "    df['ticker'] = df['ticker'].str.upper()\n",
    "    df['accumulation_score'] = df['accumulation_score'].apply(parse_accum)\n",
    "    return df.dropna(subset=['date', 'accumulation_score'])\n",
    "\n",
    "def parse_accum(value):\n",
    "    if value is None:\n",
    "        return np.nan\n",
    "    if isinstance(value, (int, float, np.number)):\n",
    "        if np.isnan(value):\n",
    "            return np.nan\n",
    "        return float(value)\n",
    "    s = str(value).strip()\n",
    "    if not s:\n",
    "        return np.nan\n",
    "    s = s.replace('%', '').replace(',', '')\n",
    "    m = re.search(r'[-+]?\\d+\\.?\\d*', s)\n",
    "    return float(m.group(0)) if m else np.nan\n",
    "\n",
    "df_raw = normalize_accum_df(df_raw)\n",
    "ACCUM_COL_SELECTED = ACCUM_COL\n",
    "if df_raw.empty:\n",
    "    print('Primary accumulation column returned no usable values; trying fallbacks...')\n",
    "    conn, db_type = connect_db(DB_PATH)\n",
    "    try:\n",
    "        cols = get_columns(conn, db_type, SELECT_TABLE)\n",
    "    finally:\n",
    "        conn.close()\n",
    "    candidates = []\n",
    "    for cand in [\n",
    "        'accumulation_score_display', 'accum_score_display',\n",
    "        'accumulation_score', 'accum_score', 'accumulation', 'accum'\n",
    "    ]:\n",
    "        col = pick_column(cols, [cand])\n",
    "        if col and col not in candidates:\n",
    "            candidates.append(col)\n",
    "    candidates = [c for c in candidates if c != ACCUM_COL]\n",
    "    print('Accumulation fallback candidates:', candidates)\n",
    "    for col in candidates:\n",
    "        print('Trying accumulation column:', col)\n",
    "        df_try = fetch_accum_column(col)\n",
    "        print('Fallback rows loaded:', len(df_try))\n",
    "        print('Fallback raw sample:', df_try['accumulation_score'].head(5).tolist())\n",
    "        df_try = normalize_accum_df(df_try)\n",
    "        if not df_try.empty:\n",
    "            ACCUM_COL_SELECTED = col\n",
    "            df_raw = df_try\n",
    "            break\n",
    "\n",
    "if df_raw.empty:\n",
    "    print('All accumulation score rows are null after parsing.')\n",
    "    raise SystemExit\n",
    "print('Using accumulation column:', ACCUM_COL_SELECTED)\n",
    "\n",
    "df_raw_full = df_raw.copy()\n",
    "\n",
    "max_date = df_raw['date'].max()\n",
    "if END_DATE is None or str(END_DATE).strip() == '':\n",
    "    END_DATE_RESOLVED = max_date\n",
    "else:\n",
    "    END_DATE_RESOLVED = pd.to_datetime(END_DATE).date()\n",
    "    if END_DATE_RESOLVED > max_date:\n",
    "        print(f'END_DATE {END_DATE_RESOLVED} exceeds DB max date {max_date}; using max date.')\n",
    "        END_DATE_RESOLVED = max_date\n",
    "\n",
    "flow_days = int(FLOW_PERIOD_DAYS) if FLOW_PERIOD_DAYS and int(FLOW_PERIOD_DAYS) > 0 else 1\n",
    "print(\"Flow period days:\", flow_days, \"END_DATE:\", END_DATE_RESOLVED)\n",
    "\n",
    "all_dates = sorted([d for d in df_raw[\"date\"].unique() if pd.notna(d)])\n",
    "end_dates = [d for d in all_dates if d <= END_DATE_RESOLVED]\n",
    "window_dates = end_dates[-flow_days:]\n",
    "if not window_dates:\n",
    "    print(\"No dates available within FLOW_PERIOD_DAYS window.\")\n",
    "    raise SystemExit\n",
    "print(\"Window date range used:\", window_dates[0], \"->\", window_dates[-1], \"count:\", len(window_dates))\n",
    "df_accum_daily = df_raw_full.sort_values([\"ticker\", \"date\"]).copy()\n",
    "df_accum_daily[\"accum_net\"] = df_accum_daily.groupby(\"ticker\")[\"accumulation_score\"].diff()\n",
    "df_accum_daily[\"accum_net\"] = df_accum_daily[\"accum_net\"].fillna(0.0)\n",
    "df_accum_daily = df_accum_daily[df_accum_daily[\"date\"].isin(window_dates)][[\"ticker\", \"date\", \"accum_net\"]]\n",
    "df_raw = df_raw[df_raw[\"date\"].isin(window_dates)]\n",
    "print(\"Accumulation window rows:\", len(df_raw))\n",
    "missing = sorted(set(ticker_list) - set(df_raw[\"ticker\"].unique()))\n",
    "if missing:\n",
    "    print(\"Missing tickers in accumulation window:\", missing)\n",
    "if df_raw.empty:\n",
    "    print(\"No accumulation data within the selected window.\")\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "def tail_n(df_ticker, n, end_date):\n",
    "    df = df_ticker[df_ticker['date'] <= end_date].sort_values('date')\n",
    "    if df.empty:\n",
    "        return df\n",
    "    return df.tail(n)\n",
    "\n",
    "\n",
    "rows = []\n",
    "for ticker in ticker_order:\n",
    "    df_t = df_raw[df_raw['ticker'] == ticker]\n",
    "    tail = tail_n(df_t, flow_days, END_DATE_RESOLVED)\n",
    "    if len(tail) < 2:\n",
    "        a_start = None\n",
    "        a_end = None\n",
    "    else:\n",
    "        a_start = float(tail['accumulation_score'].iloc[0])\n",
    "        a_end = float(tail['accumulation_score'].iloc[-1])\n",
    "    rows.append({\n",
    "        'ticker': ticker,\n",
    "        'category': ticker_category.get(ticker, 'UNKNOWN'),\n",
    "        'A_end': a_end,\n",
    "        'A_start': a_start,\n",
    "        'end_date': END_DATE_RESOLVED,\n",
    "        'start_date': tail['date'].iloc[0] if len(tail) else None,\n",
    "        'samples': len(tail),\n",
    "    })\n",
    "\n",
    "df_scores = pd.DataFrame(rows)\n",
    "\n",
    "# --- Volume data discovery (lit/short buy/sell + new columns for rings) ---\n",
    "LIT_BUY_CANDIDATES = ['lit_buy_volume', 'lit_buy_vol', 'lit_buy']\n",
    "LIT_SELL_CANDIDATES = ['lit_sell_volume', 'lit_sell_vol', 'lit_sell']\n",
    "SHORT_BUY_CANDIDATES = ['finra_buy_volume', 'short_buy_volume', 'short_buy_vol', 'short_buy']\n",
    "SHORT_SELL_CANDIDATES = ['short_sell_volume', 'short_sell_vol', 'short_sell']\n",
    "# New columns for rings\n",
    "OTC_VOLUME_CANDIDATES = ['otc_off_exchange_volume', 'otc_volume', 'dark_volume']\n",
    "LIT_TOTAL_CANDIDATES = ['lit_total_volume', 'lit_volume', 'lit_total']\n",
    "FINRA_BUY_CANDIDATES = ['finra_buy_volume', 'finra_buy', 'finra_buy_vol']\n",
    "SHORT_RATIO_CANDIDATES = ['short_ratio']\n",
    "SHORT_BUY_SELL_RATIO_CANDIDATES = ['short_buy_sell_ratio', 'short_buy_sell_ratio_z']\n",
    "\n",
    "\n",
    "def find_volume_table(conn, db_type):\n",
    "    tables = get_tables(conn, db_type)  # Fixed: was list_tables\n",
    "    candidates = []\n",
    "    for table in tables:\n",
    "        try:\n",
    "            cols = get_columns(conn, db_type, table)\n",
    "        except Exception:\n",
    "            continue\n",
    "        ticker_col = pick_column(cols, TICKER_COL_CANDIDATES)\n",
    "        date_col = pick_column(cols, DATE_COL_CANDIDATES)\n",
    "        lit_buy_col = pick_column(cols, LIT_BUY_CANDIDATES)\n",
    "        lit_sell_col = pick_column(cols, LIT_SELL_CANDIDATES)\n",
    "        short_buy_col = pick_column(cols, SHORT_BUY_CANDIDATES)\n",
    "        short_sell_col = pick_column(cols, SHORT_SELL_CANDIDATES)\n",
    "        # New columns (optional but preferred)\n",
    "        otc_vol_col = pick_column(cols, OTC_VOLUME_CANDIDATES)\n",
    "        lit_total_col = pick_column(cols, LIT_TOTAL_CANDIDATES)\n",
    "        finra_buy_col = pick_column(cols, FINRA_BUY_CANDIDATES)\n",
    "        short_ratio_col = pick_column(cols, SHORT_RATIO_CANDIDATES)\n",
    "        if not (ticker_col and date_col and lit_buy_col and lit_sell_col and short_buy_col and short_sell_col):\n",
    "            continue\n",
    "        row_count = None\n",
    "        distinct_tickers = None\n",
    "        try:\n",
    "            row_count = conn.execute(\n",
    "                f'SELECT COUNT(*) FROM {quote_ident(table)}'\n",
    "            ).fetchone()[0]\n",
    "            distinct_tickers = conn.execute(\n",
    "                f'SELECT COUNT(DISTINCT {quote_ident(ticker_col)}) FROM {quote_ident(table)}'\n",
    "            ).fetchone()[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "        candidates.append({\n",
    "            'table': table,\n",
    "            'ticker_col': ticker_col,\n",
    "            'date_col': date_col,\n",
    "            'lit_buy_col': lit_buy_col,\n",
    "            'lit_sell_col': lit_sell_col,\n",
    "            'short_buy_col': short_buy_col,\n",
    "            'short_sell_col': short_sell_col,\n",
    "            'otc_vol_col': otc_vol_col,\n",
    "            'lit_total_col': lit_total_col,\n",
    "            'finra_buy_col': finra_buy_col,\n",
    "            'short_ratio_col': short_ratio_col,\n",
    "            'short_buy_sell_ratio_col': pick_column(cols, SHORT_BUY_SELL_RATIO_CANDIDATES),\n",
    "            'row_count': row_count,\n",
    "            'distinct_tickers': distinct_tickers,\n",
    "        })\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates.sort(key=lambda r: (r['row_count'] or 0, r['distinct_tickers'] or 0), reverse=True)\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "conn, DB_TYPE = connect_db(DB_PATH)\n",
    "try:\n",
    "    volume_info = find_volume_table(conn, DB_TYPE)\n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "VOLUME_DATA_AVAILABLE = volume_info is not None\n",
    "if VOLUME_DATA_AVAILABLE:\n",
    "    print(\"Volume table selected:\", volume_info[\"table\"])\n",
    "    print(\"Volume columns:\", {k: volume_info[k] for k in [\"ticker_col\", \"date_col\", \"lit_buy_col\", \"lit_sell_col\", \"short_buy_col\", \"short_sell_col\"]})\n",
    "    print(\"Ring columns:\", {k: volume_info.get(k) for k in [\"otc_vol_col\", \"lit_total_col\", \"finra_buy_col\"]})\n",
    "else:\n",
    "    print(\"No volume table found with lit/short buy/sell columns.\")\n",
    "\n",
    "if VOLUME_DATA_AVAILABLE:\n",
    "    conn, DB_TYPE = connect_db(DB_PATH)\n",
    "    try:\n",
    "        if DB_TYPE == 'duckdb':\n",
    "            date_expr = f\"CAST({quote_ident(volume_info['date_col'])} AS DATE)\"\n",
    "            num_cast = \"TRY_CAST\"\n",
    "        else:\n",
    "            date_expr = f\"DATE({quote_ident(volume_info['date_col'])})\"\n",
    "            num_cast = \"CAST\"\n",
    "\n",
    "        # Build query with all columns including new ring columns\n",
    "        select_cols = [\n",
    "            f\"UPPER({quote_ident(volume_info['ticker_col'])}) AS ticker\",\n",
    "            f\"{date_expr} AS date\",\n",
    "            f\"{num_cast}({quote_ident(volume_info['lit_buy_col'])} AS DOUBLE) AS lit_buy\",\n",
    "            f\"{num_cast}({quote_ident(volume_info['lit_sell_col'])} AS DOUBLE) AS lit_sell\",\n",
    "            f\"{num_cast}({quote_ident(volume_info['short_buy_col'])} AS DOUBLE) AS short_buy\",\n",
    "            f\"{num_cast}({quote_ident(volume_info['short_sell_col'])} AS DOUBLE) AS short_sell\",\n",
    "        ]\n",
    "        # Add optional ring columns if available\n",
    "        if volume_info.get('otc_vol_col'):\n",
    "            select_cols.append(f\"{num_cast}({quote_ident(volume_info['otc_vol_col'])} AS DOUBLE) AS otc_volume\")\n",
    "        if volume_info.get('lit_total_col'):\n",
    "            select_cols.append(f\"{num_cast}({quote_ident(volume_info['lit_total_col'])} AS DOUBLE) AS lit_total\")\n",
    "        # Add lit buy/sell for Ring 2 coloring (lit buy ratio)\n",
    "        select_cols.append(f\"{num_cast}({quote_ident(volume_info['lit_buy_col'])} AS DOUBLE) AS lit_buy_volume\")\n",
    "        select_cols.append(f\"{num_cast}({quote_ident(volume_info['lit_sell_col'])} AS DOUBLE) AS lit_sell_volume\")\n",
    "        if volume_info.get('finra_buy_col'):\n",
    "            select_cols.append(f\"{num_cast}({quote_ident(volume_info['finra_buy_col'])} AS DOUBLE) AS finra_buy\")\n",
    "\n",
    "        # Add short_ratio for Ring 3 coloring\n",
    "        if volume_info.get('short_ratio_col'):\n",
    "            select_cols.append(f\"{num_cast}({quote_ident(volume_info['short_ratio_col'])} AS DOUBLE) AS short_ratio\")\n",
    "        # Add short_buy_sell_ratio for Ring 3 coloring (FINRA short sale buy/sell ratio)\n",
    "        if volume_info.get('short_buy_sell_ratio_col'):\n",
    "            select_cols.append(f\"{num_cast}({quote_ident(volume_info['short_buy_sell_ratio_col'])} AS DOUBLE) AS short_buy_sell_ratio\")\n",
    "\n",
    "        query = (\n",
    "            f\"SELECT {', '.join(select_cols)} \"\n",
    "            f\"FROM {quote_ident(volume_info['table'])} \"\n",
    "            f\"WHERE UPPER({quote_ident(volume_info['ticker_col'])}) IN ({placeholders})\"\n",
    "        )\n",
    "\n",
    "        if DB_TYPE == 'duckdb':\n",
    "            df_vol_raw = conn.execute(query, ticker_list).df()\n",
    "        else:\n",
    "            df_vol_raw = pd.read_sql_query(query, conn, params=ticker_list)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    df_vol_raw['date'] = pd.to_datetime(df_vol_raw['date'], errors='coerce').dt.date\n",
    "    print('Volume table rows loaded:', len(df_vol_raw))\n",
    "    print('Volume table date range:', df_vol_raw['date'].min(), '->', df_vol_raw['date'].max())\n",
    "    df_vol_raw = df_vol_raw[df_vol_raw['date'].isin(window_dates)]\n",
    "    print('Volume window date range:', df_vol_raw['date'].min(), '->', df_vol_raw['date'].max(), 'rows:', len(df_vol_raw))\n",
    "    df_vol_raw['ticker'] = df_vol_raw['ticker'].str.upper()\n",
    "    for col in ['lit_buy', 'lit_sell', 'short_buy', 'short_sell']:\n",
    "        df_vol_raw[col] = pd.to_numeric(df_vol_raw[col], errors='coerce')\n",
    "    # Also convert new columns if present\n",
    "    for col in ['otc_volume', 'lit_total', 'finra_buy']:\n",
    "        if col in df_vol_raw.columns:\n",
    "            df_vol_raw[col] = pd.to_numeric(df_vol_raw[col], errors='coerce')\n",
    "    df_vol_raw = df_vol_raw.dropna(subset=['date'])\n",
    "    \n",
    "    df_lit_daily = (\n",
    "        df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n",
    "        .agg({'lit_buy': 'sum', 'lit_sell': 'sum'})\n",
    "    )\n",
    "    df_lit_daily['lit_net'] = df_lit_daily['lit_buy'] - df_lit_daily['lit_sell']\n",
    "    df_lit_daily['lit_total'] = df_lit_daily['lit_buy'] + df_lit_daily['lit_sell']\n",
    "    df_lit_daily['lit_buy_ratio'] = df_lit_daily['lit_buy'] / df_lit_daily['lit_total'].replace(0, np.nan)\n",
    "    df_lit_daily['lit_buy_ratio'] = df_lit_daily['lit_buy_ratio'].fillna(0.5)\n",
    "    df_short_daily = (\n",
    "        df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n",
    "        .agg({'short_buy': 'sum', 'short_sell': 'sum'})\n",
    "    )\n",
    "    df_short_daily['short_net'] = df_short_daily['short_buy'] - df_short_daily['short_sell']\n",
    "\n",
    "    # Create dark/lit daily data for Ring 2\n",
    "    if 'otc_volume' in df_vol_raw.columns and 'lit_total' in df_vol_raw.columns:\n",
    "        df_dark_lit_daily = (\n",
    "            df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n",
    "            .agg({'otc_volume': 'sum', 'lit_total': 'sum'})\n",
    "        )\n",
    "        df_dark_lit_daily['total_volume'] = df_dark_lit_daily['otc_volume'] + df_dark_lit_daily['lit_total']\n",
    "        df_dark_lit_daily['dark_ratio'] = df_dark_lit_daily['otc_volume'] / df_dark_lit_daily['total_volume'].replace(0, np.nan)\n",
    "        df_dark_lit_daily['dark_ratio'] = df_dark_lit_daily['dark_ratio'].fillna(0.5)\n",
    "        print('Dark/Lit daily data created:', len(df_dark_lit_daily), 'rows')\n",
    "\n",
    "        # Query 20-day lookback for dark_ratio z-score normalization\n",
    "        DARK_LIT_LOOKBACK_DAYS = 20\n",
    "        latest_date = pd.Timestamp(max(window_dates))\n",
    "        lookback_start = latest_date - pd.Timedelta(days=DARK_LIT_LOOKBACK_DAYS + 10)  # buffer for weekends/holidays\n",
    "\n",
    "        try:\n",
    "            conn, db_type = connect_db(DB_PATH)\n",
    "            query_lookback = (\n",
    "                f\"SELECT UPPER({quote_ident(volume_info['ticker_col'])}) AS ticker, \"\n",
    "                f\"{date_expr} AS date, \"\n",
    "                f\"{num_cast}({quote_ident(volume_info['otc_vol_col'])} AS DOUBLE) AS otc_volume, \"\n",
    "                f\"{num_cast}({quote_ident(volume_info['lit_total_col'])} AS DOUBLE) AS lit_total \"\n",
    "                f\"FROM {quote_ident(volume_info['table'])} \"\n",
    "                f\"WHERE UPPER({quote_ident(volume_info['ticker_col'])}) IN ({placeholders}) \"\n",
    "                f\"AND {date_expr} >= '{lookback_start.strftime('%Y-%m-%d')}'\"\n",
    "            )\n",
    "            if db_type == 'duckdb':\n",
    "                df_dark_lit_lookback = conn.execute(query_lookback, ticker_list).df()\n",
    "            else:\n",
    "                df_dark_lit_lookback = pd.read_sql_query(query_lookback, conn, params=ticker_list)\n",
    "            conn.close()\n",
    "\n",
    "            df_dark_lit_lookback['total_volume'] = df_dark_lit_lookback['otc_volume'] + df_dark_lit_lookback['lit_total']\n",
    "            df_dark_lit_lookback['dark_ratio'] = df_dark_lit_lookback['otc_volume'] / df_dark_lit_lookback['total_volume'].replace(0, np.nan)\n",
    "            df_dark_lit_lookback['dark_ratio'] = df_dark_lit_lookback['dark_ratio'].fillna(0.5)\n",
    "\n",
    "            # Calculate per-ticker z-score stats from 20-day lookback\n",
    "            dark_ratio_stats = df_dark_lit_lookback.groupby('ticker')['dark_ratio'].agg(['mean', 'std']).reset_index()\n",
    "            dark_ratio_stats.columns = ['ticker', 'dark_ratio_mean', 'dark_ratio_std']\n",
    "            print(f'Dark/Lit 20-day lookback stats calculated for {len(dark_ratio_stats)} tickers')\n",
    "        except Exception as e:\n",
    "            print(f'Warning: Could not query 20-day lookback: {e}')\n",
    "            dark_ratio_stats = pd.DataFrame(columns=['ticker', 'dark_ratio_mean', 'dark_ratio_std'])\n",
    "    else:\n",
    "        df_dark_lit_daily = pd.DataFrame(columns=['ticker', 'date', 'otc_volume', 'lit_total', 'total_volume', 'dark_ratio'])\n",
    "        dark_ratio_stats = pd.DataFrame(columns=['ticker', 'dark_ratio_mean', 'dark_ratio_std'])\n",
    "        print('Dark/Lit columns not available - using empty dataframe')\n",
    "\n",
    "    # Create finra_buy daily data for Ring 3\n",
    "    if 'finra_buy' in df_vol_raw.columns:\n",
    "        df_finra_daily = (\n",
    "            df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n",
    "            .agg({'finra_buy': 'sum'})\n",
    "        )\n",
    "        print('Finra buy daily data created:', len(df_finra_daily), 'rows')\n",
    "\n",
    "        # Add short_ratio to df_finra_daily for Ring 3 coloring\n",
    "        if 'short_ratio' in df_vol_raw.columns:\n",
    "            df_short_ratio_daily = (\n",
    "                df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n",
    "                .agg({'short_ratio': 'mean'})\n",
    "            )\n",
    "            df_finra_daily = df_finra_daily.merge(df_short_ratio_daily, on=['ticker', 'date'], how='left')\n",
    "            df_finra_daily['short_ratio'] = df_finra_daily['short_ratio'].fillna(0.5)\n",
    "            print('Short ratio added to finra daily data')\n",
    "    else:\n",
    "        df_finra_daily = pd.DataFrame(columns=['ticker', 'date', 'finra_buy'])\n",
    "        print('Finra buy column not available - using empty dataframe')\n",
    "\n",
    "    vol_rows = []\n",
    "    for ticker in ticker_order:\n",
    "        df_t = df_vol_raw[df_vol_raw['ticker'] == ticker]\n",
    "        tail = tail_n(df_t, flow_days, END_DATE_RESOLVED)\n",
    "        if tail.empty:\n",
    "            vol_rows.append({\n",
    "                'ticker': ticker,\n",
    "                'lit_buy_sum': None,\n",
    "                'lit_sell_sum': None,\n",
    "                'short_buy_sum': None,\n",
    "                'short_sell_sum': None,\n",
    "                'finra_buy_sum': None,\n",
    "                'volume_samples': 0,\n",
    "            })\n",
    "            continue\n",
    "        vol_rows.append({\n",
    "            'ticker': ticker,\n",
    "            'lit_buy_sum': float(tail['lit_buy'].sum(skipna=True)),\n",
    "            'lit_sell_sum': float(tail['lit_sell'].sum(skipna=True)),\n",
    "            'short_buy_sum': float(tail['short_buy'].sum(skipna=True)),\n",
    "            'short_sell_sum': float(tail['short_sell'].sum(skipna=True)),\n",
    "            'finra_buy_sum': float(tail['finra_buy'].sum(skipna=True)) if 'finra_buy' in tail.columns else None,\n",
    "            'volume_samples': len(tail),\n",
    "        })\n",
    "\n",
    "    df_volume = pd.DataFrame(vol_rows)\n",
    "    total_samples = int(df_volume['volume_samples'].sum()) if not df_volume.empty else 0\n",
    "    lit_total = float(df_volume['lit_buy_sum'].fillna(0).sum() + df_volume['lit_sell_sum'].fillna(0).sum()) if not df_volume.empty else 0.0\n",
    "    short_total = float(df_volume['short_buy_sum'].fillna(0).sum() + df_volume['short_sell_sum'].fillna(0).sum()) if not df_volume.empty else 0.0\n",
    "    if total_samples == 0 or (lit_total == 0 and short_total == 0):\n",
    "        print('Volume data missing for selected period. Lit/short chords require non-zero buy/sell volume data.')\n",
    "        raise SystemExit\n",
    "else:\n",
    "    df_volume = pd.DataFrame(columns=[\n",
    "        'ticker', 'lit_buy_sum', 'lit_sell_sum', 'short_buy_sum', 'short_sell_sum', 'finra_buy_sum', 'volume_samples'\n",
    "    ])\n",
    "    df_dark_lit_daily = pd.DataFrame(columns=['ticker', 'date', 'otc_volume', 'lit_total', 'total_volume', 'dark_ratio'])\n",
    "    df_finra_daily = pd.DataFrame(columns=['ticker', 'date', 'finra_buy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "def build_edges_from_value(df, value_col, top_k_winners, top_k_losers):\n",
    "    df = df[['ticker', value_col]].dropna().copy()\n",
    "    winners = df[df[value_col] > 0].nlargest(top_k_winners, value_col)\n",
    "    losers = df[df[value_col] < 0].copy()\n",
    "    losers['supply'] = -losers[value_col]\n",
    "    losers = losers.nlargest(top_k_losers, 'supply')\n",
    "\n",
    "    edges = []\n",
    "    total_demand = winners[value_col].sum() if not winners.empty else 0.0\n",
    "    total_supply = losers['supply'].sum() if not losers.empty else 0.0\n",
    "\n",
    "    if winners.empty or losers.empty:\n",
    "        return pd.DataFrame(edges), winners, losers, total_demand, total_supply\n",
    "\n",
    "    if DISTRIBUTION_MODE not in {'equal', 'demand_weighted'}:\n",
    "        raise ValueError('DISTRIBUTION_MODE must be \"equal\" or \"demand_weighted\"')\n",
    "\n",
    "    if DISTRIBUTION_MODE == 'equal':\n",
    "        for _, loser in losers.iterrows():\n",
    "            flow_each = loser['supply'] / len(winners)\n",
    "            for _, winner in winners.iterrows():\n",
    "                if flow_each >= MIN_EDGE_FLOW:\n",
    "                    edges.append({\n",
    "                        'source': loser['ticker'],\n",
    "                        'dest': winner['ticker'],\n",
    "                        'flow': float(flow_each),\n",
    "                    })\n",
    "    else:\n",
    "        if total_demand > 0:\n",
    "            for _, loser in losers.iterrows():\n",
    "                for _, winner in winners.iterrows():\n",
    "                    flow = loser['supply'] * (winner[value_col] / total_demand)\n",
    "                    if flow >= MIN_EDGE_FLOW:\n",
    "                        edges.append({\n",
    "                            'source': loser['ticker'],\n",
    "                            'dest': winner['ticker'],\n",
    "                            'flow': float(flow),\n",
    "                        })\n",
    "\n",
    "    edges_df = pd.DataFrame(edges)\n",
    "    if not edges_df.empty:\n",
    "        edges_df = edges_df.sort_values('flow', ascending=False).reset_index(drop=True)\n",
    "    return edges_df, winners, losers, total_demand, total_supply\n",
    "\n",
    "\n",
    "def build_edges_from_positive_value(df, value_col, top_k_high, top_k_low):\n",
    "    \"\"\"Build edges for metrics that are always positive (like finra_buy_volume).\n",
    "    Flow goes from low-value tickers to high-value tickers.\"\"\"\n",
    "    df = df[['ticker', value_col]].dropna().copy()\n",
    "    df = df[df[value_col] > 0]  # Only positive values\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), 0.0, 0.0\n",
    "    \n",
    "    median_val = df[value_col].median()\n",
    "    high_tickers = df[df[value_col] >= median_val].nlargest(top_k_high, value_col)\n",
    "    low_tickers = df[df[value_col] < median_val].nsmallest(top_k_low, value_col)\n",
    "    \n",
    "    if high_tickers.empty or low_tickers.empty:\n",
    "        return pd.DataFrame(), high_tickers, low_tickers, 0.0, 0.0\n",
    "    \n",
    "    total_high = high_tickers[value_col].sum()\n",
    "    total_low = low_tickers[value_col].sum()\n",
    "    \n",
    "    edges = []\n",
    "    for _, low_row in low_tickers.iterrows():\n",
    "        for _, high_row in high_tickers.iterrows():\n",
    "            # Flow proportional to the difference\n",
    "            flow = (high_row[value_col] - low_row[value_col]) * (low_row[value_col] / total_low)\n",
    "            if flow > MIN_EDGE_FLOW:\n",
    "                edges.append({\n",
    "                    'source': low_row['ticker'],\n",
    "                    'dest': high_row['ticker'],\n",
    "                    'flow': float(flow),\n",
    "                })\n",
    "    \n",
    "    edges_df = pd.DataFrame(edges)\n",
    "    if not edges_df.empty:\n",
    "        edges_df = edges_df.sort_values('flow', ascending=False).reset_index(drop=True)\n",
    "    return edges_df, high_tickers, low_tickers, total_high, total_low\n",
    "\n",
    "\n",
    "# Accumulation flow (average level over window, centered by mean)\n",
    "if df_scores['A_end'].notna().any() and df_scores['A_start'].notna().any():\n",
    "    df_scores['delta'] = df_scores['A_end'] - df_scores['A_start']\n",
    "else:\n",
    "    df_scores['delta'] = np.nan\n",
    "\n",
    "df_accum_level = df_raw_full[df_raw_full['date'].isin(window_dates)][['ticker', 'date', 'accumulation_score']].copy()\n",
    "if 'accum_avg' in df_scores.columns:\n",
    "    df_scores = df_scores.drop(columns=['accum_avg', 'accum_centered'], errors='ignore')\n",
    "if not df_accum_level.empty:\n",
    "    df_accum_avg = df_accum_level.groupby('ticker')['accumulation_score'].mean()\n",
    "    df_scores = df_scores.merge(df_accum_avg.rename('accum_avg'), on='ticker', how='left')\n",
    "else:\n",
    "    df_scores['accum_avg'] = np.nan\n",
    "\n",
    "if df_scores['accum_avg'].notna().any():\n",
    "    mean_level = df_scores['accum_avg'].mean()\n",
    "    df_scores['accum_centered'] = df_scores['accum_avg'] - mean_level\n",
    "else:\n",
    "    df_scores['accum_centered'] = np.nan\n",
    "\n",
    "accum_for_flow = df_scores['accum_centered']\n",
    "\n",
    "df_scores['role'] = np.where(\n",
    "    accum_for_flow > 0,\n",
    "    'winner',\n",
    "    np.where(accum_for_flow < 0, 'loser', 'neutral')\n",
    ")\n",
    "\n",
    "df_scores_sorted = df_scores.sort_values(\n",
    "    by='accum_centered', key=lambda s: s.abs(), ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "accum_edges_df, accum_winners, accum_losers, accum_demand, accum_supply = build_edges_from_value(\n",
    "    df_scores, 'accum_centered', TOP_K_WINNERS, TOP_K_LOSERS\n",
    ")\n",
    "print(\"Accum winners/losers:\", len(accum_winners), len(accum_losers), \"edges:\", len(accum_edges_df))\n",
    "\n",
    "# Lit and Short flows (net buy - sell)\n",
    "if VOLUME_DATA_AVAILABLE and not df_volume.empty:\n",
    "    df_volume = df_volume.copy()\n",
    "    df_volume['lit_net'] = df_volume['lit_buy_sum'] - df_volume['lit_sell_sum']\n",
    "    df_volume['short_net'] = df_volume['short_buy_sum'] - df_volume['short_sell_sum']\n",
    "\n",
    "    lit_edges_df, lit_winners, lit_losers, lit_demand, lit_supply = build_edges_from_value(\n",
    "        df_volume, 'lit_net', TOP_K_WINNERS, TOP_K_LOSERS\n",
    "    )\n",
    "    short_edges_df, short_winners, short_losers, short_demand, short_supply = build_edges_from_value(\n",
    "        df_volume, 'short_net', TOP_K_WINNERS, TOP_K_LOSERS\n",
    "    )\n",
    "\n",
    "    print(\"Lit winners/losers:\", len(lit_winners), len(lit_losers), \"edges:\", len(lit_edges_df))\n",
    "    print(\"Short winners/losers:\", len(short_winners), len(short_losers), \"edges:\", len(short_edges_df))\n",
    "    \n",
    "    # Finra buy flow (positive values - flow from low to high)\n",
    "    if 'finra_buy_sum' in df_volume.columns and df_volume['finra_buy_sum'].notna().any():\n",
    "        finra_edges_df, finra_high, finra_low, finra_high_total, finra_low_total = build_edges_from_positive_value(\n",
    "            df_volume, 'finra_buy_sum', TOP_K_WINNERS, TOP_K_LOSERS\n",
    "        )\n",
    "        print(\"Finra buy high/low:\", len(finra_high), len(finra_low), \"edges:\", len(finra_edges_df))\n",
    "    else:\n",
    "        finra_edges_df = pd.DataFrame()\n",
    "        finra_high = pd.DataFrame()\n",
    "        finra_low = pd.DataFrame()\n",
    "        finra_high_total = finra_low_total = 0.0\n",
    "        print(\"Finra buy data not available\")\n",
    "else:\n",
    "    lit_edges_df = pd.DataFrame()\n",
    "    short_edges_df = pd.DataFrame()\n",
    "    finra_edges_df = pd.DataFrame()\n",
    "    lit_winners = pd.DataFrame()\n",
    "    short_winners = pd.DataFrame()\n",
    "    finra_high = pd.DataFrame()\n",
    "    lit_losers = pd.DataFrame()\n",
    "    short_losers = pd.DataFrame()\n",
    "    finra_low = pd.DataFrame()\n",
    "    lit_demand = lit_supply = 0.0\n",
    "    short_demand = short_supply = 0.0\n",
    "    finra_high_total = finra_low_total = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "EDGE_MULTIPLIER = 1000\n",
    "\n",
    "display(df_scores_sorted[['ticker', 'category', 'A_end', 'A_start', 'accum_avg', 'accum_centered', 'role']])\n",
    "\n",
    "if SHOW_ACCUM_FLOW:\n",
    "    if accum_edges_df.empty:\n",
    "        display(pd.DataFrame(columns=['source', 'dest', 'flow']))\n",
    "    else:\n",
    "        display(accum_edges_df[['source', 'dest', 'flow']].assign(flow=lambda d: d['flow'] * EDGE_MULTIPLIER))\n",
    "\n",
    "\n",
    "def chord_counts(edges_df):\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return pd.DataFrame(columns=['out', 'in'])\n",
    "    out = edges_df.groupby('source')['flow'].sum().rename('out')\n",
    "    inn = edges_df.groupby('dest')['flow'].sum().rename('in')\n",
    "    df = out.to_frame().join(inn, how='outer').fillna(0)\n",
    "    return (df * EDGE_MULTIPLIER).round(0).astype(int)\n",
    "\n",
    "\n",
    "if SHOW_ACCUM_FLOW:\n",
    "    print('Accum chord counts (out/in):')\n",
    "    display(chord_counts(accum_edges_df))\n",
    "if SHOW_LIT_FLOW:\n",
    "    print('Lit chord counts (out/in):')\n",
    "    display(chord_counts(lit_edges_df))\n",
    "if SHOW_SHORT_FLOW:\n",
    "    print('Short chord counts (out/in):')\n",
    "    display(chord_counts(short_edges_df))\n",
    "\n",
    "print('Volume availability:', VOLUME_DATA_AVAILABLE, 'rows:', len(df_volume))\n",
    "if VOLUME_DATA_AVAILABLE and not df_volume.empty:\n",
    "    display(df_volume[['ticker', 'lit_buy_sum', 'lit_sell_sum', 'short_buy_sum', 'short_sell_sum']])\n",
    "    if (df_volume['lit_buy_sum'].fillna(0).sum() == 0 and df_volume['lit_sell_sum'].fillna(0).sum() == 0):\n",
    "        print('Note: lit volumes sum to zero across selected period.')\n",
    "else:\n",
    "    print('Volume data not available for lit/short buy/sell flows.')\n",
    "\n",
    "summary_parts = [\n",
    "    f'END_DATE={END_DATE_RESOLVED}',\n",
    "    f'FLOW_PERIOD_DAYS={FLOW_PERIOD_DAYS}',\n",
    "    f'accum_supply={accum_supply:.2f}',\n",
    "    f'accum_demand={accum_demand:.2f}',\n",
    "]\n",
    "if SHOW_LIT_FLOW:\n",
    "    summary_parts.append(f'lit_supply={lit_supply:.2f}')\n",
    "    summary_parts.append(f'lit_demand={lit_demand:.2f}')\n",
    "if SHOW_SHORT_FLOW:\n",
    "    summary_parts.append(f'short_supply={short_supply:.2f}')\n",
    "    summary_parts.append(f'short_demand={short_demand:.2f}')\n",
    "\n",
    "summary_parts.append(f'accum_edges={len(accum_edges_df)}')\n",
    "summary_parts.append(f'lit_edges={len(lit_edges_df)}')\n",
    "summary_parts.append(f'short_edges={len(short_edges_df)}')\n",
    "\n",
    "print(' | '.join(summary_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection, PolyCollection\n",
    "from matplotlib.colors import to_rgba\n",
    "from matplotlib.patches import Wedge, PathPatch, Rectangle\n",
    "from matplotlib.path import Path\n",
    "\n",
    "BG_COLOR = '#0b0f1a'\n",
    "\n",
    "CATEGORY_LABELS = {\n",
    "    'GLOBAL_MACRO': 'GLOBAL MACRO',\n",
    "    'MAG8': 'MAG8',\n",
    "    'SECTOR_SUMMARY': 'Sector Summary',\n",
    "    'SECTOR_CORE': 'SECTORS',\n",
    "    'COMMODITIES': 'COMMODITIES',\n",
    "    'SPECULATIVE': 'SPECULATIVE',\n",
    "}\n",
    "CATEGORY_PALETTE = {\n",
    "    'GLOBAL_MACRO': \"#0059FF\",\n",
    "    'MAG8': \"#D38CFA\",\n",
    "    'SECTOR_SUMMARY': \"#00E4C5\",\n",
    "    'SECTOR_CORE': \"#FAAF00F2\",\n",
    "    'COMMODITIES': \"#FFFC2F\",\n",
    "    'SPECULATIVE': \"#FF7A45\",\n",
    "    'UNKNOWN': \"#8F8E8E\",\n",
    "}\n",
    "\n",
    "# Updated metric orders to include finra_buy for chords\n",
    "CHORD_METRIC_ORDER = ['accum', 'short', 'lit', 'finra_buy']\n",
    "BAND_ORDER = ['lit', 'accum', 'short', 'finra_buy']\n",
    "\n",
    "# Ring metrics (different from chord metrics)\n",
    "RING_METRIC_ORDER = ['accum', 'dark_lit', 'finra_buy']\n",
    "\n",
    "METRIC_LABELS = {\n",
    "    'accum': 'Accumulation',\n",
    "    'short': 'Daily Short',\n",
    "    'lit': 'Lit',\n",
    "    'finra_buy': 'Finra Buy',\n",
    "    'dark_lit': 'Dark/Lit Ratio',\n",
    "}\n",
    "\n",
    "# Chord colors\n",
    "METRIC_COLORS = {\n",
    "    'accum': {'sell': \"#8304B9\", 'buy': \"#26FF00\"},\n",
    "    'short': {'sell': \"#280042\", 'buy': \"#00AEFF\"},\n",
    "    'lit': {'sell': \"#FF0B0B\", 'buy': \"#1E7237\"},\n",
    "    'finra_buy': {'low': \"#63238D\", 'high': \"#00E4C5\"},  # Purple gradient for finra\n",
    "}\n",
    "\n",
    "# Ring colors (new encodings)\n",
    "RING_COLORS = {\n",
    "    'accum': {'negative': \"#8304B9\", 'positive': \"#26FF00\"},  # Purple (-) to Green (+)\n",
    "    'dark_lit': {'lit': \"#4488FF\", 'neutral': \"#888888\", 'dark': \"#FF4444\"},  # Blue-Gray-Red\n",
    "    'finra_buy': {},  # Uses category colors dynamically\n",
    "}\n",
    "\n",
    "\n",
    "def blend_color(c1, c2, t=0.5):\n",
    "    a = np.array(to_rgba(c1))\n",
    "    b = np.array(to_rgba(c2))\n",
    "    return a * (1 - t) + b * t\n",
    "\n",
    "\n",
    "def soften_color(color, amount, base=BG_COLOR):\n",
    "    return blend_color(color, base, max(0.0, min(1.0, amount)))\n",
    "\n",
    "\n",
    "def darken_color(color, factor):\n",
    "    \"\"\"Darken a color by factor (0=black, 1=original)\"\"\"\n",
    "    rgba = np.array(to_rgba(color))\n",
    "    rgba[:3] = rgba[:3] * max(0.0, min(1.0, factor))\n",
    "    return rgba\n",
    "\n",
    "\n",
    "def add_gradient_curve(ax, points, color_start, color_end, lw, alpha):\n",
    "    if len(points) < 2:\n",
    "        return\n",
    "    segments = np.stack([points[:-1], points[1:]], axis=1)\n",
    "    c0 = np.array(to_rgba(color_start))\n",
    "    c1 = np.array(to_rgba(color_end))\n",
    "    t = np.linspace(0, 1, len(segments))[:, None]\n",
    "    colors = c0 * (1 - t) + c1 * t\n",
    "    colors[:, 3] = colors[:, 3] * alpha\n",
    "    lc = LineCollection(segments, colors=colors, linewidths=lw, capstyle='round')\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "\n",
    "def arc_points(a0, a1, r, n=None):\n",
    "    count = n or CHORD_ARC_POINTS\n",
    "    angles = np.linspace(a0, a1, count)\n",
    "    return np.column_stack([r * np.cos(angles), r * np.sin(angles)])\n",
    "\n",
    "\n",
    "def bezier_curve(p0, p1, p2, n=None):\n",
    "    count = n or CHORD_CURVE_POINTS\n",
    "    t = np.linspace(0, 1, count)[:, None]\n",
    "    return (1 - t) ** 2 * p0 + 2 * (1 - t) * t * p1 + t ** 2 * p2\n",
    "\n",
    "\n",
    "def ribbon_patch(a0, a1, b0, b1, r, color_start, color_end, alpha):\n",
    "    arc1 = arc_points(a0, a1, r, n=16)\n",
    "    arc2 = arc_points(b0, b1, r, n=16)\n",
    "    curve1 = bezier_curve(arc1[-1], np.array([0.0, 0.0]), arc2[0], n=24)\n",
    "    curve2 = bezier_curve(arc2[-1], np.array([0.0, 0.0]), arc1[0], n=24)\n",
    "    poly = np.vstack([arc1, curve1, arc2, curve2])\n",
    "    codes = [Path.MOVETO] + [Path.LINETO] * (len(poly) - 1)\n",
    "    path = Path(poly, codes)\n",
    "    mid = blend_color(color_start, color_end, 0.5)\n",
    "    return PathPatch(path, facecolor=mid, edgecolor='none', alpha=alpha)\n",
    "\n",
    "\n",
    "def gradient_fill_collection(a0, a1, b0, b1, r, color_start, color_end, alpha, steps=None):\n",
    "    steps = steps or CHORD_GRADIENT_STEPS\n",
    "    arc1 = arc_points(a0, a1, r, n=4)\n",
    "    arc2 = arc_points(b0, b1, r, n=4)\n",
    "    p_a0, p_a1 = arc1[0], arc1[-1]\n",
    "    p_b0, p_b1 = arc2[0], arc2[-1]\n",
    "    left = bezier_curve(p_a0, np.array([0.0, 0.0]), p_b0, n=steps + 1)\n",
    "    right = bezier_curve(p_a1, np.array([0.0, 0.0]), p_b1, n=steps + 1)\n",
    "    polys = []\n",
    "    colors = []\n",
    "    for i in range(steps):\n",
    "        quad = np.vstack([left[i], left[i + 1], right[i + 1], right[i]])\n",
    "        t = (i + 0.5) / steps\n",
    "        color = blend_color(color_start, color_end, t)\n",
    "        color[3] = color[3] * alpha\n",
    "        polys.append(quad)\n",
    "        colors.append(color)\n",
    "    return PolyCollection(polys, facecolors=colors, edgecolors='none')\n",
    "\n",
    "\n",
    "def draw_ribbon(ax, a0, a1, b0, b1, r, color_start, color_end, fill_alpha, line_alpha, lw):\n",
    "    if USE_GRADIENT_FILL:\n",
    "        clip = ribbon_patch(a0, a1, b0, b1, r, color_start, color_end, alpha=1.0)\n",
    "        clip.set_facecolor('none')\n",
    "        clip.set_edgecolor('none')\n",
    "        ax.add_patch(clip)\n",
    "        fill = gradient_fill_collection(a0, a1, b0, b1, r, color_start, color_end, fill_alpha)\n",
    "        fill.set_clip_path(clip)\n",
    "        ax.add_collection(fill)\n",
    "    else:\n",
    "        patch = ribbon_patch(a0, a1, b0, b1, r, color_start, color_end, alpha=fill_alpha)\n",
    "        ax.add_patch(patch)\n",
    "    mid_a = (a0 + a1) / 2\n",
    "    mid_b = (b0 + b1) / 2\n",
    "    p0 = np.array([r * np.cos(mid_a), r * np.sin(mid_a)])\n",
    "    p2 = np.array([r * np.cos(mid_b), r * np.sin(mid_b)])\n",
    "    curve = bezier_curve(p0, np.array([0.0, 0.0]), p2)\n",
    "    add_gradient_curve(ax, curve, color_start, color_end, lw=lw, alpha=line_alpha)\n",
    "\n",
    "\n",
    "def make_time_bins(dates, bins):\n",
    "    if not dates:\n",
    "        return []\n",
    "    if bins is None or bins <= 0:\n",
    "        return [dates]\n",
    "    bins = min(bins, len(dates))\n",
    "    split = np.array_split(dates, bins)\n",
    "    return [list(s) for s in split if len(s)]\n",
    "\n",
    "\n",
    "def compute_metric_totals(edges_df):\n",
    "    totals = {}\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return totals\n",
    "    for row in edges_df.itertuples():\n",
    "        totals[row.source] = totals.get(row.source, 0.0) + row.flow\n",
    "        totals[row.dest] = totals.get(row.dest, 0.0) + row.flow\n",
    "    return totals\n",
    "\n",
    "\n",
    "def filter_edges(edges_df):\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return edges_df\n",
    "    df = edges_df.copy()\n",
    "    if MAX_EDGES_PER_METRIC and MAX_EDGES_PER_METRIC > 0:\n",
    "        df = df.nlargest(MAX_EDGES_PER_METRIC, 'flow')\n",
    "    return df\n",
    "\n",
    "\n",
    "def expand_edges(edges_df, splits):\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return edges_df\n",
    "    if not splits or splits <= 1:\n",
    "        return edges_df\n",
    "    max_flow = edges_df['flow'].max() if 'flow' in edges_df.columns and not edges_df.empty else 0.0\n",
    "    if max_flow <= 0:\n",
    "        return edges_df\n",
    "    rows = []\n",
    "    for row in edges_df.itertuples():\n",
    "        n = max(1, int(round(splits * (row.flow / max_flow))))\n",
    "        if EDGE_RIBBON_MAX and EDGE_RIBBON_MAX > 0:\n",
    "            n = min(n, EDGE_RIBBON_MAX)\n",
    "        flow = row.flow / n\n",
    "        for _ in range(n):\n",
    "            rows.append({'source': row.source, 'dest': row.dest, 'flow': flow})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def allocate_intervals(edges_df, band_map, metric_key, centered=True, center_offset=0.0):\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return []\n",
    "    \n",
    "    min_width_rad = RIBBON_MIN_WIDTH_RAD.get(metric_key, 0.003)\n",
    "    \n",
    "    out_counts = edges_df.groupby('source').size().to_dict()\n",
    "    in_counts = edges_df.groupby('dest').size().to_dict()\n",
    "    max_flow = edges_df['flow'].max() if not edges_df.empty else 1.0\n",
    "    \n",
    "    out_slot_info = {}\n",
    "    in_slot_info = {}\n",
    "    \n",
    "    for ticker in ticker_order:\n",
    "        spans = band_map.get(ticker, {}).get(metric_key)\n",
    "        if not spans:\n",
    "            continue\n",
    "        \n",
    "        out_range = spans['out']\n",
    "        out_arc = out_range[1] - out_range[0]\n",
    "        arc_center_out = (out_range[0] + out_range[1]) / 2 + center_offset\n",
    "        n_out = out_counts.get(ticker, 0)\n",
    "        if n_out > 0:\n",
    "            total_gap = RIBBON_GAP_RAD * (n_out - 1)\n",
    "            usable_out = max(0.0, out_arc - total_gap)\n",
    "            slot_width_out = usable_out / n_out\n",
    "            if slot_width_out < min_width_rad and n_out > 1:\n",
    "                slot_width_out = min_width_rad\n",
    "                total_gap = max(0.0, out_arc - n_out * slot_width_out)\n",
    "            \n",
    "            actual_gap = total_gap / max(1, n_out - 1) if n_out > 1 else 0.0\n",
    "            total_width = n_out * slot_width_out + (n_out - 1) * actual_gap\n",
    "            \n",
    "            if centered:\n",
    "                start_pos = arc_center_out - total_width / 2\n",
    "            else:\n",
    "                start_pos = out_range[0]\n",
    "            \n",
    "            out_slot_info[ticker] = {\n",
    "                'slot_width': slot_width_out,\n",
    "                'cursor': start_pos,\n",
    "                'gap': actual_gap,\n",
    "            }\n",
    "        \n",
    "        in_range = spans['in']\n",
    "        in_arc = in_range[1] - in_range[0]\n",
    "        arc_center_in = (in_range[0] + in_range[1]) / 2 + center_offset\n",
    "        n_in = in_counts.get(ticker, 0)\n",
    "        if n_in > 0:\n",
    "            total_gap = RIBBON_GAP_RAD * (n_in - 1)\n",
    "            usable_in = max(0.0, in_arc - total_gap)\n",
    "            slot_width_in = usable_in / n_in\n",
    "            if slot_width_in < min_width_rad and n_in > 1:\n",
    "                slot_width_in = min_width_rad\n",
    "                total_gap = max(0.0, in_arc - n_in * slot_width_in)\n",
    "            \n",
    "            actual_gap = total_gap / max(1, n_in - 1) if n_in > 1 else 0.0\n",
    "            total_width = n_in * slot_width_in + (n_in - 1) * actual_gap\n",
    "            \n",
    "            if centered:\n",
    "                start_pos = arc_center_in - total_width / 2\n",
    "            else:\n",
    "                start_pos = in_range[0]\n",
    "            \n",
    "            in_slot_info[ticker] = {\n",
    "                'slot_width': slot_width_in,\n",
    "                'cursor': start_pos,\n",
    "                'gap': actual_gap,\n",
    "            }\n",
    "    \n",
    "    intervals = []\n",
    "    for row in edges_df.sort_values('flow', ascending=False).itertuples():\n",
    "        src, dst, flow = row.source, row.dest, row.flow\n",
    "        if src not in out_slot_info or dst not in in_slot_info:\n",
    "            continue\n",
    "        \n",
    "        out_info = out_slot_info[src]\n",
    "        in_info = in_slot_info[dst]\n",
    "        \n",
    "        if RIBBON_WIDTH_SCALE_BY_FLOW:\n",
    "            flow_scale = (flow / max_flow) ** 0.5 if max_flow > 0 else 1.0\n",
    "            out_width = max(min_width_rad * 0.5, out_info['slot_width'] * flow_scale)\n",
    "            in_width = max(min_width_rad * 0.5, in_info['slot_width'] * flow_scale)\n",
    "        else:\n",
    "            out_width = out_info['slot_width']\n",
    "            in_width = in_info['slot_width']\n",
    "        \n",
    "        a0 = out_info['cursor']\n",
    "        a1 = a0 + out_width\n",
    "        b0 = in_info['cursor']\n",
    "        b1 = b0 + in_width\n",
    "        \n",
    "        out_info['cursor'] = a0 + out_info['slot_width'] + out_info['gap']\n",
    "        in_info['cursor'] = b0 + in_info['slot_width'] + in_info['gap']\n",
    "        \n",
    "        if a1 > a0 and b1 > b0:\n",
    "            intervals.append({'source': src, 'dest': dst, 'flow': flow, 'a0': a0, 'a1': a1, 'b0': b0, 'b1': b1})\n",
    "    \n",
    "    return intervals\n",
    "\n",
    "\n",
    "def metric_visible(metric_key):\n",
    "    if metric_key == \"accum\":\n",
    "        return SHOW_ACCUM_FLOW\n",
    "    if metric_key == \"short\":\n",
    "        return SHOW_SHORT_FLOW\n",
    "    if metric_key == \"lit\":\n",
    "        return SHOW_LIT_FLOW\n",
    "    if metric_key == \"finra_buy\":\n",
    "        return SHOW_FINRA_FLOW\n",
    "    return True\n",
    "\n",
    "\n",
    "# Prepare chord metric datasets\n",
    "accum_edges_plot = filter_edges(accum_edges_df)\n",
    "short_edges_plot = filter_edges(short_edges_df)\n",
    "lit_edges_plot = filter_edges(lit_edges_df)\n",
    "finra_edges_plot = filter_edges(finra_edges_df) if 'finra_edges_df' in dir() else pd.DataFrame()\n",
    "\n",
    "metric_edges = {\n",
    "    'accum': accum_edges_plot,\n",
    "    'short': short_edges_plot,\n",
    "    'lit': lit_edges_plot,\n",
    "    'finra_buy': finra_edges_plot,\n",
    "}\n",
    "\n",
    "metric_edges_draw = {m: expand_edges(metric_edges[m], EDGE_RIBBON_SPLITS) for m in CHORD_METRIC_ORDER}\n",
    "\n",
    "metric_nets = {\n",
    "    'accum': df_scores.set_index('ticker')['accum_centered'].to_dict() if 'accum_centered' in df_scores.columns else df_scores.set_index('ticker')['delta'].to_dict(),\n",
    "    'short': df_volume.set_index('ticker')['short_net'].to_dict() if 'short_net' in df_volume.columns else {},\n",
    "    'lit': df_volume.set_index('ticker')['lit_net'].to_dict() if 'lit_net' in df_volume.columns else {},\n",
    "    'finra_buy': df_volume.set_index('ticker')['finra_buy_sum'].to_dict() if 'finra_buy_sum' in df_volume.columns else {},\n",
    "}\n",
    "\n",
    "df_accum_level = df_raw_full[df_raw_full['date'].isin(window_dates)][['ticker', 'date', 'accumulation_score']].copy()\n",
    "if not df_accum_level.empty:\n",
    "    df_accum_level = df_accum_level.rename(columns={'accumulation_score': 'value'})\n",
    "\n",
    "# Ring metric daily data\n",
    "# Query 20-day lookback for z-score normalization (all ring metrics)\n",
    "RING_LOOKBACK_DAYS = 20\n",
    "latest_date = pd.Timestamp(max(window_dates)) if window_dates else pd.Timestamp.now()\n",
    "lookback_start = latest_date - pd.Timedelta(days=RING_LOOKBACK_DAYS + 10)\n",
    "\n",
    "# 20-day stats for accumulation score\n",
    "accum_stats = pd.DataFrame()\n",
    "try:\n",
    "    query_accum_lookback = f'''\n",
    "        SELECT UPPER({quote_ident(info['ticker_col'])}) AS ticker,\n",
    "               {date_expr} AS date,\n",
    "               CAST(accumulation_score AS DOUBLE) AS accumulation_score\n",
    "        FROM {quote_ident(info['table'])}\n",
    "        WHERE UPPER({quote_ident(info['ticker_col'])}) IN ({placeholders})\n",
    "        AND {date_expr} >= '{lookback_start.strftime('%Y-%m-%d')}'\n",
    "    '''\n",
    "    df_accum_lookback = pd.read_sql(query_accum_lookback, conn, params=ticker_list)\n",
    "    if not df_accum_lookback.empty:\n",
    "        accum_stats = df_accum_lookback.groupby('ticker')['accumulation_score'].agg(['mean', 'std']).reset_index()\n",
    "        accum_stats.columns = ['ticker', 'accum_mean', 'accum_std']\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load accum lookback: {e}\")\n",
    "\n",
    "# 20-day stats for dark_ratio (kept for backward compatibility)\n",
    "dark_lit_stats = pd.DataFrame()\n",
    "try:\n",
    "    query_dark_lookback = f'''\n",
    "        SELECT UPPER({quote_ident(volume_info['ticker_col'])}) AS ticker,\n",
    "               {date_expr} AS date,\n",
    "               {num_cast}({quote_ident(volume_info['otc_vol_col'])} AS DOUBLE) AS otc_volume,\n",
    "               {num_cast}({quote_ident(volume_info['lit_total_col'])} AS DOUBLE) AS lit_total\n",
    "        FROM {quote_ident(volume_info['table'])}\n",
    "        WHERE UPPER({quote_ident(volume_info['ticker_col'])}) IN ({placeholders})\n",
    "        AND {date_expr} >= '{lookback_start.strftime('%Y-%m-%d')}'\n",
    "    '''\n",
    "    df_dark_lookback = pd.read_sql(query_dark_lookback, conn, params=ticker_list)\n",
    "    if not df_dark_lookback.empty:\n",
    "        df_dark_lookback['total_volume'] = df_dark_lookback['otc_volume'] + df_dark_lookback['lit_total']\n",
    "        df_dark_lookback['dark_ratio'] = df_dark_lookback['otc_volume'] / df_dark_lookback['total_volume'].replace(0, np.nan)\n",
    "        df_dark_lookback['dark_ratio'] = df_dark_lookback['dark_ratio'].fillna(0.5)\n",
    "        dark_lit_stats = df_dark_lookback.groupby('ticker')['dark_ratio'].agg(['mean', 'std']).reset_index()\n",
    "        dark_lit_stats.columns = ['ticker', 'dark_ratio_mean', 'dark_ratio_std']\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load dark_lit lookback: {e}\")\n",
    "\n",
    "# 20-day stats for lit volume (Ring 2 sizing)\n",
    "lit_stats = pd.DataFrame()\n",
    "try:\n",
    "    query_lit_lookback = f'''\n",
    "        SELECT UPPER({quote_ident(volume_info['ticker_col'])}) AS ticker,\n",
    "               {date_expr} AS date,\n",
    "               {num_cast}({quote_ident(volume_info['lit_total_col'])} AS DOUBLE) AS lit_total\n",
    "        FROM {quote_ident(volume_info['table'])}\n",
    "        WHERE UPPER({quote_ident(volume_info['ticker_col'])}) IN ({placeholders})\n",
    "        AND {date_expr} >= '{lookback_start.strftime('%Y-%m-%d')}'\n",
    "    '''\n",
    "    df_lit_lookback = pd.read_sql(query_lit_lookback, conn, params=ticker_list)\n",
    "    if not df_lit_lookback.empty:\n",
    "        lit_stats = df_lit_lookback.groupby('ticker')['lit_total'].agg(['mean', 'std']).reset_index()\n",
    "        lit_stats.columns = ['ticker', 'lit_vol_mean', 'lit_vol_std']\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load lit lookback: {e}\")\n",
    "\n",
    "# 20-day stats for finra_buy and short_buy_sell_ratio\n",
    "finra_stats = pd.DataFrame()\n",
    "try:\n",
    "    finra_cols = [f\"{num_cast}({quote_ident(volume_info['finra_buy_col'])} AS DOUBLE) AS finra_buy\"]\n",
    "    if volume_info.get('short_buy_sell_ratio_col'):\n",
    "        finra_cols.append(f\"{num_cast}({quote_ident(volume_info['short_buy_sell_ratio_col'])} AS DOUBLE) AS short_buy_sell_ratio\")\n",
    "    query_finra_lookback = f'''\n",
    "        SELECT UPPER({quote_ident(volume_info['ticker_col'])}) AS ticker,\n",
    "               {date_expr} AS date,\n",
    "               {', '.join(finra_cols)}\n",
    "        FROM {quote_ident(volume_info['table'])}\n",
    "        WHERE UPPER({quote_ident(volume_info['ticker_col'])}) IN ({placeholders})\n",
    "        AND {date_expr} >= '{lookback_start.strftime('%Y-%m-%d')}'\n",
    "    '''\n",
    "    df_finra_lookback = pd.read_sql(query_finra_lookback, conn, params=ticker_list)\n",
    "    if not df_finra_lookback.empty:\n",
    "        agg_dict = {'finra_buy': ['mean', 'std']}\n",
    "        if 'short_buy_sell_ratio' in df_finra_lookback.columns:\n",
    "            agg_dict['short_buy_sell_ratio'] = ['mean', 'std']\n",
    "        finra_stats = df_finra_lookback.groupby('ticker').agg(agg_dict).reset_index()\n",
    "        finra_stats.columns = ['ticker'] + [f'{col}_{stat}' for col, stat in [('finra_buy', 'mean'), ('finra_buy', 'std')] + ([('sbsr', 'mean'), ('sbsr', 'std')] if 'short_buy_sell_ratio' in df_finra_lookback.columns else [])]\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load finra lookback: {e}\")\n",
    "\n",
    "# Add short_buy_sell_ratio to df_finra_daily if available\n",
    "if 'df_finra_daily' in dir() and 'df_vol_raw' in dir() and 'short_buy_sell_ratio' in df_vol_raw.columns:\n",
    "    df_sbsr = df_vol_raw.groupby(['ticker', 'date'])['short_buy_sell_ratio'].mean().reset_index()\n",
    "    df_finra_daily = df_finra_daily.merge(df_sbsr, on=['ticker', 'date'], how='left')\n",
    "\n",
    "ring_metric_daily = {\n",
    "    'accum': df_accum_level,\n",
    "    'accum_stats': accum_stats,\n",
    "    'dark_lit': df_dark_lit_daily if 'df_dark_lit_daily' in dir() else pd.DataFrame(),\n",
    "    'dark_lit_stats': dark_lit_stats,\n",
    "    'lit': df_lit_daily if 'df_lit_daily' in dir() else pd.DataFrame(),\n",
    "    'lit_stats': lit_stats if 'lit_stats' in dir() else pd.DataFrame(),\n",
    "    'finra_buy': df_finra_daily if 'df_finra_daily' in dir() else pd.DataFrame(),\n",
    "    'finra_stats': finra_stats,\n",
    "}\n",
    "\n",
    "# Build ticker layout\n",
    "grouped = {\n",
    "    'GLOBAL_MACRO': [t for t in ticker_order if ticker_category.get(t) == 'GLOBAL_MACRO'],\n",
    "    'MAG8': [t for t in ticker_order if ticker_category.get(t) == 'MAG8'],\n",
    "    'SECTOR_SUMMARY': [t for t in ticker_order if ticker_category.get(t) == 'SECTOR_SUMMARY'],\n",
    "    'SECTOR_CORE': [t for t in ticker_order if ticker_category.get(t) == 'SECTOR_CORE'],\n",
    "    'COMMODITIES': [t for t in ticker_order if ticker_category.get(t) == 'COMMODITIES'],\n",
    "    'SPECULATIVE': [t for t in ticker_order if ticker_category.get(t) == 'SPECULATIVE'],\n",
    "}\n",
    "\n",
    "metric_totals = {m: compute_metric_totals(metric_edges[m]) for m in CHORD_METRIC_ORDER}\n",
    "\n",
    "total_nodes = sum(len(v) for v in grouped.values())\n",
    "if total_nodes == 0:\n",
    "    print('No nodes to plot.')\n",
    "else:\n",
    "    gap = math.radians(CATEGORY_GAP_DEG)\n",
    "    total_gap = gap * len([g for g in grouped.values() if g])\n",
    "    usable = 2 * math.pi - total_gap\n",
    "    if usable <= 0:\n",
    "        usable = 2 * math.pi\n",
    "    step = usable / total_nodes\n",
    "    arc_span = step * 0.85\n",
    "\n",
    "    angles = {}\n",
    "    spans = {}\n",
    "    angle = 0.0\n",
    "    for cat in ['GLOBAL_MACRO', 'MAG8', 'SECTOR_SUMMARY', 'SECTOR_CORE', 'COMMODITIES', 'SPECULATIVE']:\n",
    "        if not grouped[cat]:\n",
    "            continue\n",
    "        angle += gap / 2\n",
    "        for t in grouped[cat]:\n",
    "            angles[t] = angle\n",
    "            spans[t] = (angle - arc_span / 2, angle + arc_span / 2)\n",
    "            angle += step\n",
    "        angle += gap / 2\n",
    "\n",
    "    # Metric bands per ticker (for chords)\n",
    "    band_map = {}\n",
    "    for t, (a0, a1) in spans.items():\n",
    "        max_span = (a1 - a0)\n",
    "        chord_span = min(max_span, max_span * CHORD_ARC_FRACTION)\n",
    "        chord_center = (a0 + a1) / 2\n",
    "        band_gap = chord_span * BAND_GAP_FRAC\n",
    "\n",
    "        if METRIC_BAND_MODE == 'proportional':\n",
    "            weights = {m: metric_totals[m].get(t, 0.0) for m in BAND_ORDER}\n",
    "            if sum(weights.values()) <= 0:\n",
    "                weights = {m: 1.0 for m in BAND_ORDER}\n",
    "        else:\n",
    "            weights = {m: 1.0 for m in BAND_ORDER}\n",
    "\n",
    "        total_w = sum(weights.values())\n",
    "        if total_w <= 0:\n",
    "            weights = {m: 1.0 for m in BAND_ORDER}\n",
    "            total_w = sum(weights.values())\n",
    "\n",
    "        available = chord_span - band_gap * 3  # 3 gaps for 4 metrics\n",
    "        if available <= 0:\n",
    "            band_gap = 0.0\n",
    "            available = chord_span\n",
    "\n",
    "        lengths = {m: max(0.0, available * (weights[m] / total_w)) for m in BAND_ORDER}\n",
    "        acc_len = lengths.get('accum', available / 4)\n",
    "        lit_len = lengths.get('lit', available / 4)\n",
    "        short_len = lengths.get('short', available / 4)\n",
    "        finra_len = lengths.get('finra_buy', available / 4)\n",
    "\n",
    "        # Layout: lit | accum | short | finra_buy\n",
    "        total_len = lit_len + acc_len + short_len + finra_len + 3 * band_gap\n",
    "        start = chord_center - total_len / 2\n",
    "        \n",
    "        lit_start = start\n",
    "        lit_end = lit_start + lit_len\n",
    "        acc_start = lit_end + band_gap\n",
    "        acc_end = acc_start + acc_len\n",
    "        short_start = acc_end + band_gap\n",
    "        short_end = short_start + short_len\n",
    "        finra_start = short_end + band_gap\n",
    "        finra_end = finra_start + finra_len\n",
    "\n",
    "        def band_slices(start, end):\n",
    "            dir_gap = (end - start) * DIR_GAP_FRAC\n",
    "            dir_gap = min(dir_gap, (end - start) * 0.4)\n",
    "            half = max(0.0, (end - start - dir_gap) / 2)\n",
    "            return {\n",
    "                'band': (start, end),\n",
    "                'out': (start, start + half),\n",
    "                'in': (start + half + dir_gap, end),\n",
    "            }\n",
    "\n",
    "        band_map[t] = {\n",
    "            'lit': band_slices(lit_start, lit_end),\n",
    "            'accum': band_slices(acc_start, acc_end),\n",
    "            'short': band_slices(short_start, short_end),\n",
    "            'finra_buy': band_slices(finra_start, finra_end),\n",
    "        }\n",
    "\n",
    "    # Prepare intervals per metric with per-metric center offset\n",
    "    metric_intervals = {}\n",
    "    for m in CHORD_METRIC_ORDER:\n",
    "        if not metric_visible(m):\n",
    "            metric_intervals[m] = []\n",
    "            continue\n",
    "        offset = RIBBON_CENTER_OFFSET.get(m, 0.0)\n",
    "        metric_intervals[m] = allocate_intervals(\n",
    "            metric_edges_draw[m], band_map, m, \n",
    "            centered=RIBBON_CENTERED, \n",
    "            center_offset=offset\n",
    "        )\n",
    "    \n",
    "    for m in CHORD_METRIC_ORDER:\n",
    "        if metric_visible(m):\n",
    "            offset = RIBBON_CENTER_OFFSET.get(m, 0.0)\n",
    "            min_w = RIBBON_MIN_WIDTH_RAD.get(m, 0.003)\n",
    "            print(f\"{m}: {len(metric_intervals[m])} ribbons (centered={RIBBON_CENTERED}, offset={offset:.3f}, min_width={min_w:.4f})\")\n",
    "\n",
    "    # Time bins for outer ring\n",
    "    window_dates_sorted = sorted(window_dates)\n",
    "    time_bins = make_time_bins(window_dates_sorted, TIME_SLICE_BINS)\n",
    "    time_bins = list(reversed(time_bins))\n",
    "\n",
    "    # Pre-compute normalization for ring rendering\n",
    "    ring_max_mag = {}\n",
    "    ring_min_val = {}\n",
    "    for m in RING_METRIC_ORDER:\n",
    "        df_m = ring_metric_daily.get(m, pd.DataFrame())\n",
    "        if df_m is None or df_m.empty:\n",
    "            ring_max_mag[m] = 1.0\n",
    "            ring_min_val[m] = 0.0\n",
    "            continue\n",
    "        if m == 'accum' and 'value' in df_m.columns and df_m['value'].notna().any():\n",
    "            ring_max_mag[m] = float(df_m['value'].abs().max())\n",
    "            ring_min_val[m] = 0.0  # For magnitude-based thickness\n",
    "        elif m == 'dark_lit' and 'total_volume' in df_m.columns:\n",
    "            ring_max_mag[m] = float(df_m['total_volume'].max()) if df_m['total_volume'].notna().any() else 1.0\n",
    "            ring_min_val[m] = 0.0\n",
    "        elif m == 'finra_buy' and 'finra_buy' in df_m.columns:\n",
    "            vals = df_m['finra_buy'].dropna()\n",
    "            if len(vals) > 0:\n",
    "                ring_max_mag[m] = float(np.log10(vals.max() + 1)) if vals.max() > 0 else 1.0\n",
    "                ring_min_val[m] = float(np.log10(vals.min() + 1)) if vals.min() > 0 else 0.0\n",
    "            else:\n",
    "                ring_max_mag[m] = 1.0\n",
    "                ring_min_val[m] = 0.0\n",
    "        else:\n",
    "            ring_max_mag[m] = 1.0\n",
    "            ring_min_val[m] = 0.0\n",
    "\n",
    "    # Build bin value lookup for rings\n",
    "    ring_bin_data = {m: {} for m in RING_METRIC_ORDER}\n",
    "    for m in RING_METRIC_ORDER:\n",
    "        df_m = ring_metric_daily.get(m, pd.DataFrame())\n",
    "        if df_m is None or df_m.empty:\n",
    "            for t in ticker_order:\n",
    "                ring_bin_data[m][t] = [{'value': 0.0, 'extra': 0.0} for _ in time_bins]\n",
    "            continue\n",
    "        for t in ticker_order:\n",
    "            data_by_bin = []\n",
    "            for bin_dates in time_bins:\n",
    "                mask = (df_m['ticker'] == t) & (df_m['date'].isin(bin_dates))\n",
    "                if m == 'accum' and 'value' in df_m.columns:\n",
    "                    val = float(df_m.loc[mask, 'value'].mean()) if mask.any() else 0.0\n",
    "                    data_by_bin.append({'value': val, 'extra': 0.0})\n",
    "                elif m == 'dark_lit':\n",
    "                    # Ring 2 now shows Lit data: sizing=lit volume z-score, coloring=lit buy ratio\n",
    "                    df_lit = ring_metric_daily.get('lit', pd.DataFrame())\n",
    "                    if not df_lit.empty:\n",
    "                        lit_mask = (df_lit['ticker'] == t) & (df_lit['date'].isin(bin_dates))\n",
    "                        if lit_mask.any():\n",
    "                            lit_vol = float(df_lit.loc[lit_mask, 'lit_total'].sum())\n",
    "                            lit_buy_ratio = float(df_lit.loc[lit_mask, 'lit_buy_ratio'].mean())\n",
    "                        else:\n",
    "                            lit_vol = 0.0\n",
    "                            lit_buy_ratio = 0.5\n",
    "                    else:\n",
    "                        lit_vol = 0.0\n",
    "                        lit_buy_ratio = 0.5\n",
    "                    data_by_bin.append({'value': lit_vol, 'extra': lit_buy_ratio})\n",
    "                elif m == 'finra_buy' and 'finra_buy' in df_m.columns:\n",
    "                    val = float(df_m.loc[mask, 'finra_buy'].sum()) if mask.any() else 0.0\n",
    "                    # Pass short_buy_sell_ratio as extra for coloring (FINRA short sale buy/sell ratio)\n",
    "                    if 'short_buy_sell_ratio' in df_m.columns and mask.any():\n",
    "                        sbsr_val = float(df_m.loc[mask, 'short_buy_sell_ratio'].mean())\n",
    "                    else:\n",
    "                        sbsr_val = 1.0  # Neutral value (1.0 = equal buy/sell)\n",
    "                    data_by_bin.append({'value': val, 'extra': sbsr_val})\n",
    "                else:\n",
    "                    data_by_bin.append({'value': 0.0, 'extra': 0.0})\n",
    "            ring_bin_data[m][t] = data_by_bin\n",
    "\n",
    "    # Start plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 12), subplot_kw={'aspect': 'equal'})\n",
    "    fig.patch.set_facecolor(BG_COLOR)\n",
    "    ax.set_facecolor(BG_COLOR)\n",
    "    ax.axis('off')\n",
    "\n",
    "    theta = np.linspace(0, 2 * math.pi, 400)\n",
    "    ax.plot(np.cos(theta), np.sin(theta), color='#39424e', lw=1.0, alpha=0.6)\n",
    "\n",
    "    # Ring outer edge (ticker arc removed - Ring 3 shows category colors)\n",
    "    ticker_outer = 1.02\n",
    "\n",
    "    # Outer rings (NEW ENCODING: accum, dark_lit, finra_buy)\n",
    "    if SHOW_VOLUME_RING:\n",
    "        track_span = RING_BASE_THICKNESS + RING_THICKNESS_SCALE + RING_GAP\n",
    "        for idx, m in enumerate(RING_METRIC_ORDER):\n",
    "            inner_base = ticker_outer + 0.02 + idx * track_span\n",
    "            max_mag = ring_max_mag.get(m, 1.0)\n",
    "            \n",
    "            for t, (a0, a1) in spans.items():\n",
    "                bin_data = ring_bin_data[m].get(t, [])\n",
    "                if not bin_data:\n",
    "                    continue\n",
    "                n_bins = len(bin_data)\n",
    "                arc_len = a1 - a0\n",
    "                slice_gap = arc_len * 0.02 / max(1, n_bins)\n",
    "                slice_len = (arc_len - slice_gap * (n_bins - 1)) / n_bins if n_bins > 0 else arc_len\n",
    "                cursor = a0\n",
    "                for bd in bin_data:\n",
    "                    val = bd['value']\n",
    "                    extra = bd['extra']\n",
    "                    \n",
    "                    if m == 'accum':\n",
    "                        # Ring 1: Sizing = 20-day z-score deviation, Coloring = accumulation thresholds\n",
    "                        # Get 20-day stats for this ticker\n",
    "                        df_astats = ring_metric_daily.get('accum_stats', pd.DataFrame())\n",
    "                        if not df_astats.empty and t in df_astats['ticker'].values:\n",
    "                            stats = df_astats[df_astats['ticker'] == t].iloc[0]\n",
    "                            mean_val = stats['accum_mean']\n",
    "                            std_val = stats['accum_std']\n",
    "                            if std_val > 0:\n",
    "                                z = abs(val - mean_val) / std_val\n",
    "                                thickness = RING_BASE_THICKNESS + RING_THICKNESS_SCALE * min(z / 2.0, 1.0)\n",
    "                            else:\n",
    "                                thickness = RING_BASE_THICKNESS\n",
    "                        else:\n",
    "                            # Fallback: deviation from 50\n",
    "                            deviation = abs(val - 50)\n",
    "                            thickness = RING_BASE_THICKNESS + RING_THICKNESS_SCALE * (deviation / 50.0)\n",
    "\n",
    "                        # Coloring: <=30 = max purple, >=70 = max green, 30-70 = blend\n",
    "                        if val >= 70:\n",
    "                            color = RING_COLORS['accum']['positive']  # Max green\n",
    "                        elif val <= 30:\n",
    "                            color = RING_COLORS['accum']['negative']  # Max purple\n",
    "                        else:\n",
    "                            # Blend based on position between 30-70\n",
    "                            t_blend = (val - 30) / 40.0  # 0 at 30, 1 at 70\n",
    "                            color = blend_color(RING_COLORS['accum']['negative'], RING_COLORS['accum']['positive'], t_blend)\n",
    "                    \n",
    "                    elif m == 'dark_lit':\n",
    "                        # Ring 2: Sizing = Lit volume 20-day z-score, Coloring = Lit buy ratio\n",
    "                        lit_vol = val  # lit_total from data\n",
    "                        lit_buy_ratio = extra  # lit_buy_ratio from data\n",
    "\n",
    "                        # Get 20-day lit volume stats for this ticker\n",
    "                        df_lstats = ring_metric_daily.get('lit_stats', pd.DataFrame())\n",
    "                        if not df_lstats.empty and t in df_lstats['ticker'].values:\n",
    "                            stats = df_lstats[df_lstats['ticker'] == t].iloc[0]\n",
    "                            mean_vol = stats['lit_vol_mean']\n",
    "                            std_vol = stats['lit_vol_std']\n",
    "                            if std_vol > 0 and lit_vol > 0:\n",
    "                                z = (lit_vol - mean_vol) / std_vol\n",
    "                                # Sizing: abs(z-score) deviation\n",
    "                                thickness = RING_BASE_THICKNESS + RING_THICKNESS_SCALE * min(abs(z) / 2.0, 1.0)\n",
    "                            else:\n",
    "                                thickness = RING_BASE_THICKNESS\n",
    "                        else:\n",
    "                            # Fallback: use raw volume ratio\n",
    "                            thickness = RING_BASE_THICKNESS + RING_THICKNESS_SCALE * (lit_vol / max_mag if max_mag > 0 else 0.0)\n",
    "\n",
    "                        # Coloring: Lit buy ratio (0=all sells/red, 0.5=neutral/gray, 1=all buys/green)\n",
    "                        # Typical range is 0.4-0.6, so use tighter scaling\n",
    "                        normalized = (lit_buy_ratio - 0.4) / 0.2  # 0.4->0, 0.5->0.5, 0.6->1\n",
    "                        normalized = max(0.0, min(1.0, normalized))\n",
    "\n",
    "                        # Color: sell (red) to neutral (gray) to buy (green)\n",
    "                        if normalized < 0.5:\n",
    "                            color = blend_color('#FF6666', '#888888', normalized * 2)  # Red to gray\n",
    "                        else:\n",
    "                            color = blend_color('#888888', '#66FF66', (normalized - 0.5) * 2)  # Gray to green\n",
    "                    \n",
    "                    elif m == 'finra_buy':\n",
    "                        # Ring 3: Sizing = 20-day z-score deviation, Coloring = short_buy_sell_ratio\n",
    "                        # Get short_buy_sell_ratio from extra (passed from bin_data)\n",
    "                        short_buy_sell_ratio = extra if extra is not None else 1.0\n",
    "\n",
    "                        # Sizing: 20-day z-score deviation of finra_buy\n",
    "                        df_fstats = ring_metric_daily.get('finra_stats', pd.DataFrame())\n",
    "                        if not df_fstats.empty and t in df_fstats['ticker'].values:\n",
    "                            stats = df_fstats[df_fstats['ticker'] == t].iloc[0]\n",
    "                            mean_fb = stats.get('finra_buy_mean', 0)\n",
    "                            std_fb = stats.get('finra_buy_std', 0)\n",
    "                            if std_fb > 0 and val > 0:\n",
    "                                z = abs(val - mean_fb) / std_fb\n",
    "                                thickness = RING_BASE_THICKNESS + RING_THICKNESS_SCALE * min(z / 2.0, 1.0)\n",
    "                            else:\n",
    "                                thickness = RING_BASE_THICKNESS\n",
    "                        else:\n",
    "                            # Fallback to percentile\n",
    "                            df_fb = ring_metric_daily.get('finra_buy', pd.DataFrame())\n",
    "                            if not df_fb.empty and 'finra_buy' in df_fb.columns and val > 0:\n",
    "                                all_vals = df_fb['finra_buy'].dropna()\n",
    "                                if len(all_vals) > 1:\n",
    "                                    pct = (all_vals < val).sum() / len(all_vals)\n",
    "                                else:\n",
    "                                    pct = 0.5\n",
    "                                thickness = RING_BASE_THICKNESS + RING_THICKNESS_SCALE * pct\n",
    "                            else:\n",
    "                                thickness = RING_BASE_THICKNESS\n",
    "\n",
    "                        # Coloring: short_buy_sell_ratio (buy/sell ratio from FINRA short sales)\n",
    "                        # Ratio > 1 = more buying, < 1 = more selling\n",
    "                        # Typical range: 0.5 to 2.0, center around 1.0\n",
    "                        if short_buy_sell_ratio > 0:\n",
    "                            # Log scale to handle wide range\n",
    "                            log_ratio = np.log(short_buy_sell_ratio)\n",
    "                            # Normalize: -0.5 to +0.5 -> 0 to 1\n",
    "                            normalized = 0.5 + (log_ratio / 1.0)  # +/-0.5 log units = 0.6x to 1.6x\n",
    "                            normalized = max(0.0, min(1.0, normalized))\n",
    "                        else:\n",
    "                            normalized = 0.5\n",
    "\n",
    "                        # Color: low ratio (selling) = red, high ratio (buying) = green\n",
    "                        cat_color = CATEGORY_PALETTE.get(ticker_category.get(t, 'UNKNOWN'), '#A0A0A0')\n",
    "                        if normalized < 0.5:\n",
    "                            # More selling = darker/reddish tint\n",
    "                            brightness = 0.3 + 0.4 * (normalized * 2)\n",
    "                            color = darken_color(cat_color, brightness)\n",
    "                        else:\n",
    "                            # More buying = brighter\n",
    "                            brightness = 0.5 + 0.5 * ((normalized - 0.5) * 2)\n",
    "                            color = darken_color(cat_color, brightness)\n",
    "                    \n",
    "                    else:\n",
    "                        thickness = RING_BASE_THICKNESS\n",
    "                        color = '#666666'\n",
    "                    \n",
    "                    wedge = Wedge(\n",
    "                        (0, 0), inner_base + thickness, math.degrees(cursor), math.degrees(cursor + slice_len),\n",
    "                        width=thickness,\n",
    "                        facecolor=color, edgecolor='none', alpha=0.85,\n",
    "                    )\n",
    "                    ax.add_patch(wedge)\n",
    "                    cursor += slice_len + slice_gap\n",
    "\n",
    "    # Draw chords per metric\n",
    "    for m in CHORD_METRIC_ORDER:\n",
    "        if not metric_visible(m):\n",
    "            continue\n",
    "        intervals = metric_intervals[m]\n",
    "        if not intervals:\n",
    "            continue\n",
    "        \n",
    "        if m == 'finra_buy':\n",
    "            raw_start = METRIC_COLORS[m]['low']\n",
    "            raw_end = METRIC_COLORS[m]['high']\n",
    "        else:\n",
    "            raw_start = METRIC_COLORS[m]['sell']\n",
    "            raw_end = METRIC_COLORS[m]['buy']\n",
    "        color_start = soften_color(raw_start, CHORD_COLOR_SOFTEN)\n",
    "        color_end = soften_color(raw_end, CHORD_COLOR_SOFTEN)\n",
    "        max_flow = metric_edges[m]['flow'].max() if metric_edges[m] is not None and not metric_edges[m].empty else 1.0\n",
    "        for edge in intervals:\n",
    "            flow = edge['flow']\n",
    "            lw = 0.6 + 2.2 * ((flow / max_flow) ** 0.6) if max_flow > 0 else 1.0\n",
    "            draw_ribbon(ax, edge['a0'], edge['a1'], edge['b0'], edge['b1'], CHORD_RADIUS, color_start, color_end, fill_alpha=CHORD_FILL_ALPHA, line_alpha=CHORD_LINE_ALPHA, lw=lw)\n",
    "\n",
    "    # Ticker labels\n",
    "    for t, ang in angles.items():\n",
    "        x, y = math.cos(ang), math.sin(ang)\n",
    "        r = CHORD_RADIUS + (ticker_outer - CHORD_RADIUS) * 0.4\n",
    "        rot = math.degrees(ang)\n",
    "        if math.pi / 2 < ang < 3 * math.pi / 2:\n",
    "            rot += 180\n",
    "        ax.text(r * x, r * y, t, color='#FFFFFF', fontsize=9, fontweight='bold',\n",
    "                ha='center', va='center', rotation=rot, rotation_mode='anchor')\n",
    "\n",
    "    # Title and settings\n",
    "    date_range = f\"{window_dates_sorted[0]} -> {window_dates_sorted[-1]}\" if window_dates_sorted else 'n/a'\n",
    "    fig.text(0.5, 0.97, 'Sector Money-Flow Chord Summary', ha='center', va='top', color='white', fontsize=14, fontweight='bold')\n",
    "    fig.text(0.5, 0.945, f'Date Range: {date_range}', ha='center', va='top', color='#C9D1D9', fontsize=10)\n",
    "    \n",
    "    # Upper-left Legend (Chords only)\n",
    "    leg = fig.add_axes([0.08, 0.75, 0.30, 0.18])\n",
    "    leg.axis('off')\n",
    "    leg.set_facecolor('none')\n",
    "    leg.set_xlim(0, 1)\n",
    "    leg.set_ylim(0, 1)\n",
    "\n",
    "    y = 0.95\n",
    "    leg.text(0.0, y, 'Chords', color='white', fontsize=10, fontweight='bold', va='top')\n",
    "    y -= 0.15\n",
    "\n",
    "    chord_items = [\n",
    "        ('accum', 'Accumulation', METRIC_COLORS['accum']['sell'], METRIC_COLORS['accum']['buy']),\n",
    "        ('short', 'Short', METRIC_COLORS['short']['sell'], METRIC_COLORS['short']['buy']),\n",
    "        ('lit', 'Lit', METRIC_COLORS['lit']['sell'], METRIC_COLORS['lit']['buy']),\n",
    "        ('finra_buy', 'Finra Buy', METRIC_COLORS['finra_buy']['low'], METRIC_COLORS['finra_buy']['high']),\n",
    "    ]\n",
    "    for key, label, c_start, c_end in chord_items:\n",
    "        if not metric_visible(key):\n",
    "            continue\n",
    "        xs = np.linspace(0.0, 0.20, 30)\n",
    "        points = np.column_stack([xs, np.full_like(xs, y)])\n",
    "        segments = np.stack([points[:-1], points[1:]], axis=1)\n",
    "        c0 = np.array(to_rgba(c_start))\n",
    "        c1 = np.array(to_rgba(c_end))\n",
    "        t_arr = np.linspace(0, 1, len(segments))[:, None]\n",
    "        colors = c0 * (1 - t_arr) + c1 * t_arr\n",
    "        leg.add_collection(LineCollection(segments, colors=colors, linewidths=4))\n",
    "        leg.text(0.24, y, label, color='white', fontsize=8, va='center')\n",
    "        y -= 0.15\n",
    "\n",
    "    # Bottom-left Ring Legend (Sizing + Coloring sections)\n",
    "    ring_leg = fig.add_axes([0.05, 0.02, 0.42, 0.22])\n",
    "    ring_leg.axis('off')\n",
    "    ring_leg.set_facecolor('none')\n",
    "    ring_leg.set_xlim(0, 1)\n",
    "    ring_leg.set_ylim(0, 1)\n",
    "\n",
    "    y = 0.98\n",
    "    ring_leg.text(0.0, y, 'Ring Sizing', color='white', fontsize=9, fontweight='bold', va='top')\n",
    "    y -= 0.12\n",
    "\n",
    "    sizing_items = [\n",
    "        ('Ring 1', 'Deviation from 20-day mean'),\n",
    "        ('Ring 2', 'Lit volume z-score'),\n",
    "        ('Ring 3', 'Deviation from 20-day mean'),\n",
    "    ]\n",
    "    for ring, desc in sizing_items:\n",
    "        ring_leg.text(0.02, y, f'{ring}:', color='#888888', fontsize=7, va='center')\n",
    "        ring_leg.text(0.14, y, desc, color='#C9D1D9', fontsize=7, va='center')\n",
    "        y -= 0.09\n",
    "\n",
    "    y -= 0.04\n",
    "    ring_leg.text(0.0, y, 'Ring Coloring', color='white', fontsize=9, fontweight='bold', va='top')\n",
    "    y -= 0.12\n",
    "\n",
    "    coloring_items = [\n",
    "        ('Ring 1', '<=30 purple, >=70 green', RING_COLORS['accum']['negative'], RING_COLORS['accum']['positive']),\n",
    "        ('Ring 2', 'Lit buy ratio (sell->buy)', '#FF6666', '#66FF66'),\n",
    "        ('Ring 3', 'Short B/S ratio (sell->buy)', '#8B4040', '#40B040'),  # Dark red (selling) to bright green (buying)\n",
    "    ]\n",
    "    for ring, desc, c_start, c_end in coloring_items:\n",
    "        if c_start and c_end:\n",
    "            xs = np.linspace(0.0, 0.10, 30)\n",
    "            points = np.column_stack([xs, np.full_like(xs, y)])\n",
    "            segments = np.stack([points[:-1], points[1:]], axis=1)\n",
    "            c0 = np.array(to_rgba(c_start))\n",
    "            c1 = np.array(to_rgba(c_end))\n",
    "            t_arr = np.linspace(0, 1, len(segments))[:, None]\n",
    "            colors_arr = c0 * (1 - t_arr) + c1 * t_arr\n",
    "            ring_leg.add_collection(LineCollection(segments, colors=colors_arr, linewidths=4))\n",
    "            ring_leg.text(0.12, y, f'{ring}: {desc}', color='#C9D1D9', fontsize=7, va='center')\n",
    "        else:\n",
    "            ring_leg.text(0.02, y, f'{ring}: {desc}', color='#C9D1D9', fontsize=7, va='center')\n",
    "        y -= 0.09\n",
    "\n",
    "    # Upper-right Category Legend (dynamic, gradient squares)\n",
    "    present_categories = [cat for cat in ['GLOBAL_MACRO', 'MAG8', 'SECTOR_SUMMARY', 'SECTOR_CORE', 'COMMODITIES', 'SPECULATIVE'] \n",
    "                         if grouped.get(cat)]\n",
    "    \n",
    "    if present_categories:\n",
    "        cat_leg = fig.add_axes([0.78, 0.78, 0.18, 0.18])\n",
    "        cat_leg.axis('off')\n",
    "        cat_leg.set_facecolor('none')\n",
    "        cat_leg.set_xlim(0, 1)\n",
    "        cat_leg.set_ylim(0, 1)\n",
    "        \n",
    "        y = 0.95\n",
    "        cat_leg.text(0.0, y, 'Categories', color='white', fontsize=10, fontweight='bold', va='top')\n",
    "        y -= 0.12\n",
    "        \n",
    "        for cat in present_categories:\n",
    "            base_color = CATEGORY_PALETTE.get(cat, '#A0A0A0')\n",
    "            dark_color = darken_color(base_color, 0.2)\n",
    "            \n",
    "            # Draw gradient square\n",
    "            for i in range(10):\n",
    "                t = i / 9.0\n",
    "                color = blend_color(dark_color, base_color, t)\n",
    "                rect = Rectangle((i * 0.008, y - 0.04), 0.008, 0.08, facecolor=color, edgecolor='none')\n",
    "                cat_leg.add_patch(rect)\n",
    "            \n",
    "            cat_leg.text(0.12, y, CATEGORY_LABELS.get(cat, cat), color='white', fontsize=8, va='center')\n",
    "            y -= 0.16\n",
    "\n",
    "    # Top inflow/outflow table\n",
    "    def top_tickers(net_map, positive=True, k=3):\n",
    "        if not net_map:\n",
    "            return 'n/a'\n",
    "        items = [(k, v) for k, v in net_map.items() if v is not None and np.isfinite(v)]\n",
    "        if positive:\n",
    "            items = sorted([x for x in items if x[1] > 0], key=lambda x: x[1], reverse=True)\n",
    "        else:\n",
    "            items = sorted([x for x in items if x[1] < 0], key=lambda x: x[1])\n",
    "        return ', '.join([k for k, _ in items[:k]]) or 'n/a'\n",
    "\n",
    "    table_rows = [\n",
    "        ('Accumulation Score', top_tickers(metric_nets.get('accum', {}), True), top_tickers(metric_nets.get('accum', {}), False)),\n",
    "        ('Daily Short Buy/Sell', top_tickers(metric_nets.get('short', {}), True), top_tickers(metric_nets.get('short', {}), False)),\n",
    "        ('Lit Buy/Sell Ratio', top_tickers(metric_nets.get('lit', {}), True), top_tickers(metric_nets.get('lit', {}), False)),\n",
    "    ]\n",
    "    col1, col2, col3 = 26, 24, 24\n",
    "    table_lines = [f\"{'Metric':<{col1}}{'Buy (Top)':<{col2}}{'Sell (Top)':<{col3}}\", '-' * (col1 + col2 + col3)]\n",
    "    for label, buy_str, sell_str in table_rows:\n",
    "        table_lines.append(f\"{label:<{col1}}{buy_str:<{col2}}{sell_str:<{col3}}\")\n",
    "    fig.text(0.50, 0.05, '\\n'.join(table_lines), ha='left', va='bottom', color='#C9D1D9', fontsize=9, fontfamily='monospace', linespacing=1.2)\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

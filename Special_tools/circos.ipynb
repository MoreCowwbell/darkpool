{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eeaa72d",
   "metadata": {},
   "source": [
    "# Cell 1\n",
    "# Circos-Style Money Flow Chord Diagram\n",
    "\n",
    "This notebook builds a self-contained Circos-style chord diagram from Accumulation Score time series in a local database.\n",
    "\n",
    "Constraints honored:\n",
    "- No project modules are imported or modified.\n",
    "- Database access is read-only (SELECT-only).\n",
    "- No files are written or mutated; all outputs are in-notebook only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c759775f",
   "metadata": {},
   "outputs": [],
   "source": "# Ticker groups (hardcoded)\nSECTOR_CORE_TICKERS = [\n    \"XLF\",\"KRE\",\"XLK\",\"SMH\",\"XLI\",\"XLY\",\"XLE\",\"XLV\",\"XLP\",\"XLU\"\n]\n\nSECTOR_SUMMARY_TICKERS = [\n    \"XLE\", \"XLF\", \"XLK\", \"XLY\", \"XLP\", \"XLV\", \"XLU\", \"XLI\", \"XLC\", \"XLB\", \"XLRE\", \"SPY\",\n]\n\nGLOBAL_MACRO_TICKERS = [\n    \"SPY\",\"QQQ\",\"TQQQ\",\"IWM\",\"VGK\",\"EWJ\",\"EFA\",\"EEM\",\"FXI\",\"UUP\",\"TLT\",\"GLD\",\"USO\",\"VIXY\"\n]\n\nCOMMODITIES_TICKERS = [\n    \"GLD\",\"SLV\",\"GDX\",\"USO\",\"UNG\",\"URA\"\n]\n\nMAG8_TICKERS = [\n    \"AAPL\",\"MSFT\",\"NVDA\",\"AMZN\",\"GOOGL\",\"META\",\"TSLA\",\"AVGO\"\n]\n\nENABLED_GROUPS = {\n    \"GLOBAL_MACRO\": False,\n    \"MAG8\": False,\n    \"SECTOR_SUMMARY\": True,\n    \"SECTOR_CORE\": False,\n    \"COMMODITIES\": False,\n}\n\n# Controls\nEND_DATE = None                         # End of window; None = auto-detect max date in DB\nFLOW_PERIOD_DAYS = 3                    # Trading-day window used for flows (start->end inside this window)\nTOP_K_WINNERS = 8                       # Max winners by demand (positive delta) to include\nTOP_K_LOSERS = 8                        # Max losers by supply (negative delta) to include\nMIN_EDGE_FLOW = 0.0                     # Drop edges smaller than this flow (pre-strand)\nDISTRIBUTION_MODE = \"demand_weighted\"   # \"equal\" or \"demand_weighted\" split from sources to winners\n\n# Chord + ring layout\nMETRIC_BAND_MODE = \"proportional\"       # \"equal\" = same band width per metric; \"proportional\" = band width by flow\nMAX_EDGES_PER_METRIC = 40               # cap edges per metric to keep plot readable\nEDGE_RIBBON_SPLITS = 25                 # split each edge into N thin ribbons (approx density)\nEDGE_RIBBON_MAX = 60                    # cap ribbons per edge (perf)\nCHORD_ARC_FRACTION = 1.0                # fraction of ticker arc reserved for chord endpoints\nCHORD_RADIUS = 0.78                     # inner radius for chord ribbons\nTIME_SLICE_BINS = 3                     # number of time slices per outer ring\nRING_BASE_THICKNESS = 0.012             # minimum ring thickness\nRING_THICKNESS_SCALE = 0.06             # added thickness scaled by magnitude\nRING_GAP = 0.01                         # gap between metric rings\nBAND_GAP_FRAC = 0.02                    # gap between metric bands within a ticker\nDIR_GAP_FRAC = 0.02                     # gap between outflow/inflow halves inside a band\nCATEGORY_GAP_DEG = 1                    # degrees; category block gap (0 = uniform spacing)\n\n# Fanned chord layout config\nRIBBON_MIN_WIDTH_RAD = {                # minimum ribbon arc width in radians (per-metric)\n    'accum': 0.005,\n    'short': 0.005,\n    'lit': 0.005,\n    'finra_buy': 0.005,\n}\nRIBBON_GAP_RAD = 0.002                  # gap between ribbons in radians\nRIBBON_WIDTH_SCALE_BY_FLOW = True       # if True, ribbon width scales with flow; if False, all equal\nRIBBON_CENTERED = True                  # if True, ribbons fan out from center; if False, from edge\nRIBBON_CENTER_OFFSET = {                # per-metric center offset in radians (positive = clockwise)\n    'accum': 0.0,                       # accumulation stays centered\n    'short': 0.5,                       # short offset (adjust as needed)\n    'lit': -0.5,                        # lit offset (adjust as needed)\n    'finra_buy': 0.0,                   # finra buy offset\n}\n\n# Render quality (performance vs fidelity)\nRENDER_MODE = \"balanced\"  # \"fast\", \"balanced\", \"quality\"\nCHORD_FILL_ALPHA = 0.55\nCHORD_LINE_ALPHA = 0.8\nCHORD_COLOR_SOFTEN = 0.25  # blend toward background to reduce saturation\nif RENDER_MODE == \"fast\":\n    USE_GRADIENT_FILL = False\n    CHORD_GRADIENT_STEPS = 8\n    CHORD_ARC_POINTS = 8\n    CHORD_CURVE_POINTS = 30\nelif RENDER_MODE == \"quality\":\n    USE_GRADIENT_FILL = True\n    CHORD_GRADIENT_STEPS = 32\n    CHORD_ARC_POINTS = 18\n    CHORD_CURVE_POINTS = 70\nelse:\n    USE_GRADIENT_FILL = True\n    CHORD_GRADIENT_STEPS = 18\n    CHORD_ARC_POINTS = 12\n    CHORD_CURVE_POINTS = 50\n\n# Layer toggles\nSHOW_ACCUM_FLOW = True\nSHOW_LIT_FLOW = True\nSHOW_SHORT_FLOW = True\nSHOW_FINRA_FLOW = True                  # 4th chord: finra buy volume flow\nSHOW_VOLUME_RING = True\nSHOW_TICKER_ARC = False                 # Disabled - Ring 3 now shows category colors"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b232f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB candidates (newest first):\n",
      "No database files found under ./data or ./darkpool_analysis/data.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def find_db_candidates():\n",
    "    root = Path('.').resolve()\n",
    "    search_dirs = [\n",
    "        root / 'data',\n",
    "        root / 'darkpool_analysis' / 'data',\n",
    "        root,\n",
    "    ]\n",
    "    patterns = ['*.duckdb', '*.db', '*.sqlite', '*.sqlite3']\n",
    "    candidates = []\n",
    "    for base in search_dirs:\n",
    "        if not base.exists():\n",
    "            continue\n",
    "        for pattern in patterns:\n",
    "            candidates.extend(list(base.rglob(pattern)))\n",
    "    # Unique + files only\n",
    "    unique = []\n",
    "    seen = set()\n",
    "    for p in candidates:\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        if p in seen:\n",
    "            continue\n",
    "        seen.add(p)\n",
    "        unique.append(p)\n",
    "    unique.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return unique\n",
    "\n",
    "def connect_db(db_path):\n",
    "    db_path = Path(db_path)\n",
    "    suffix = db_path.suffix.lower()\n",
    "    errors = []\n",
    "\n",
    "    def try_duckdb():\n",
    "        import duckdb\n",
    "        return duckdb.connect(database=str(db_path), read_only=True), 'duckdb'\n",
    "\n",
    "    def try_sqlite():\n",
    "        import sqlite3\n",
    "        uri = f\"file:{db_path.resolve().as_posix()}->mode=ro\"\n",
    "        return sqlite3.connect(uri, uri=True), 'sqlite'\n",
    "\n",
    "    if suffix == '.duckdb':\n",
    "        try:\n",
    "            return try_duckdb()\n",
    "        except Exception as e:\n",
    "            errors.append(f'duckdb: {e}')\n",
    "        try:\n",
    "            return try_sqlite()\n",
    "        except Exception as e:\n",
    "            errors.append(f'sqlite: {e}')\n",
    "    else:\n",
    "        try:\n",
    "            return try_sqlite()\n",
    "        except Exception as e:\n",
    "            errors.append(f'sqlite: {e}')\n",
    "        try:\n",
    "            return try_duckdb()\n",
    "        except Exception as e:\n",
    "            errors.append(f'duckdb: {e}')\n",
    "\n",
    "    raise RuntimeError('Unable to open database. ' + '; '.join(errors))\n",
    "\n",
    "def list_tables(conn, db_type):\n",
    "    if db_type == 'duckdb':\n",
    "        return [r[0] for r in conn.execute('SHOW TABLES').fetchall()]\n",
    "    rows = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()\n",
    "    return [r[0] for r in rows]\n",
    "\n",
    "def get_columns(conn, db_type, table):\n",
    "    pragma = f'PRAGMA table_info(\"{table}\")'\n",
    "    rows = conn.execute(pragma).fetchall()\n",
    "    # rows: (cid, name, type, notnull, dflt_value, pk)\n",
    "    return [r[1] for r in rows]\n",
    "\n",
    "def normalize_name(name):\n",
    "    return ''.join([c for c in name.lower() if c.isalnum()])\n",
    "\n",
    "def pick_column(columns, candidates):\n",
    "    norm_map = {normalize_name(c): c for c in columns}\n",
    "    for cand in candidates:\n",
    "        cand_norm = normalize_name(cand)\n",
    "        if cand_norm in norm_map:\n",
    "            return norm_map[cand_norm]\n",
    "    for cand in candidates:\n",
    "        cand_norm = normalize_name(cand)\n",
    "        for col_norm, original in norm_map.items():\n",
    "            if cand_norm in col_norm:\n",
    "                return original\n",
    "    return None\n",
    "\n",
    "def quote_ident(name):\n",
    "    safe = name.replace('\"', '\"\"')\n",
    "    return f'\"{safe}\"'\n",
    "\n",
    "TICKER_COL_CANDIDATES = ['ticker', 'symbol', 'sym']\n",
    "DATE_COL_CANDIDATES = ['date', 'trade_date', 'market_date', 'dt', 'day']\n",
    "ACCUM_COL_CANDIDATES = [\n",
    "    'accumulation_score', 'accum_score', 'accumulation', 'accum',\n",
    "    'accumulation_score_display', 'accum_score_display'\n",
    "]\n",
    "\n",
    "db_candidates = find_db_candidates()\n",
    "print('DB candidates (newest first):')\n",
    "for p in db_candidates:\n",
    "    print(' -', p)\n",
    "\n",
    "if not db_candidates:\n",
    "    print('No database files found under ./data or ./darkpool_analysis/data.')\n",
    "    raise SystemExit\n",
    "\n",
    "DB_PATH = db_candidates[0]\n",
    "print(f'Selected DB: {DB_PATH}')\n",
    "\n",
    "conn, DB_TYPE = connect_db(DB_PATH)\n",
    "print(f'Detected DB type: {DB_TYPE}')\n",
    "\n",
    "tables = list_tables(conn, DB_TYPE)\n",
    "print(f'Found {len(tables)} tables.')\n",
    "\n",
    "scan_rows = []\n",
    "for table in tables:\n",
    "    try:\n",
    "        cols = get_columns(conn, DB_TYPE, table)\n",
    "    except Exception as e:\n",
    "        scan_rows.append({\n",
    "            'table': table,\n",
    "            'ticker_col': None,\n",
    "            'date_col': None,\n",
    "            'accum_col': None,\n",
    "            'row_count': None,\n",
    "            'distinct_tickers': None,\n",
    "            'error': str(e),\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    ticker_col = pick_column(cols, TICKER_COL_CANDIDATES)\n",
    "    date_col = pick_column(cols, DATE_COL_CANDIDATES)\n",
    "    accum_col = pick_column(cols, ACCUM_COL_CANDIDATES)\n",
    "\n",
    "    row_count = None\n",
    "    distinct_tickers = None\n",
    "    if ticker_col and date_col and accum_col:\n",
    "        try:\n",
    "            row_count = conn.execute(\n",
    "                f'SELECT COUNT(*) FROM {quote_ident(table)}'\n",
    "            ).fetchone()[0]\n",
    "            distinct_tickers = conn.execute(\n",
    "                f'SELECT COUNT(DISTINCT {quote_ident(ticker_col)}) FROM {quote_ident(table)}'\n",
    "            ).fetchone()[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    scan_rows.append({\n",
    "        'table': table,\n",
    "        'ticker_col': ticker_col,\n",
    "        'date_col': date_col,\n",
    "        'accum_col': accum_col,\n",
    "        'row_count': row_count,\n",
    "        'distinct_tickers': distinct_tickers,\n",
    "        'error': None,\n",
    "    })\n",
    "\n",
    "print('Scanned tables (ticker/date/accum detection):')\n",
    "for row in scan_rows:\n",
    "    print(\n",
    "        f\" - {row['table']}: ticker={row['ticker_col']} date={row['date_col']} accum={row['accum_col']} rows={row['row_count']} distinct_tickers={row['distinct_tickers']}\"\n",
    "    )\n",
    "\n",
    "candidates = [r for r in scan_rows if r['ticker_col'] and r['date_col'] and r['accum_col']]\n",
    "if not candidates:\n",
    "    print('No tables matched the required ticker/date/accumulation schema.')\n",
    "    conn.close()\n",
    "    raise SystemExit\n",
    "\n",
    "multi = [r for r in candidates if (r['distinct_tickers'] or 0) > 1]\n",
    "pool = multi if multi else candidates\n",
    "pool.sort(key=lambda r: (r['row_count'] or 0, r['distinct_tickers'] or 0), reverse=True)\n",
    "best = pool[0]\n",
    "\n",
    "SELECT_TABLE = best['table']\n",
    "TICKER_COL = best['ticker_col']\n",
    "DATE_COL = best['date_col']\n",
    "ACCUM_COL = best['accum_col']\n",
    "\n",
    "print('Selected table:')\n",
    "print(f'  table={SELECT_TABLE}')\n",
    "print(f'  ticker_col={TICKER_COL}, date_col={DATE_COL}, accum_col={ACCUM_COL}')\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409971c",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4\ndef build_ticker_universe():\n    seen = set()\n    ordered = []\n    categories = {}\n    enabled = globals().get(\"ENABLED_GROUPS\", {})\n    group_defs = [\n        (\"GLOBAL_MACRO\", globals().get(\"GLOBAL_MACRO_TICKERS\", [])),\n        (\"MAG8\", globals().get(\"MAG8_TICKERS\", [])),\n        (\"SECTOR_SUMMARY\", globals().get(\"SECTOR_SUMMARY_TICKERS\", [])),\n        (\"SECTOR_CORE\", globals().get(\"SECTOR_CORE_TICKERS\", [])),\n        (\"COMMODITIES\", globals().get(\"COMMODITIES_TICKERS\", [])),\n    ]\n    for group_name, tickers in group_defs:\n        if not enabled.get(group_name, True):\n            continue\n        for t in tickers:\n            if t not in seen:\n                seen.add(t)\n                ordered.append(t)\n                categories[t] = group_name\n    return ordered, categories\n\n\nticker_order, ticker_category = build_ticker_universe()\nprint(\"Ticker universe size:\", len(ticker_order))\nprint(\"Enabled groups:\", {k: v for k, v in ENABLED_GROUPS.items()})\nticker_list = [t.upper() for t in ticker_order]\nimport re\n\nconn, DB_TYPE = connect_db(DB_PATH)\n\nif DB_TYPE == 'duckdb':\n    date_expr = f\"CAST({quote_ident(DATE_COL)} AS DATE)\"\n    accum_expr = f\"{quote_ident(ACCUM_COL)}\"\nelse:\n    date_expr = f\"DATE({quote_ident(DATE_COL)})\"\n    accum_expr = f\"{quote_ident(ACCUM_COL)}\"\n\nplaceholders = ','.join(['?'] * len(ticker_list))\nquery = (\n    f\"SELECT UPPER({quote_ident(TICKER_COL)}) AS ticker, \"\n    f\"{date_expr} AS date, {accum_expr} AS accumulation_score \"\n    f\"FROM {quote_ident(SELECT_TABLE)} \"\n    f\"WHERE UPPER({quote_ident(TICKER_COL)}) IN ({placeholders})\"\n)\n\nprint(f\"Using accumulation table: {SELECT_TABLE} (ticker={TICKER_COL}, date={DATE_COL}, accum={ACCUM_COL})\")\n\ntry:\n    if DB_TYPE == 'duckdb':\n        df_raw = conn.execute(query, ticker_list).df()\n    else:\n        df_raw = pd.read_sql_query(query, conn, params=ticker_list)\nfinally:\n    conn.close()\n\nif df_raw.empty:\n    print('No data returned for the specified tickers.')\n    raise SystemExit\n\nprint(\"Accumulation rows loaded:\", len(df_raw))\nprint(\"Accumulation table date range:\", df_raw[\"date\"].min(), \"->\", df_raw[\"date\"].max())\nprint(\"Accumulation raw sample:\", df_raw[\"accumulation_score\"].head(5).tolist())\ndf_raw['date'] = pd.to_datetime(df_raw['date'], errors='coerce').dt.date\ndf_raw['ticker'] = df_raw['ticker'].str.upper()\n\ndef fetch_accum_column(accum_col_name):\n    conn, db_type = connect_db(DB_PATH)\n    try:\n        if db_type == 'duckdb':\n            date_expr = f\"CAST({quote_ident(DATE_COL)} AS DATE)\"\n        else:\n            date_expr = f\"DATE({quote_ident(DATE_COL)})\"\n        query = (\n            f\"SELECT UPPER({quote_ident(TICKER_COL)}) AS ticker, \"\n            f\"{date_expr} AS date, {quote_ident(accum_col_name)} AS accumulation_score \"\n            f\"FROM {quote_ident(SELECT_TABLE)} \"\n            f\"WHERE UPPER({quote_ident(TICKER_COL)}) IN ({placeholders})\"\n        )\n        if db_type == 'duckdb':\n            return conn.execute(query, ticker_list).df()\n        return pd.read_sql_query(query, conn, params=ticker_list)\n    finally:\n        conn.close()\n\ndef normalize_accum_df(df):\n    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.date\n    df['ticker'] = df['ticker'].str.upper()\n    df['accumulation_score'] = df['accumulation_score'].apply(parse_accum)\n    return df.dropna(subset=['date', 'accumulation_score'])\n\ndef parse_accum(value):\n    if value is None:\n        return np.nan\n    if isinstance(value, (int, float, np.number)):\n        if np.isnan(value):\n            return np.nan\n        return float(value)\n    s = str(value).strip()\n    if not s:\n        return np.nan\n    s = s.replace('%', '').replace(',', '')\n    m = re.search(r'-->\\d+(->:\\.\\d+)->', s)\n    return float(m.group(0)) if m else np.nan\n\ndf_raw = normalize_accum_df(df_raw)\nACCUM_COL_SELECTED = ACCUM_COL\nif df_raw.empty:\n    print('Primary accumulation column returned no usable values; trying fallbacks...')\n    conn, db_type = connect_db(DB_PATH)\n    try:\n        cols = get_columns(conn, db_type, SELECT_TABLE)\n    finally:\n        conn.close()\n    candidates = []\n    for cand in [\n        'accumulation_score_display', 'accum_score_display',\n        'accumulation_score', 'accum_score', 'accumulation', 'accum'\n    ]:\n        col = pick_column(cols, [cand])\n        if col and col not in candidates:\n            candidates.append(col)\n    candidates = [c for c in candidates if c != ACCUM_COL]\n    print('Accumulation fallback candidates:', candidates)\n    for col in candidates:\n        print('Trying accumulation column:', col)\n        df_try = fetch_accum_column(col)\n        print('Fallback rows loaded:', len(df_try))\n        print('Fallback raw sample:', df_try['accumulation_score'].head(5).tolist())\n        df_try = normalize_accum_df(df_try)\n        if not df_try.empty:\n            ACCUM_COL_SELECTED = col\n            df_raw = df_try\n            break\n\nif df_raw.empty:\n    print('All accumulation score rows are null after parsing.')\n    raise SystemExit\nprint('Using accumulation column:', ACCUM_COL_SELECTED)\n\ndf_raw_full = df_raw.copy()\n\nmax_date = df_raw['date'].max()\nif END_DATE is None or str(END_DATE).strip() == '':\n    END_DATE_RESOLVED = max_date\nelse:\n    END_DATE_RESOLVED = pd.to_datetime(END_DATE).date()\n    if END_DATE_RESOLVED > max_date:\n        print(f'END_DATE {END_DATE_RESOLVED} exceeds DB max date {max_date}; using max date.')\n        END_DATE_RESOLVED = max_date\n\nflow_days = int(FLOW_PERIOD_DAYS) if FLOW_PERIOD_DAYS and int(FLOW_PERIOD_DAYS) > 0 else 1\nprint(\"Flow period days:\", flow_days, \"END_DATE:\", END_DATE_RESOLVED)\n\nall_dates = sorted([d for d in df_raw[\"date\"].unique() if pd.notna(d)])\nend_dates = [d for d in all_dates if d <= END_DATE_RESOLVED]\nwindow_dates = end_dates[-flow_days:]\nif not window_dates:\n    print(\"No dates available within FLOW_PERIOD_DAYS window.\")\n    raise SystemExit\nprint(\"Window date range used:\", window_dates[0], \"->\", window_dates[-1], \"count:\", len(window_dates))\ndf_accum_daily = df_raw_full.sort_values([\"ticker\", \"date\"]).copy()\ndf_accum_daily[\"accum_net\"] = df_accum_daily.groupby(\"ticker\")[\"accumulation_score\"].diff()\ndf_accum_daily[\"accum_net\"] = df_accum_daily[\"accum_net\"].fillna(0.0)\ndf_accum_daily = df_accum_daily[df_accum_daily[\"date\"].isin(window_dates)][[\"ticker\", \"date\", \"accum_net\"]]\ndf_raw = df_raw[df_raw[\"date\"].isin(window_dates)]\nprint(\"Accumulation window rows:\", len(df_raw))\nmissing = sorted(set(ticker_list) - set(df_raw[\"ticker\"].unique()))\nif missing:\n    print(\"Missing tickers in accumulation window:\", missing)\nif df_raw.empty:\n    print(\"No accumulation data within the selected window.\")\n    raise SystemExit\n\n\ndef tail_n(df_ticker, n, end_date):\n    df = df_ticker[df_ticker['date'] <= end_date].sort_values('date')\n    if df.empty:\n        return df\n    return df.tail(n)\n\n\nrows = []\nfor ticker in ticker_order:\n    df_t = df_raw[df_raw['ticker'] == ticker]\n    tail = tail_n(df_t, flow_days, END_DATE_RESOLVED)\n    if len(tail) < 2:\n        a_start = None\n        a_end = None\n    else:\n        a_start = float(tail['accumulation_score'].iloc[0])\n        a_end = float(tail['accumulation_score'].iloc[-1])\n    rows.append({\n        'ticker': ticker,\n        'category': ticker_category.get(ticker, 'UNKNOWN'),\n        'A_end': a_end,\n        'A_start': a_start,\n        'end_date': END_DATE_RESOLVED,\n        'start_date': tail['date'].iloc[0] if len(tail) else None,\n        'samples': len(tail),\n    })\n\ndf_scores = pd.DataFrame(rows)\n\n# --- Volume data discovery (lit/short buy/sell + new columns for rings) ---\nLIT_BUY_CANDIDATES = ['lit_buy_volume', 'lit_buy_vol', 'lit_buy']\nLIT_SELL_CANDIDATES = ['lit_sell_volume', 'lit_sell_vol', 'lit_sell']\nSHORT_BUY_CANDIDATES = ['short_buy_volume', 'short_buy_vol', 'short_buy']\nSHORT_SELL_CANDIDATES = ['short_sell_volume', 'short_sell_vol', 'short_sell']\n# New columns for rings\nOTC_VOLUME_CANDIDATES = ['otc_off_exchange_volume', 'otc_volume', 'dark_volume']\nLIT_TOTAL_CANDIDATES = ['lit_total_volume', 'lit_volume', 'lit_total']\nFINRA_BUY_CANDIDATES = ['finra_buy_volume', 'finra_buy', 'finra_buy_vol']\n\n\ndef find_volume_table(conn, db_type):\n    tables = list_tables(conn, db_type)\n    candidates = []\n    for table in tables:\n        try:\n            cols = get_columns(conn, db_type, table)\n        except Exception:\n            continue\n        ticker_col = pick_column(cols, TICKER_COL_CANDIDATES)\n        date_col = pick_column(cols, DATE_COL_CANDIDATES)\n        lit_buy_col = pick_column(cols, LIT_BUY_CANDIDATES)\n        lit_sell_col = pick_column(cols, LIT_SELL_CANDIDATES)\n        short_buy_col = pick_column(cols, SHORT_BUY_CANDIDATES)\n        short_sell_col = pick_column(cols, SHORT_SELL_CANDIDATES)\n        # New columns (optional but preferred)\n        otc_vol_col = pick_column(cols, OTC_VOLUME_CANDIDATES)\n        lit_total_col = pick_column(cols, LIT_TOTAL_CANDIDATES)\n        finra_buy_col = pick_column(cols, FINRA_BUY_CANDIDATES)\n        if not (ticker_col and date_col and lit_buy_col and lit_sell_col and short_buy_col and short_sell_col):\n            continue\n        row_count = None\n        distinct_tickers = None\n        try:\n            row_count = conn.execute(\n                f'SELECT COUNT(*) FROM {quote_ident(table)}'\n            ).fetchone()[0]\n            distinct_tickers = conn.execute(\n                f'SELECT COUNT(DISTINCT {quote_ident(ticker_col)}) FROM {quote_ident(table)}'\n            ).fetchone()[0]\n        except Exception:\n            pass\n        candidates.append({\n            'table': table,\n            'ticker_col': ticker_col,\n            'date_col': date_col,\n            'lit_buy_col': lit_buy_col,\n            'lit_sell_col': lit_sell_col,\n            'short_buy_col': short_buy_col,\n            'short_sell_col': short_sell_col,\n            'otc_vol_col': otc_vol_col,\n            'lit_total_col': lit_total_col,\n            'finra_buy_col': finra_buy_col,\n            'row_count': row_count,\n            'distinct_tickers': distinct_tickers,\n        })\n    if not candidates:\n        return None\n    candidates.sort(key=lambda r: (r['row_count'] or 0, r['distinct_tickers'] or 0), reverse=True)\n    return candidates[0]\n\n\nconn, DB_TYPE = connect_db(DB_PATH)\ntry:\n    volume_info = find_volume_table(conn, DB_TYPE)\nfinally:\n    conn.close()\n\nVOLUME_DATA_AVAILABLE = volume_info is not None\nif VOLUME_DATA_AVAILABLE:\n    print(\"Volume table selected:\", volume_info[\"table\"])\n    print(\"Volume columns:\", {k: volume_info[k] for k in [\"ticker_col\", \"date_col\", \"lit_buy_col\", \"lit_sell_col\", \"short_buy_col\", \"short_sell_col\"]})\n    print(\"Ring columns:\", {k: volume_info.get(k) for k in [\"otc_vol_col\", \"lit_total_col\", \"finra_buy_col\"]})\nelse:\n    print(\"No volume table found with lit/short buy/sell columns.\")\n\nif VOLUME_DATA_AVAILABLE:\n    conn, DB_TYPE = connect_db(DB_PATH)\n    try:\n        if DB_TYPE == 'duckdb':\n            date_expr = f\"CAST({quote_ident(volume_info['date_col'])} AS DATE)\"\n            num_cast = \"TRY_CAST\"\n        else:\n            date_expr = f\"DATE({quote_ident(volume_info['date_col'])})\"\n            num_cast = \"CAST\"\n\n        # Build query with all columns including new ring columns\n        select_cols = [\n            f\"UPPER({quote_ident(volume_info['ticker_col'])}) AS ticker\",\n            f\"{date_expr} AS date\",\n            f\"{num_cast}({quote_ident(volume_info['lit_buy_col'])} AS DOUBLE) AS lit_buy\",\n            f\"{num_cast}({quote_ident(volume_info['lit_sell_col'])} AS DOUBLE) AS lit_sell\",\n            f\"{num_cast}({quote_ident(volume_info['short_buy_col'])} AS DOUBLE) AS short_buy\",\n            f\"{num_cast}({quote_ident(volume_info['short_sell_col'])} AS DOUBLE) AS short_sell\",\n        ]\n        # Add optional ring columns if available\n        if volume_info.get('otc_vol_col'):\n            select_cols.append(f\"{num_cast}({quote_ident(volume_info['otc_vol_col'])} AS DOUBLE) AS otc_volume\")\n        if volume_info.get('lit_total_col'):\n            select_cols.append(f\"{num_cast}({quote_ident(volume_info['lit_total_col'])} AS DOUBLE) AS lit_total\")\n        if volume_info.get('finra_buy_col'):\n            select_cols.append(f\"{num_cast}({quote_ident(volume_info['finra_buy_col'])} AS DOUBLE) AS finra_buy\")\n\n        query = (\n            f\"SELECT {', '.join(select_cols)} \"\n            f\"FROM {quote_ident(volume_info['table'])} \"\n            f\"WHERE UPPER({quote_ident(volume_info['ticker_col'])}) IN ({placeholders})\"\n        )\n\n        if DB_TYPE == 'duckdb':\n            df_vol_raw = conn.execute(query, ticker_list).df()\n        else:\n            df_vol_raw = pd.read_sql_query(query, conn, params=ticker_list)\n    finally:\n        conn.close()\n\n    df_vol_raw['date'] = pd.to_datetime(df_vol_raw['date'], errors='coerce').dt.date\n    print('Volume table rows loaded:', len(df_vol_raw))\n    print('Volume table date range:', df_vol_raw['date'].min(), '->', df_vol_raw['date'].max())\n    df_vol_raw = df_vol_raw[df_vol_raw['date'].isin(window_dates)]\n    print('Volume window date range:', df_vol_raw['date'].min(), '->', df_vol_raw['date'].max(), 'rows:', len(df_vol_raw))\n    df_vol_raw['ticker'] = df_vol_raw['ticker'].str.upper()\n    for col in ['lit_buy', 'lit_sell', 'short_buy', 'short_sell']:\n        df_vol_raw[col] = pd.to_numeric(df_vol_raw[col], errors='coerce')\n    # Also convert new columns if present\n    for col in ['otc_volume', 'lit_total', 'finra_buy']:\n        if col in df_vol_raw.columns:\n            df_vol_raw[col] = pd.to_numeric(df_vol_raw[col], errors='coerce')\n    df_vol_raw = df_vol_raw.dropna(subset=['date'])\n    \n    df_lit_daily = (\n        df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n        .agg({'lit_buy': 'sum', 'lit_sell': 'sum'})\n    )\n    df_lit_daily['lit_net'] = df_lit_daily['lit_buy'] - df_lit_daily['lit_sell']\n    df_short_daily = (\n        df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n        .agg({'short_buy': 'sum', 'short_sell': 'sum'})\n    )\n    df_short_daily['short_net'] = df_short_daily['short_buy'] - df_short_daily['short_sell']\n\n    # Create dark/lit daily data for Ring 2\n    if 'otc_volume' in df_vol_raw.columns and 'lit_total' in df_vol_raw.columns:\n        df_dark_lit_daily = (\n            df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n            .agg({'otc_volume': 'sum', 'lit_total': 'sum'})\n        )\n        df_dark_lit_daily['total_volume'] = df_dark_lit_daily['otc_volume'] + df_dark_lit_daily['lit_total']\n        df_dark_lit_daily['dark_ratio'] = df_dark_lit_daily['otc_volume'] / df_dark_lit_daily['total_volume'].replace(0, np.nan)\n        df_dark_lit_daily['dark_ratio'] = df_dark_lit_daily['dark_ratio'].fillna(0.5)\n        print('Dark/Lit daily data created:', len(df_dark_lit_daily), 'rows')\n    else:\n        df_dark_lit_daily = pd.DataFrame(columns=['ticker', 'date', 'otc_volume', 'lit_total', 'total_volume', 'dark_ratio'])\n        print('Dark/Lit columns not available - using empty dataframe')\n\n    # Create finra_buy daily data for Ring 3\n    if 'finra_buy' in df_vol_raw.columns:\n        df_finra_daily = (\n            df_vol_raw.groupby(['ticker', 'date'], as_index=False)\n            .agg({'finra_buy': 'sum'})\n        )\n        print('Finra buy daily data created:', len(df_finra_daily), 'rows')\n    else:\n        df_finra_daily = pd.DataFrame(columns=['ticker', 'date', 'finra_buy'])\n        print('Finra buy column not available - using empty dataframe')\n\n    vol_rows = []\n    for ticker in ticker_order:\n        df_t = df_vol_raw[df_vol_raw['ticker'] == ticker]\n        tail = tail_n(df_t, flow_days, END_DATE_RESOLVED)\n        if tail.empty:\n            vol_rows.append({\n                'ticker': ticker,\n                'lit_buy_sum': None,\n                'lit_sell_sum': None,\n                'short_buy_sum': None,\n                'short_sell_sum': None,\n                'finra_buy_sum': None,\n                'volume_samples': 0,\n            })\n            continue\n        vol_rows.append({\n            'ticker': ticker,\n            'lit_buy_sum': float(tail['lit_buy'].sum(skipna=True)),\n            'lit_sell_sum': float(tail['lit_sell'].sum(skipna=True)),\n            'short_buy_sum': float(tail['short_buy'].sum(skipna=True)),\n            'short_sell_sum': float(tail['short_sell'].sum(skipna=True)),\n            'finra_buy_sum': float(tail['finra_buy'].sum(skipna=True)) if 'finra_buy' in tail.columns else None,\n            'volume_samples': len(tail),\n        })\n\n    df_volume = pd.DataFrame(vol_rows)\n    total_samples = int(df_volume['volume_samples'].sum()) if not df_volume.empty else 0\n    lit_total = float(df_volume['lit_buy_sum'].fillna(0).sum() + df_volume['lit_sell_sum'].fillna(0).sum()) if not df_volume.empty else 0.0\n    short_total = float(df_volume['short_buy_sum'].fillna(0).sum() + df_volume['short_sell_sum'].fillna(0).sum()) if not df_volume.empty else 0.0\n    if total_samples == 0 or (lit_total == 0 and short_total == 0):\n        print('Volume data missing for selected period. Lit/short chords require non-zero buy/sell volume data.')\n        raise SystemExit\nelse:\n    df_volume = pd.DataFrame(columns=[\n        'ticker', 'lit_buy_sum', 'lit_sell_sum', 'short_buy_sum', 'short_sell_sum', 'finra_buy_sum', 'volume_samples'\n    ])\n    df_dark_lit_daily = pd.DataFrame(columns=['ticker', 'date', 'otc_volume', 'lit_total', 'total_volume', 'dark_ratio'])\n    df_finra_daily = pd.DataFrame(columns=['ticker', 'date', 'finra_buy'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95f5b2",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5\ndef build_edges_from_value(df, value_col, top_k_winners, top_k_losers):\n    df = df[['ticker', value_col]].dropna().copy()\n    winners = df[df[value_col] > 0].nlargest(top_k_winners, value_col)\n    losers = df[df[value_col] < 0].copy()\n    losers['supply'] = -losers[value_col]\n    losers = losers.nlargest(top_k_losers, 'supply')\n\n    edges = []\n    total_demand = winners[value_col].sum() if not winners.empty else 0.0\n    total_supply = losers['supply'].sum() if not losers.empty else 0.0\n\n    if winners.empty or losers.empty:\n        return pd.DataFrame(edges), winners, losers, total_demand, total_supply\n\n    if DISTRIBUTION_MODE not in {'equal', 'demand_weighted'}:\n        raise ValueError('DISTRIBUTION_MODE must be \"equal\" or \"demand_weighted\"')\n\n    if DISTRIBUTION_MODE == 'equal':\n        for _, loser in losers.iterrows():\n            flow_each = loser['supply'] / len(winners)\n            for _, winner in winners.iterrows():\n                if flow_each >= MIN_EDGE_FLOW:\n                    edges.append({\n                        'source': loser['ticker'],\n                        'dest': winner['ticker'],\n                        'flow': float(flow_each),\n                    })\n    else:\n        if total_demand > 0:\n            for _, loser in losers.iterrows():\n                for _, winner in winners.iterrows():\n                    flow = loser['supply'] * (winner[value_col] / total_demand)\n                    if flow >= MIN_EDGE_FLOW:\n                        edges.append({\n                            'source': loser['ticker'],\n                            'dest': winner['ticker'],\n                            'flow': float(flow),\n                        })\n\n    edges_df = pd.DataFrame(edges)\n    if not edges_df.empty:\n        edges_df = edges_df.sort_values('flow', ascending=False).reset_index(drop=True)\n    return edges_df, winners, losers, total_demand, total_supply\n\n\ndef build_edges_from_positive_value(df, value_col, top_k_high, top_k_low):\n    \"\"\"Build edges for metrics that are always positive (like finra_buy_volume).\n    Flow goes from low-value tickers to high-value tickers.\"\"\"\n    df = df[['ticker', value_col]].dropna().copy()\n    df = df[df[value_col] > 0]  # Only positive values\n    if df.empty:\n        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), 0.0, 0.0\n    \n    median_val = df[value_col].median()\n    high_tickers = df[df[value_col] >= median_val].nlargest(top_k_high, value_col)\n    low_tickers = df[df[value_col] < median_val].nsmallest(top_k_low, value_col)\n    \n    if high_tickers.empty or low_tickers.empty:\n        return pd.DataFrame(), high_tickers, low_tickers, 0.0, 0.0\n    \n    total_high = high_tickers[value_col].sum()\n    total_low = low_tickers[value_col].sum()\n    \n    edges = []\n    for _, low_row in low_tickers.iterrows():\n        for _, high_row in high_tickers.iterrows():\n            # Flow proportional to the difference\n            flow = (high_row[value_col] - low_row[value_col]) * (low_row[value_col] / total_low)\n            if flow > MIN_EDGE_FLOW:\n                edges.append({\n                    'source': low_row['ticker'],\n                    'dest': high_row['ticker'],\n                    'flow': float(flow),\n                })\n    \n    edges_df = pd.DataFrame(edges)\n    if not edges_df.empty:\n        edges_df = edges_df.sort_values('flow', ascending=False).reset_index(drop=True)\n    return edges_df, high_tickers, low_tickers, total_high, total_low\n\n\n# Accumulation flow (average level over window, centered by mean)\nif df_scores['A_end'].notna().any() and df_scores['A_start'].notna().any():\n    df_scores['delta'] = df_scores['A_end'] - df_scores['A_start']\nelse:\n    df_scores['delta'] = np.nan\n\ndf_accum_level = df_raw_full[df_raw_full['date'].isin(window_dates)][['ticker', 'date', 'accumulation_score']].copy()\nif 'accum_avg' in df_scores.columns:\n    df_scores = df_scores.drop(columns=['accum_avg', 'accum_centered'], errors='ignore')\nif not df_accum_level.empty:\n    df_accum_avg = df_accum_level.groupby('ticker')['accumulation_score'].mean()\n    df_scores = df_scores.merge(df_accum_avg.rename('accum_avg'), on='ticker', how='left')\nelse:\n    df_scores['accum_avg'] = np.nan\n\nif df_scores['accum_avg'].notna().any():\n    mean_level = df_scores['accum_avg'].mean()\n    df_scores['accum_centered'] = df_scores['accum_avg'] - mean_level\nelse:\n    df_scores['accum_centered'] = np.nan\n\naccum_for_flow = df_scores['accum_centered']\n\ndf_scores['role'] = np.where(\n    accum_for_flow > 0,\n    'winner',\n    np.where(accum_for_flow < 0, 'loser', 'neutral')\n)\n\ndf_scores_sorted = df_scores.sort_values(\n    by='accum_centered', key=lambda s: s.abs(), ascending=False\n).reset_index(drop=True)\n\naccum_edges_df, accum_winners, accum_losers, accum_demand, accum_supply = build_edges_from_value(\n    df_scores, 'accum_centered', TOP_K_WINNERS, TOP_K_LOSERS\n)\nprint(\"Accum winners/losers:\", len(accum_winners), len(accum_losers), \"edges:\", len(accum_edges_df))\n\n# Lit and Short flows (net buy - sell)\nif VOLUME_DATA_AVAILABLE and not df_volume.empty:\n    df_volume = df_volume.copy()\n    df_volume['lit_net'] = df_volume['lit_buy_sum'] - df_volume['lit_sell_sum']\n    df_volume['short_net'] = df_volume['short_buy_sum'] - df_volume['short_sell_sum']\n\n    lit_edges_df, lit_winners, lit_losers, lit_demand, lit_supply = build_edges_from_value(\n        df_volume, 'lit_net', TOP_K_WINNERS, TOP_K_LOSERS\n    )\n    short_edges_df, short_winners, short_losers, short_demand, short_supply = build_edges_from_value(\n        df_volume, 'short_net', TOP_K_WINNERS, TOP_K_LOSERS\n    )\n\n    print(\"Lit winners/losers:\", len(lit_winners), len(lit_losers), \"edges:\", len(lit_edges_df))\n    print(\"Short winners/losers:\", len(short_winners), len(short_losers), \"edges:\", len(short_edges_df))\n    \n    # Finra buy flow (positive values - flow from low to high)\n    if 'finra_buy_sum' in df_volume.columns and df_volume['finra_buy_sum'].notna().any():\n        finra_edges_df, finra_high, finra_low, finra_high_total, finra_low_total = build_edges_from_positive_value(\n            df_volume, 'finra_buy_sum', TOP_K_WINNERS, TOP_K_LOSERS\n        )\n        print(\"Finra buy high/low:\", len(finra_high), len(finra_low), \"edges:\", len(finra_edges_df))\n    else:\n        finra_edges_df = pd.DataFrame()\n        finra_high = pd.DataFrame()\n        finra_low = pd.DataFrame()\n        finra_high_total = finra_low_total = 0.0\n        print(\"Finra buy data not available\")\nelse:\n    lit_edges_df = pd.DataFrame()\n    short_edges_df = pd.DataFrame()\n    finra_edges_df = pd.DataFrame()\n    lit_winners = pd.DataFrame()\n    short_winners = pd.DataFrame()\n    finra_high = pd.DataFrame()\n    lit_losers = pd.DataFrame()\n    short_losers = pd.DataFrame()\n    finra_low = pd.DataFrame()\n    lit_demand = lit_supply = 0.0\n    short_demand = short_supply = 0.0\n    finra_high_total = finra_low_total = 0.0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1ac9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>category</th>\n",
       "      <th>A_end</th>\n",
       "      <th>A_start</th>\n",
       "      <th>accum_avg</th>\n",
       "      <th>accum_centered</th>\n",
       "      <th>role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XLE</td>\n",
       "      <td>SECTOR_SUMMARY</td>\n",
       "      <td>0.217822</td>\n",
       "      <td>0.651581</td>\n",
       "      <td>0.315407</td>\n",
       "      <td>0.378403</td>\n",
       "      <td>winner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XLF</td>\n",
       "      <td>SECTOR_SUMMARY</td>\n",
       "      <td>-0.472236</td>\n",
       "      <td>-0.484483</td>\n",
       "      <td>-0.393543</td>\n",
       "      <td>-0.330547</td>\n",
       "      <td>loser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XLI</td>\n",
       "      <td>SECTOR_SUMMARY</td>\n",
       "      <td>-0.205639</td>\n",
       "      <td>-0.328092</td>\n",
       "      <td>-0.211375</td>\n",
       "      <td>-0.148379</td>\n",
       "      <td>loser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XLC</td>\n",
       "      <td>SECTOR_SUMMARY</td>\n",
       "      <td>0.065686</td>\n",
       "      <td>0.225984</td>\n",
       "      <td>0.078746</td>\n",
       "      <td>0.141742</td>\n",
       "      <td>winner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XLV</td>\n",
       "      <td>SECTOR_SUMMARY</td>\n",
       "      <td>0.153938</td>\n",
       "      <td>0.172622</td>\n",
       "      <td>0.069567</td>\n",
       "      <td>0.132563</td>\n",
       "      <td>winner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XLU</td>\n",
       "      <td>SECTOR_SUMMARY</td>\n",
       "      <td>-0.478017</td>\n",
       "      <td>-0.180224</td>\n",
       "      <td>-0.180913</td>\n",
       "      <td>-0.117917</td>\n",
       "      <td>loser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XLP</td>\n",
       "      <td>SECTOR_SUMMARY</td>\n",
       "      <td>-0.066224</td>\n",
       "      <td>0.383364</td>\n",
       "      <td>0.033299</td>\n",
       "      <td>0.096295</td>\n",
       "      <td>winner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XLK</td>\n",
       "      <td>SECTOR_SUMMARY</td>\n",
       "      <td>-0.347585</td>\n",
       "      <td>-0.034062</td>\n",
       "      <td>-0.139328</td>\n",
       "      <td>-0.076332</td>\n",
       "      <td>loser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XLRE</td>\n",
       "      <td>SECTOR_SUMMARY</td>\n",
       "      <td>-0.320607</td>\n",
       "      <td>0.117716</td>\n",
       "      <td>-0.112388</td>\n",
       "      <td>-0.049393</td>\n",
       "      <td>loser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XLB</td>\n",
       "      <td>SECTOR_SUMMARY</td>\n",
       "      <td>0.049207</td>\n",
       "      <td>-0.325636</td>\n",
       "      <td>-0.100694</td>\n",
       "      <td>-0.037698</td>\n",
       "      <td>loser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SPY</td>\n",
       "      <td>SECTOR_SUMMARY</td>\n",
       "      <td>-0.316119</td>\n",
       "      <td>0.079652</td>\n",
       "      <td>-0.039220</td>\n",
       "      <td>0.023776</td>\n",
       "      <td>winner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XLY</td>\n",
       "      <td>SECTOR_SUMMARY</td>\n",
       "      <td>0.095512</td>\n",
       "      <td>-0.460492</td>\n",
       "      <td>-0.075509</td>\n",
       "      <td>-0.012513</td>\n",
       "      <td>loser</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticker        category     A_end   A_start  accum_avg  accum_centered  \\\n",
       "0     XLE  SECTOR_SUMMARY  0.217822  0.651581   0.315407        0.378403   \n",
       "1     XLF  SECTOR_SUMMARY -0.472236 -0.484483  -0.393543       -0.330547   \n",
       "2     XLI  SECTOR_SUMMARY -0.205639 -0.328092  -0.211375       -0.148379   \n",
       "3     XLC  SECTOR_SUMMARY  0.065686  0.225984   0.078746        0.141742   \n",
       "4     XLV  SECTOR_SUMMARY  0.153938  0.172622   0.069567        0.132563   \n",
       "5     XLU  SECTOR_SUMMARY -0.478017 -0.180224  -0.180913       -0.117917   \n",
       "6     XLP  SECTOR_SUMMARY -0.066224  0.383364   0.033299        0.096295   \n",
       "7     XLK  SECTOR_SUMMARY -0.347585 -0.034062  -0.139328       -0.076332   \n",
       "8    XLRE  SECTOR_SUMMARY -0.320607  0.117716  -0.112388       -0.049393   \n",
       "9     XLB  SECTOR_SUMMARY  0.049207 -0.325636  -0.100694       -0.037698   \n",
       "10    SPY  SECTOR_SUMMARY -0.316119  0.079652  -0.039220        0.023776   \n",
       "11    XLY  SECTOR_SUMMARY  0.095512 -0.460492  -0.075509       -0.012513   \n",
       "\n",
       "      role  \n",
       "0   winner  \n",
       "1    loser  \n",
       "2    loser  \n",
       "3   winner  \n",
       "4   winner  \n",
       "5    loser  \n",
       "6   winner  \n",
       "7    loser  \n",
       "8    loser  \n",
       "9    loser  \n",
       "10  winner  \n",
       "11   loser  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>dest</th>\n",
       "      <th>flow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XLF</td>\n",
       "      <td>XLE</td>\n",
       "      <td>161.857604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XLI</td>\n",
       "      <td>XLE</td>\n",
       "      <td>72.655907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XLF</td>\n",
       "      <td>XLC</td>\n",
       "      <td>60.628512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XLU</td>\n",
       "      <td>XLE</td>\n",
       "      <td>57.740061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XLF</td>\n",
       "      <td>XLV</td>\n",
       "      <td>56.702320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XLF</td>\n",
       "      <td>XLP</td>\n",
       "      <td>41.189044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XLK</td>\n",
       "      <td>XLE</td>\n",
       "      <td>37.377115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XLI</td>\n",
       "      <td>XLC</td>\n",
       "      <td>27.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XLI</td>\n",
       "      <td>XLV</td>\n",
       "      <td>25.452980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XLRE</td>\n",
       "      <td>XLE</td>\n",
       "      <td>24.185835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XLU</td>\n",
       "      <td>XLC</td>\n",
       "      <td>21.628233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XLU</td>\n",
       "      <td>XLV</td>\n",
       "      <td>20.227628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XLI</td>\n",
       "      <td>XLP</td>\n",
       "      <td>18.489260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XLB</td>\n",
       "      <td>XLE</td>\n",
       "      <td>18.459368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XLU</td>\n",
       "      <td>XLP</td>\n",
       "      <td>14.693520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>XLK</td>\n",
       "      <td>XLC</td>\n",
       "      <td>14.000694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XLK</td>\n",
       "      <td>XLV</td>\n",
       "      <td>13.094035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XLF</td>\n",
       "      <td>SPY</td>\n",
       "      <td>10.169920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>XLK</td>\n",
       "      <td>XLP</td>\n",
       "      <td>9.511618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>XLRE</td>\n",
       "      <td>XLC</td>\n",
       "      <td>9.059514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>XLRE</td>\n",
       "      <td>XLV</td>\n",
       "      <td>8.472836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>XLB</td>\n",
       "      <td>XLC</td>\n",
       "      <td>6.914498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>XLB</td>\n",
       "      <td>XLV</td>\n",
       "      <td>6.466727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>XLRE</td>\n",
       "      <td>XLP</td>\n",
       "      <td>6.154740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>XLY</td>\n",
       "      <td>XLE</td>\n",
       "      <td>6.127189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>XLB</td>\n",
       "      <td>XLP</td>\n",
       "      <td>4.697485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>XLI</td>\n",
       "      <td>SPY</td>\n",
       "      <td>4.565153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>XLU</td>\n",
       "      <td>SPY</td>\n",
       "      <td>3.627953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>XLK</td>\n",
       "      <td>SPY</td>\n",
       "      <td>2.348498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>XLY</td>\n",
       "      <td>XLC</td>\n",
       "      <td>2.295118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>XLY</td>\n",
       "      <td>XLV</td>\n",
       "      <td>2.146490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>XLY</td>\n",
       "      <td>XLP</td>\n",
       "      <td>1.559229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>XLRE</td>\n",
       "      <td>SPY</td>\n",
       "      <td>1.519657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>XLB</td>\n",
       "      <td>SPY</td>\n",
       "      <td>1.159848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>XLY</td>\n",
       "      <td>SPY</td>\n",
       "      <td>0.384987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source dest        flow\n",
       "0     XLF  XLE  161.857604\n",
       "1     XLI  XLE   72.655907\n",
       "2     XLF  XLC   60.628512\n",
       "3     XLU  XLE   57.740061\n",
       "4     XLF  XLV   56.702320\n",
       "5     XLF  XLP   41.189044\n",
       "6     XLK  XLE   37.377115\n",
       "7     XLI  XLC   27.215400\n",
       "8     XLI  XLV   25.452980\n",
       "9    XLRE  XLE   24.185835\n",
       "10    XLU  XLC   21.628233\n",
       "11    XLU  XLV   20.227628\n",
       "12    XLI  XLP   18.489260\n",
       "13    XLB  XLE   18.459368\n",
       "14    XLU  XLP   14.693520\n",
       "15    XLK  XLC   14.000694\n",
       "16    XLK  XLV   13.094035\n",
       "17    XLF  SPY   10.169920\n",
       "18    XLK  XLP    9.511618\n",
       "19   XLRE  XLC    9.059514\n",
       "20   XLRE  XLV    8.472836\n",
       "21    XLB  XLC    6.914498\n",
       "22    XLB  XLV    6.466727\n",
       "23   XLRE  XLP    6.154740\n",
       "24    XLY  XLE    6.127189\n",
       "25    XLB  XLP    4.697485\n",
       "26    XLI  SPY    4.565153\n",
       "27    XLU  SPY    3.627953\n",
       "28    XLK  SPY    2.348498\n",
       "29    XLY  XLC    2.295118\n",
       "30    XLY  XLV    2.146490\n",
       "31    XLY  XLP    1.559229\n",
       "32   XLRE  SPY    1.519657\n",
       "33    XLB  SPY    1.159848\n",
       "34    XLY  SPY    0.384987"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accum chord counts (out/in):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>out</th>\n",
       "      <th>in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLB</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLC</th>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLE</th>\n",
       "      <td>0</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLF</th>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLI</th>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLK</th>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLP</th>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLRE</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLU</th>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLV</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLY</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      out   in\n",
       "SPY     0   24\n",
       "XLB    38    0\n",
       "XLC     0  142\n",
       "XLE     0  378\n",
       "XLF   331    0\n",
       "XLI   148    0\n",
       "XLK    76    0\n",
       "XLP     0   96\n",
       "XLRE   49    0\n",
       "XLU   118    0\n",
       "XLV     0  133\n",
       "XLY    13    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lit chord counts (out/in):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>out</th>\n",
       "      <th>in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>0</td>\n",
       "      <td>-2147483648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLB</th>\n",
       "      <td>0</td>\n",
       "      <td>120494028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLC</th>\n",
       "      <td>0</td>\n",
       "      <td>262663533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLE</th>\n",
       "      <td>0</td>\n",
       "      <td>680054723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLF</th>\n",
       "      <td>0</td>\n",
       "      <td>61562797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLI</th>\n",
       "      <td>251340000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLK</th>\n",
       "      <td>1275236000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLP</th>\n",
       "      <td>21247000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLRE</th>\n",
       "      <td>0</td>\n",
       "      <td>37534729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLU</th>\n",
       "      <td>1803466000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLV</th>\n",
       "      <td>130204000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLY</th>\n",
       "      <td>613299000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             out          in\n",
       "SPY            0 -2147483648\n",
       "XLB            0   120494028\n",
       "XLC            0   262663533\n",
       "XLE            0   680054723\n",
       "XLF            0    61562797\n",
       "XLI    251340000           0\n",
       "XLK   1275236000           0\n",
       "XLP     21247000           0\n",
       "XLRE           0    37534729\n",
       "XLU   1803466000           0\n",
       "XLV    130204000           0\n",
       "XLY    613299000           0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short chord counts (out/in):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>out</th>\n",
       "      <th>in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>0</td>\n",
       "      <td>866202937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLB</th>\n",
       "      <td>0</td>\n",
       "      <td>263817654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLE</th>\n",
       "      <td>0</td>\n",
       "      <td>2099946186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLF</th>\n",
       "      <td>-2147483648</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLI</th>\n",
       "      <td>0</td>\n",
       "      <td>321594815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLP</th>\n",
       "      <td>0</td>\n",
       "      <td>520574009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLRE</th>\n",
       "      <td>0</td>\n",
       "      <td>415214929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLU</th>\n",
       "      <td>1042825000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLV</th>\n",
       "      <td>0</td>\n",
       "      <td>441028877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLY</th>\n",
       "      <td>0</td>\n",
       "      <td>283169592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             out          in\n",
       "SPY            0   866202937\n",
       "XLB            0   263817654\n",
       "XLE            0  2099946186\n",
       "XLF  -2147483648           0\n",
       "XLI            0   321594815\n",
       "XLP            0   520574009\n",
       "XLRE           0   415214929\n",
       "XLU   1042825000           0\n",
       "XLV            0   441028877\n",
       "XLY            0   283169592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume availability: True rows: 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>lit_buy_sum</th>\n",
       "      <th>lit_sell_sum</th>\n",
       "      <th>short_buy_sum</th>\n",
       "      <th>short_sell_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XLE</td>\n",
       "      <td>12186103.0</td>\n",
       "      <td>9382739.0</td>\n",
       "      <td>13765240.0</td>\n",
       "      <td>6312100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XLF</td>\n",
       "      <td>11598640.0</td>\n",
       "      <td>11344862.0</td>\n",
       "      <td>9771327.0</td>\n",
       "      <td>13940051.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XLK</td>\n",
       "      <td>3527516.0</td>\n",
       "      <td>4802752.0</td>\n",
       "      <td>3457046.0</td>\n",
       "      <td>3124057.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XLY</td>\n",
       "      <td>1958870.0</td>\n",
       "      <td>2572169.0</td>\n",
       "      <td>2199123.0</td>\n",
       "      <td>1194096.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XLP</td>\n",
       "      <td>3700953.0</td>\n",
       "      <td>3722200.0</td>\n",
       "      <td>3788064.0</td>\n",
       "      <td>1940440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XLV</td>\n",
       "      <td>2325662.0</td>\n",
       "      <td>2455866.0</td>\n",
       "      <td>2772763.0</td>\n",
       "      <td>1207461.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XLU</td>\n",
       "      <td>4206592.0</td>\n",
       "      <td>6010058.0</td>\n",
       "      <td>5316540.0</td>\n",
       "      <td>6359365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XLI</td>\n",
       "      <td>1810930.0</td>\n",
       "      <td>2062270.0</td>\n",
       "      <td>2191203.0</td>\n",
       "      <td>1049797.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XLC</td>\n",
       "      <td>2772048.0</td>\n",
       "      <td>1689280.0</td>\n",
       "      <td>1925475.0</td>\n",
       "      <td>1137209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XLB</td>\n",
       "      <td>2532591.0</td>\n",
       "      <td>2035883.0</td>\n",
       "      <td>2787059.0</td>\n",
       "      <td>1850716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XLRE</td>\n",
       "      <td>3218452.0</td>\n",
       "      <td>3063724.0</td>\n",
       "      <td>4947707.0</td>\n",
       "      <td>3474024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SPY</td>\n",
       "      <td>42239429.0</td>\n",
       "      <td>30150968.0</td>\n",
       "      <td>27005885.0</td>\n",
       "      <td>23931553.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticker  lit_buy_sum  lit_sell_sum  short_buy_sum  short_sell_sum\n",
       "0     XLE   12186103.0     9382739.0     13765240.0       6312100.0\n",
       "1     XLF   11598640.0    11344862.0      9771327.0      13940051.0\n",
       "2     XLK    3527516.0     4802752.0      3457046.0       3124057.0\n",
       "3     XLY    1958870.0     2572169.0      2199123.0       1194096.0\n",
       "4     XLP    3700953.0     3722200.0      3788064.0       1940440.0\n",
       "5     XLV    2325662.0     2455866.0      2772763.0       1207461.0\n",
       "6     XLU    4206592.0     6010058.0      5316540.0       6359365.0\n",
       "7     XLI    1810930.0     2062270.0      2191203.0       1049797.0\n",
       "8     XLC    2772048.0     1689280.0      1925475.0       1137209.0\n",
       "9     XLB    2532591.0     2035883.0      2787059.0       1850716.0\n",
       "10   XLRE    3218452.0     3063724.0      4947707.0       3474024.0\n",
       "11    SPY   42239429.0    30150968.0     27005885.0      23931553.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END_DATE=2025-12-31 | FLOW_PERIOD_DAYS=3 | accum_supply=0.77 | accum_demand=0.77 | lit_supply=4094792.00 | lit_demand=16879807.00 | short_supply=5211549.00 | short_demand=18496857.00 | accum_edges=35 | lit_edges=36 | short_edges=16\n"
     ]
    }
   ],
   "source": [
    "# Cell 6\n",
    "EDGE_MULTIPLIER = 1000\n",
    "\n",
    "display(df_scores_sorted[['ticker', 'category', 'A_end', 'A_start', 'accum_avg', 'accum_centered', 'role']])\n",
    "\n",
    "if SHOW_ACCUM_FLOW:\n",
    "    if accum_edges_df.empty:\n",
    "        display(pd.DataFrame(columns=['source', 'dest', 'flow']))\n",
    "    else:\n",
    "        display(accum_edges_df[['source', 'dest', 'flow']].assign(flow=lambda d: d['flow'] * EDGE_MULTIPLIER))\n",
    "\n",
    "\n",
    "def chord_counts(edges_df):\n",
    "    if edges_df is None or edges_df.empty:\n",
    "        return pd.DataFrame(columns=['out', 'in'])\n",
    "    out = edges_df.groupby('source')['flow'].sum().rename('out')\n",
    "    inn = edges_df.groupby('dest')['flow'].sum().rename('in')\n",
    "    df = out.to_frame().join(inn, how='outer').fillna(0)\n",
    "    return (df * EDGE_MULTIPLIER).round(0).astype(int)\n",
    "\n",
    "\n",
    "if SHOW_ACCUM_FLOW:\n",
    "    print('Accum chord counts (out/in):')\n",
    "    display(chord_counts(accum_edges_df))\n",
    "if SHOW_LIT_FLOW:\n",
    "    print('Lit chord counts (out/in):')\n",
    "    display(chord_counts(lit_edges_df))\n",
    "if SHOW_SHORT_FLOW:\n",
    "    print('Short chord counts (out/in):')\n",
    "    display(chord_counts(short_edges_df))\n",
    "\n",
    "print('Volume availability:', VOLUME_DATA_AVAILABLE, 'rows:', len(df_volume))\n",
    "if VOLUME_DATA_AVAILABLE and not df_volume.empty:\n",
    "    display(df_volume[['ticker', 'lit_buy_sum', 'lit_sell_sum', 'short_buy_sum', 'short_sell_sum']])\n",
    "    if (df_volume['lit_buy_sum'].fillna(0).sum() == 0 and df_volume['lit_sell_sum'].fillna(0).sum() == 0):\n",
    "        print('Note: lit volumes sum to zero across selected period.')\n",
    "else:\n",
    "    print('Volume data not available for lit/short buy/sell flows.')\n",
    "\n",
    "summary_parts = [\n",
    "    f'END_DATE={END_DATE_RESOLVED}',\n",
    "    f'FLOW_PERIOD_DAYS={FLOW_PERIOD_DAYS}',\n",
    "    f'accum_supply={accum_supply:.2f}',\n",
    "    f'accum_demand={accum_demand:.2f}',\n",
    "]\n",
    "if SHOW_LIT_FLOW:\n",
    "    summary_parts.append(f'lit_supply={lit_supply:.2f}')\n",
    "    summary_parts.append(f'lit_demand={lit_demand:.2f}')\n",
    "if SHOW_SHORT_FLOW:\n",
    "    summary_parts.append(f'short_supply={short_supply:.2f}')\n",
    "    summary_parts.append(f'short_demand={short_demand:.2f}')\n",
    "\n",
    "summary_parts.append(f'accum_edges={len(accum_edges_df)}')\n",
    "summary_parts.append(f'lit_edges={len(lit_edges_df)}')\n",
    "summary_parts.append(f'short_edges={len(short_edges_df)}')\n",
    "\n",
    "print(' | '.join(summary_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741ff79",
   "metadata": {},
   "outputs": [],
   "source": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection, PolyCollection\nfrom matplotlib.colors import to_rgba\nfrom matplotlib.patches import Wedge, PathPatch, Rectangle\nfrom matplotlib.path import Path\n\nBG_COLOR = '#0b0f1a'\n\nCATEGORY_LABELS = {\n    'GLOBAL_MACRO': 'GLOBAL MACRO',\n    'MAG8': 'MAG8',\n    'SECTOR_SUMMARY': 'Sector Summary',\n    'SECTOR_CORE': 'SECTORS',\n    'COMMODITIES': 'COMMODITIES',\n}\nCATEGORY_PALETTE = {\n    'GLOBAL_MACRO': \"#00AAFF\",\n    'MAG8': '#DDA0FF',\n    'SECTOR_SUMMARY': \"#72ADAF\",\n    'SECTOR_CORE': '#F6C453',\n    'COMMODITIES': '#7CDE8A',\n    'UNKNOWN': '#A0A0A0',\n}\n\n# Updated metric orders to include finra_buy for chords\nCHORD_METRIC_ORDER = ['accum', 'short', 'lit', 'finra_buy']\nBAND_ORDER = ['lit', 'accum', 'short', 'finra_buy']\n\n# Ring metrics (different from chord metrics)\nRING_METRIC_ORDER = ['accum', 'dark_lit', 'finra_buy']\n\nMETRIC_LABELS = {\n    'accum': 'Accumulation',\n    'short': 'Daily Short',\n    'lit': 'Lit',\n    'finra_buy': 'Finra Buy',\n    'dark_lit': 'Dark/Lit Ratio',\n}\n\n# Chord colors\nMETRIC_COLORS = {\n    'accum': {'sell': \"#8304B9\", 'buy': \"#26FF00\"},\n    'short': {'sell': \"#FF1E1E\", 'buy': \"#00AEFF\"},\n    'lit': {'sell': \"#FF9D0B\", 'buy': \"#EEFF00\"},\n    'finra_buy': {'low': \"#4A1A6B\", 'high': \"#FF44FF\"},  # Purple gradient for finra\n}\n\n# Ring colors (new encodings)\nRING_COLORS = {\n    'accum': {'negative': \"#8304B9\", 'positive': \"#26FF00\"},  # Purple (-) to Green (+)\n    'dark_lit': {'lit': \"#4488FF\", 'neutral': \"#888888\", 'dark': \"#FF4444\"},  # Blue-Gray-Red\n    'finra_buy': {},  # Uses category colors dynamically\n}\n\n\ndef blend_color(c1, c2, t=0.5):\n    a = np.array(to_rgba(c1))\n    b = np.array(to_rgba(c2))\n    return a * (1 - t) + b * t\n\n\ndef soften_color(color, amount, base=BG_COLOR):\n    return blend_color(color, base, max(0.0, min(1.0, amount)))\n\n\ndef darken_color(color, factor):\n    \"\"\"Darken a color by factor (0=black, 1=original)\"\"\"\n    rgba = np.array(to_rgba(color))\n    rgba[:3] = rgba[:3] * max(0.0, min(1.0, factor))\n    return rgba\n\n\ndef add_gradient_curve(ax, points, color_start, color_end, lw, alpha):\n    if len(points) < 2:\n        return\n    segments = np.stack([points[:-1], points[1:]], axis=1)\n    c0 = np.array(to_rgba(color_start))\n    c1 = np.array(to_rgba(color_end))\n    t = np.linspace(0, 1, len(segments))[:, None]\n    colors = c0 * (1 - t) + c1 * t\n    colors[:, 3] = colors[:, 3] * alpha\n    lc = LineCollection(segments, colors=colors, linewidths=lw, capstyle='round')\n    ax.add_collection(lc)\n\n\ndef arc_points(a0, a1, r, n=None):\n    count = n or CHORD_ARC_POINTS\n    angles = np.linspace(a0, a1, count)\n    return np.column_stack([r * np.cos(angles), r * np.sin(angles)])\n\n\ndef bezier_curve(p0, p1, p2, n=None):\n    count = n or CHORD_CURVE_POINTS\n    t = np.linspace(0, 1, count)[:, None]\n    return (1 - t) ** 2 * p0 + 2 * (1 - t) * t * p1 + t ** 2 * p2\n\n\ndef ribbon_patch(a0, a1, b0, b1, r, color_start, color_end, alpha):\n    arc1 = arc_points(a0, a1, r, n=16)\n    arc2 = arc_points(b0, b1, r, n=16)\n    curve1 = bezier_curve(arc1[-1], np.array([0.0, 0.0]), arc2[0], n=24)\n    curve2 = bezier_curve(arc2[-1], np.array([0.0, 0.0]), arc1[0], n=24)\n    poly = np.vstack([arc1, curve1, arc2, curve2])\n    codes = [Path.MOVETO] + [Path.LINETO] * (len(poly) - 1)\n    path = Path(poly, codes)\n    mid = blend_color(color_start, color_end, 0.5)\n    return PathPatch(path, facecolor=mid, edgecolor='none', alpha=alpha)\n\n\ndef gradient_fill_collection(a0, a1, b0, b1, r, color_start, color_end, alpha, steps=None):\n    steps = steps or CHORD_GRADIENT_STEPS\n    arc1 = arc_points(a0, a1, r, n=4)\n    arc2 = arc_points(b0, b1, r, n=4)\n    p_a0, p_a1 = arc1[0], arc1[-1]\n    p_b0, p_b1 = arc2[0], arc2[-1]\n    left = bezier_curve(p_a0, np.array([0.0, 0.0]), p_b0, n=steps + 1)\n    right = bezier_curve(p_a1, np.array([0.0, 0.0]), p_b1, n=steps + 1)\n    polys = []\n    colors = []\n    for i in range(steps):\n        quad = np.vstack([left[i], left[i + 1], right[i + 1], right[i]])\n        t = (i + 0.5) / steps\n        color = blend_color(color_start, color_end, t)\n        color[3] = color[3] * alpha\n        polys.append(quad)\n        colors.append(color)\n    return PolyCollection(polys, facecolors=colors, edgecolors='none')\n\n\ndef draw_ribbon(ax, a0, a1, b0, b1, r, color_start, color_end, fill_alpha, line_alpha, lw):\n    if USE_GRADIENT_FILL:\n        clip = ribbon_patch(a0, a1, b0, b1, r, color_start, color_end, alpha=1.0)\n        clip.set_facecolor('none')\n        clip.set_edgecolor('none')\n        ax.add_patch(clip)\n        fill = gradient_fill_collection(a0, a1, b0, b1, r, color_start, color_end, fill_alpha)\n        fill.set_clip_path(clip)\n        ax.add_collection(fill)\n    else:\n        patch = ribbon_patch(a0, a1, b0, b1, r, color_start, color_end, alpha=fill_alpha)\n        ax.add_patch(patch)\n    mid_a = (a0 + a1) / 2\n    mid_b = (b0 + b1) / 2\n    p0 = np.array([r * np.cos(mid_a), r * np.sin(mid_a)])\n    p2 = np.array([r * np.cos(mid_b), r * np.sin(mid_b)])\n    curve = bezier_curve(p0, np.array([0.0, 0.0]), p2)\n    add_gradient_curve(ax, curve, color_start, color_end, lw=lw, alpha=line_alpha)\n\n\ndef make_time_bins(dates, bins):\n    if not dates:\n        return []\n    if bins is None or bins <= 0:\n        return [dates]\n    bins = min(bins, len(dates))\n    split = np.array_split(dates, bins)\n    return [list(s) for s in split if len(s)]\n\n\ndef compute_metric_totals(edges_df):\n    totals = {}\n    if edges_df is None or edges_df.empty:\n        return totals\n    for row in edges_df.itertuples():\n        totals[row.source] = totals.get(row.source, 0.0) + row.flow\n        totals[row.dest] = totals.get(row.dest, 0.0) + row.flow\n    return totals\n\n\ndef filter_edges(edges_df):\n    if edges_df is None or edges_df.empty:\n        return edges_df\n    df = edges_df.copy()\n    if MAX_EDGES_PER_METRIC and MAX_EDGES_PER_METRIC > 0:\n        df = df.nlargest(MAX_EDGES_PER_METRIC, 'flow')\n    return df\n\n\ndef expand_edges(edges_df, splits):\n    if edges_df is None or edges_df.empty:\n        return edges_df\n    if not splits or splits <= 1:\n        return edges_df\n    max_flow = edges_df['flow'].max() if 'flow' in edges_df.columns and not edges_df.empty else 0.0\n    if max_flow <= 0:\n        return edges_df\n    rows = []\n    for row in edges_df.itertuples():\n        n = max(1, int(round(splits * (row.flow / max_flow))))\n        if EDGE_RIBBON_MAX and EDGE_RIBBON_MAX > 0:\n            n = min(n, EDGE_RIBBON_MAX)\n        flow = row.flow / n\n        for _ in range(n):\n            rows.append({'source': row.source, 'dest': row.dest, 'flow': flow})\n    return pd.DataFrame(rows)\n\n\ndef allocate_intervals(edges_df, band_map, metric_key, centered=True, center_offset=0.0):\n    if edges_df is None or edges_df.empty:\n        return []\n    \n    min_width_rad = RIBBON_MIN_WIDTH_RAD.get(metric_key, 0.003)\n    \n    out_counts = edges_df.groupby('source').size().to_dict()\n    in_counts = edges_df.groupby('dest').size().to_dict()\n    max_flow = edges_df['flow'].max() if not edges_df.empty else 1.0\n    \n    out_slot_info = {}\n    in_slot_info = {}\n    \n    for ticker in ticker_order:\n        spans = band_map.get(ticker, {}).get(metric_key)\n        if not spans:\n            continue\n        \n        out_range = spans['out']\n        out_arc = out_range[1] - out_range[0]\n        arc_center_out = (out_range[0] + out_range[1]) / 2 + center_offset\n        n_out = out_counts.get(ticker, 0)\n        if n_out > 0:\n            total_gap = RIBBON_GAP_RAD * (n_out - 1)\n            usable_out = max(0.0, out_arc - total_gap)\n            slot_width_out = usable_out / n_out\n            if slot_width_out < min_width_rad and n_out > 1:\n                slot_width_out = min_width_rad\n                total_gap = max(0.0, out_arc - n_out * slot_width_out)\n            \n            actual_gap = total_gap / max(1, n_out - 1) if n_out > 1 else 0.0\n            total_width = n_out * slot_width_out + (n_out - 1) * actual_gap\n            \n            if centered:\n                start_pos = arc_center_out - total_width / 2\n            else:\n                start_pos = out_range[0]\n            \n            out_slot_info[ticker] = {\n                'slot_width': slot_width_out,\n                'cursor': start_pos,\n                'gap': actual_gap,\n            }\n        \n        in_range = spans['in']\n        in_arc = in_range[1] - in_range[0]\n        arc_center_in = (in_range[0] + in_range[1]) / 2 + center_offset\n        n_in = in_counts.get(ticker, 0)\n        if n_in > 0:\n            total_gap = RIBBON_GAP_RAD * (n_in - 1)\n            usable_in = max(0.0, in_arc - total_gap)\n            slot_width_in = usable_in / n_in\n            if slot_width_in < min_width_rad and n_in > 1:\n                slot_width_in = min_width_rad\n                total_gap = max(0.0, in_arc - n_in * slot_width_in)\n            \n            actual_gap = total_gap / max(1, n_in - 1) if n_in > 1 else 0.0\n            total_width = n_in * slot_width_in + (n_in - 1) * actual_gap\n            \n            if centered:\n                start_pos = arc_center_in - total_width / 2\n            else:\n                start_pos = in_range[0]\n            \n            in_slot_info[ticker] = {\n                'slot_width': slot_width_in,\n                'cursor': start_pos,\n                'gap': actual_gap,\n            }\n    \n    intervals = []\n    for row in edges_df.sort_values('flow', ascending=False).itertuples():\n        src, dst, flow = row.source, row.dest, row.flow\n        if src not in out_slot_info or dst not in in_slot_info:\n            continue\n        \n        out_info = out_slot_info[src]\n        in_info = in_slot_info[dst]\n        \n        if RIBBON_WIDTH_SCALE_BY_FLOW:\n            flow_scale = (flow / max_flow) ** 0.5 if max_flow > 0 else 1.0\n            out_width = max(min_width_rad * 0.5, out_info['slot_width'] * flow_scale)\n            in_width = max(min_width_rad * 0.5, in_info['slot_width'] * flow_scale)\n        else:\n            out_width = out_info['slot_width']\n            in_width = in_info['slot_width']\n        \n        a0 = out_info['cursor']\n        a1 = a0 + out_width\n        b0 = in_info['cursor']\n        b1 = b0 + in_width\n        \n        out_info['cursor'] = a0 + out_info['slot_width'] + out_info['gap']\n        in_info['cursor'] = b0 + in_info['slot_width'] + in_info['gap']\n        \n        if a1 > a0 and b1 > b0:\n            intervals.append({'source': src, 'dest': dst, 'flow': flow, 'a0': a0, 'a1': a1, 'b0': b0, 'b1': b1})\n    \n    return intervals\n\n\ndef metric_visible(metric_key):\n    if metric_key == \"accum\":\n        return SHOW_ACCUM_FLOW\n    if metric_key == \"short\":\n        return SHOW_SHORT_FLOW\n    if metric_key == \"lit\":\n        return SHOW_LIT_FLOW\n    if metric_key == \"finra_buy\":\n        return SHOW_FINRA_FLOW\n    return True\n\n\n# Prepare chord metric datasets\naccum_edges_plot = filter_edges(accum_edges_df)\nshort_edges_plot = filter_edges(short_edges_df)\nlit_edges_plot = filter_edges(lit_edges_df)\nfinra_edges_plot = filter_edges(finra_edges_df) if 'finra_edges_df' in dir() else pd.DataFrame()\n\nmetric_edges = {\n    'accum': accum_edges_plot,\n    'short': short_edges_plot,\n    'lit': lit_edges_plot,\n    'finra_buy': finra_edges_plot,\n}\n\nmetric_edges_draw = {m: expand_edges(metric_edges[m], EDGE_RIBBON_SPLITS) for m in CHORD_METRIC_ORDER}\n\nmetric_nets = {\n    'accum': df_scores.set_index('ticker')['accum_centered'].to_dict() if 'accum_centered' in df_scores.columns else df_scores.set_index('ticker')['delta'].to_dict(),\n    'short': df_volume.set_index('ticker')['short_net'].to_dict() if 'short_net' in df_volume.columns else {},\n    'lit': df_volume.set_index('ticker')['lit_net'].to_dict() if 'lit_net' in df_volume.columns else {},\n    'finra_buy': df_volume.set_index('ticker')['finra_buy_sum'].to_dict() if 'finra_buy_sum' in df_volume.columns else {},\n}\n\ndf_accum_level = df_raw_full[df_raw_full['date'].isin(window_dates)][['ticker', 'date', 'accumulation_score']].copy()\nif not df_accum_level.empty:\n    df_accum_level = df_accum_level.rename(columns={'accumulation_score': 'value'})\n\n# Ring metric daily data\nring_metric_daily = {\n    'accum': df_accum_level,\n    'dark_lit': df_dark_lit_daily if 'df_dark_lit_daily' in dir() else pd.DataFrame(),\n    'finra_buy': df_finra_daily if 'df_finra_daily' in dir() else pd.DataFrame(),\n}\n\n# Build ticker layout\ngrouped = {\n    'GLOBAL_MACRO': [t for t in ticker_order if ticker_category.get(t) == 'GLOBAL_MACRO'],\n    'MAG8': [t for t in ticker_order if ticker_category.get(t) == 'MAG8'],\n    'SECTOR_SUMMARY': [t for t in ticker_order if ticker_category.get(t) == 'SECTOR_SUMMARY'],\n    'SECTOR_CORE': [t for t in ticker_order if ticker_category.get(t) == 'SECTOR_CORE'],\n    'COMMODITIES': [t for t in ticker_order if ticker_category.get(t) == 'COMMODITIES'],\n}\n\nmetric_totals = {m: compute_metric_totals(metric_edges[m]) for m in CHORD_METRIC_ORDER}\n\ntotal_nodes = sum(len(v) for v in grouped.values())\nif total_nodes == 0:\n    print('No nodes to plot.')\nelse:\n    gap = math.radians(CATEGORY_GAP_DEG)\n    total_gap = gap * len([g for g in grouped.values() if g])\n    usable = 2 * math.pi - total_gap\n    if usable <= 0:\n        usable = 2 * math.pi\n    step = usable / total_nodes\n    arc_span = step * 0.85\n\n    angles = {}\n    spans = {}\n    angle = 0.0\n    for cat in ['GLOBAL_MACRO', 'MAG8', 'SECTOR_SUMMARY', 'SECTOR_CORE', 'COMMODITIES']:\n        if not grouped[cat]:\n            continue\n        angle += gap / 2\n        for t in grouped[cat]:\n            angles[t] = angle\n            spans[t] = (angle - arc_span / 2, angle + arc_span / 2)\n            angle += step\n        angle += gap / 2\n\n    # Metric bands per ticker (for chords)\n    band_map = {}\n    for t, (a0, a1) in spans.items():\n        max_span = (a1 - a0)\n        chord_span = min(max_span, max_span * CHORD_ARC_FRACTION)\n        chord_center = (a0 + a1) / 2\n        band_gap = chord_span * BAND_GAP_FRAC\n\n        if METRIC_BAND_MODE == 'proportional':\n            weights = {m: metric_totals[m].get(t, 0.0) for m in BAND_ORDER}\n            if sum(weights.values()) <= 0:\n                weights = {m: 1.0 for m in BAND_ORDER}\n        else:\n            weights = {m: 1.0 for m in BAND_ORDER}\n\n        total_w = sum(weights.values())\n        if total_w <= 0:\n            weights = {m: 1.0 for m in BAND_ORDER}\n            total_w = sum(weights.values())\n\n        available = chord_span - band_gap * 3  # 3 gaps for 4 metrics\n        if available <= 0:\n            band_gap = 0.0\n            available = chord_span\n\n        lengths = {m: max(0.0, available * (weights[m] / total_w)) for m in BAND_ORDER}\n        acc_len = lengths.get('accum', available / 4)\n        lit_len = lengths.get('lit', available / 4)\n        short_len = lengths.get('short', available / 4)\n        finra_len = lengths.get('finra_buy', available / 4)\n\n        # Layout: lit | accum | short | finra_buy\n        total_len = lit_len + acc_len + short_len + finra_len + 3 * band_gap\n        start = chord_center - total_len / 2\n        \n        lit_start = start\n        lit_end = lit_start + lit_len\n        acc_start = lit_end + band_gap\n        acc_end = acc_start + acc_len\n        short_start = acc_end + band_gap\n        short_end = short_start + short_len\n        finra_start = short_end + band_gap\n        finra_end = finra_start + finra_len\n\n        def band_slices(start, end):\n            dir_gap = (end - start) * DIR_GAP_FRAC\n            dir_gap = min(dir_gap, (end - start) * 0.4)\n            half = max(0.0, (end - start - dir_gap) / 2)\n            return {\n                'band': (start, end),\n                'out': (start, start + half),\n                'in': (start + half + dir_gap, end),\n            }\n\n        band_map[t] = {\n            'lit': band_slices(lit_start, lit_end),\n            'accum': band_slices(acc_start, acc_end),\n            'short': band_slices(short_start, short_end),\n            'finra_buy': band_slices(finra_start, finra_end),\n        }\n\n    # Prepare intervals per metric with per-metric center offset\n    metric_intervals = {}\n    for m in CHORD_METRIC_ORDER:\n        if not metric_visible(m):\n            metric_intervals[m] = []\n            continue\n        offset = RIBBON_CENTER_OFFSET.get(m, 0.0)\n        metric_intervals[m] = allocate_intervals(\n            metric_edges_draw[m], band_map, m, \n            centered=RIBBON_CENTERED, \n            center_offset=offset\n        )\n    \n    for m in CHORD_METRIC_ORDER:\n        if metric_visible(m):\n            offset = RIBBON_CENTER_OFFSET.get(m, 0.0)\n            min_w = RIBBON_MIN_WIDTH_RAD.get(m, 0.003)\n            print(f\"{m}: {len(metric_intervals[m])} ribbons (centered={RIBBON_CENTERED}, offset={offset:.3f}, min_width={min_w:.4f})\")\n\n    # Time bins for outer ring\n    window_dates_sorted = sorted(window_dates)\n    time_bins = make_time_bins(window_dates_sorted, TIME_SLICE_BINS)\n    time_bins = list(reversed(time_bins))\n\n    # Pre-compute normalization for ring rendering\n    ring_max_mag = {}\n    ring_min_val = {}\n    for m in RING_METRIC_ORDER:\n        df_m = ring_metric_daily.get(m, pd.DataFrame())\n        if df_m is None or df_m.empty:\n            ring_max_mag[m] = 1.0\n            ring_min_val[m] = 0.0\n            continue\n        if m == 'accum' and 'value' in df_m.columns and df_m['value'].notna().any():\n            ring_max_mag[m] = float(df_m['value'].abs().max())\n            ring_min_val[m] = 0.0  # For magnitude-based thickness\n        elif m == 'dark_lit' and 'total_volume' in df_m.columns:\n            ring_max_mag[m] = float(df_m['total_volume'].max()) if df_m['total_volume'].notna().any() else 1.0\n            ring_min_val[m] = 0.0\n        elif m == 'finra_buy' and 'finra_buy' in df_m.columns:\n            vals = df_m['finra_buy'].dropna()\n            if len(vals) > 0:\n                ring_max_mag[m] = float(np.log10(vals.max() + 1)) if vals.max() > 0 else 1.0\n                ring_min_val[m] = float(np.log10(vals.min() + 1)) if vals.min() > 0 else 0.0\n            else:\n                ring_max_mag[m] = 1.0\n                ring_min_val[m] = 0.0\n        else:\n            ring_max_mag[m] = 1.0\n            ring_min_val[m] = 0.0\n\n    # Build bin value lookup for rings\n    ring_bin_data = {m: {} for m in RING_METRIC_ORDER}\n    for m in RING_METRIC_ORDER:\n        df_m = ring_metric_daily.get(m, pd.DataFrame())\n        if df_m is None or df_m.empty:\n            for t in ticker_order:\n                ring_bin_data[m][t] = [{'value': 0.0, 'extra': 0.0} for _ in time_bins]\n            continue\n        for t in ticker_order:\n            data_by_bin = []\n            for bin_dates in time_bins:\n                mask = (df_m['ticker'] == t) & (df_m['date'].isin(bin_dates))\n                if m == 'accum' and 'value' in df_m.columns:\n                    val = float(df_m.loc[mask, 'value'].mean()) if mask.any() else 0.0\n                    data_by_bin.append({'value': val, 'extra': 0.0})\n                elif m == 'dark_lit':\n                    if mask.any():\n                        total = float(df_m.loc[mask, 'total_volume'].sum())\n                        ratio = float(df_m.loc[mask, 'dark_ratio'].mean())\n                    else:\n                        total = 0.0\n                        ratio = 0.5\n                    data_by_bin.append({'value': total, 'extra': ratio})\n                elif m == 'finra_buy' and 'finra_buy' in df_m.columns:\n                    val = float(df_m.loc[mask, 'finra_buy'].sum()) if mask.any() else 0.0\n                    data_by_bin.append({'value': val, 'extra': 0.0})\n                else:\n                    data_by_bin.append({'value': 0.0, 'extra': 0.0})\n            ring_bin_data[m][t] = data_by_bin\n\n    # Start plot\n    fig, ax = plt.subplots(figsize=(12, 12), subplot_kw={'aspect': 'equal'})\n    fig.patch.set_facecolor(BG_COLOR)\n    ax.set_facecolor(BG_COLOR)\n    ax.axis('off')\n\n    theta = np.linspace(0, 2 * math.pi, 400)\n    ax.plot(np.cos(theta), np.sin(theta), color='#39424e', lw=1.0, alpha=0.6)\n\n    # Ticker arcs (disabled by default now)\n    if SHOW_TICKER_ARC:\n        ticker_outer = 1.05\n        ticker_width = 0.035\n        for t, (a0, a1) in spans.items():\n            wedge = Wedge(\n                (0, 0), ticker_outer, math.degrees(a0), math.degrees(a1),\n                width=ticker_width,\n                facecolor=CATEGORY_PALETTE.get(ticker_category.get(t, 'UNKNOWN'), '#A0A0A0'),\n                edgecolor='#222831', lw=0.4, alpha=0.9,\n            )\n            ax.add_patch(wedge)\n    else:\n        ticker_outer = 1.02\n\n    # Outer rings (NEW ENCODING: accum, dark_lit, finra_buy)\n    if SHOW_VOLUME_RING:\n        track_span = RING_BASE_THICKNESS + RING_THICKNESS_SCALE + RING_GAP\n        for idx, m in enumerate(RING_METRIC_ORDER):\n            inner_base = ticker_outer + 0.02 + idx * track_span\n            max_mag = ring_max_mag.get(m, 1.0)\n            \n            for t, (a0, a1) in spans.items():\n                bin_data = ring_bin_data[m].get(t, [])\n                if not bin_data:\n                    continue\n                slice_gap = (a1 - a0) * 0.02\n                total_slice = (a1 - a0) - slice_gap * (len(bin_data) - 1)\n                if total_slice <= 0:\n                    total_slice = (a1 - a0)\n                    slice_gap = 0.0\n                slice_len = total_slice / len(bin_data)\n                cursor = a0\n                \n                for data in bin_data:\n                    val = data['value']\n                    extra = data['extra']\n                    \n                    if m == 'accum':\n                        # Ring 1: Thickness = |value|, Color = direction\n                        mag = abs(val)\n                        thickness = RING_BASE_THICKNESS + RING_THICKNESS_SCALE * (mag / max_mag if max_mag > 0 else 0.0)\n                        color = RING_COLORS['accum']['positive'] if val >= 0 else RING_COLORS['accum']['negative']\n                    \n                    elif m == 'dark_lit':\n                        # Ring 2: Thickness = total volume, Color = dark/lit ratio\n                        thickness = RING_BASE_THICKNESS + RING_THICKNESS_SCALE * (val / max_mag if max_mag > 0 else 0.0)\n                        ratio = extra  # dark_ratio: 0=all lit, 0.5=balanced, 1=all dark\n                        if ratio < 0.5:\n                            color = blend_color(RING_COLORS['dark_lit']['lit'], RING_COLORS['dark_lit']['neutral'], ratio * 2)\n                        else:\n                            color = blend_color(RING_COLORS['dark_lit']['neutral'], RING_COLORS['dark_lit']['dark'], (ratio - 0.5) * 2)\n                    \n                    elif m == 'finra_buy':\n                        # Ring 3: Thickness = log volume, Color = category color gradient\n                        if val > 0:\n                            log_val = np.log10(val + 1)\n                            thickness = RING_BASE_THICKNESS + RING_THICKNESS_SCALE * (log_val / max_mag if max_mag > 0 else 0.0)\n                            brightness = 0.2 + 0.8 * (log_val / max_mag if max_mag > 0 else 0.5)\n                        else:\n                            thickness = RING_BASE_THICKNESS\n                            brightness = 0.2\n                        cat_color = CATEGORY_PALETTE.get(ticker_category.get(t, 'UNKNOWN'), '#A0A0A0')\n                        color = darken_color(cat_color, brightness)\n                    \n                    else:\n                        thickness = RING_BASE_THICKNESS\n                        color = '#666666'\n                    \n                    wedge = Wedge(\n                        (0, 0), inner_base + thickness, math.degrees(cursor), math.degrees(cursor + slice_len),\n                        width=thickness,\n                        facecolor=color, edgecolor='none', alpha=0.85,\n                    )\n                    ax.add_patch(wedge)\n                    cursor += slice_len + slice_gap\n\n    # Draw chords per metric\n    for m in CHORD_METRIC_ORDER:\n        if not metric_visible(m):\n            continue\n        intervals = metric_intervals[m]\n        if not intervals:\n            continue\n        \n        if m == 'finra_buy':\n            raw_start = METRIC_COLORS[m]['low']\n            raw_end = METRIC_COLORS[m]['high']\n        else:\n            raw_start = METRIC_COLORS[m]['sell']\n            raw_end = METRIC_COLORS[m]['buy']\n        color_start = soften_color(raw_start, CHORD_COLOR_SOFTEN)\n        color_end = soften_color(raw_end, CHORD_COLOR_SOFTEN)\n        max_flow = metric_edges[m]['flow'].max() if metric_edges[m] is not None and not metric_edges[m].empty else 1.0\n        for edge in intervals:\n            flow = edge['flow']\n            lw = 0.6 + 2.2 * ((flow / max_flow) ** 0.6) if max_flow > 0 else 1.0\n            draw_ribbon(ax, edge['a0'], edge['a1'], edge['b0'], edge['b1'], CHORD_RADIUS, color_start, color_end, fill_alpha=CHORD_FILL_ALPHA, line_alpha=CHORD_LINE_ALPHA, lw=lw)\n\n    # Ticker labels\n    for t, ang in angles.items():\n        x, y = math.cos(ang), math.sin(ang)\n        r = CHORD_RADIUS + (ticker_outer - CHORD_RADIUS) * 0.4\n        rot = math.degrees(ang)\n        if math.pi / 2 < ang < 3 * math.pi / 2:\n            rot += 180\n        ax.text(r * x, r * y, t, color='#FFFFFF', fontsize=9, fontweight='bold',\n                ha='center', va='center', rotation=rot, rotation_mode='anchor')\n\n    # Category labels\n    for cat, tickers in grouped.items():\n        if not tickers or cat == 'SECTOR_SUMMARY':\n            continue\n        mid_angle = np.mean([angles[t] for t in tickers])\n        ax.text(\n            (ticker_outer + 0.4) * math.cos(mid_angle),\n            (ticker_outer + 0.4) * math.sin(mid_angle),\n            CATEGORY_LABELS.get(cat, cat),\n            color=CATEGORY_PALETTE.get(cat, '#A0A0A0'),\n            fontsize=12, fontweight='bold', ha='center', va='center'\n        )\n\n    # Title and settings\n    date_range = f\"{window_dates_sorted[0]} -> {window_dates_sorted[-1]}\" if window_dates_sorted else 'n/a'\n    fig.text(0.5, 0.97, 'Sector Money-Flow Chord Summary', ha='center', va='top', color='white', fontsize=14, fontweight='bold')\n    fig.text(0.5, 0.945, f'Date Range: {date_range}', ha='center', va='top', color='#C9D1D9', fontsize=10)\n    \n    settings_lines = [\n        f\"Date range: {date_range}\",\n        f\"Tickers: {len(ticker_order)} | FLOW_PERIOD_DAYS={FLOW_PERIOD_DAYS}\",\n    ]\n    fig.text(0.1, 0.05, '\\n'.join(settings_lines), ha='left', va='bottom', color='#C9D1D9', fontsize=8)\n\n    # Upper-left Legend (Chords + Rings sections)\n    leg = fig.add_axes([0.08, 0.72, 0.30, 0.25])\n    leg.axis('off')\n    leg.set_facecolor('none')\n    leg.set_xlim(0, 1)\n    leg.set_ylim(0, 1)\n    \n    y = 0.95\n    # Chords section title\n    leg.text(0.0, y, 'Chords', color='white', fontsize=10, fontweight='bold', va='top')\n    y -= 0.10\n    \n    # Chord legend items\n    chord_items = [\n        ('accum', 'Accumulation', METRIC_COLORS['accum']['sell'], METRIC_COLORS['accum']['buy']),\n        ('short', 'Short', METRIC_COLORS['short']['sell'], METRIC_COLORS['short']['buy']),\n        ('lit', 'Lit', METRIC_COLORS['lit']['sell'], METRIC_COLORS['lit']['buy']),\n        ('finra_buy', 'Finra Buy', METRIC_COLORS['finra_buy']['low'], METRIC_COLORS['finra_buy']['high']),\n    ]\n    for key, label, c_start, c_end in chord_items:\n        if not metric_visible(key):\n            continue\n        xs = np.linspace(0.0, 0.20, 30)\n        points = np.column_stack([xs, np.full_like(xs, y)])\n        segments = np.stack([points[:-1], points[1:]], axis=1)\n        c0 = np.array(to_rgba(c_start))\n        c1 = np.array(to_rgba(c_end))\n        t_arr = np.linspace(0, 1, len(segments))[:, None]\n        colors = c0 * (1 - t_arr) + c1 * t_arr\n        leg.add_collection(LineCollection(segments, colors=colors, linewidths=4))\n        leg.text(0.24, y, label, color='white', fontsize=8, va='center')\n        y -= 0.09\n    \n    y -= 0.05\n    # Rings section title\n    leg.text(0.0, y, 'Rings', color='white', fontsize=10, fontweight='bold', va='top')\n    y -= 0.10\n    \n    # Ring legend items\n    ring_items = [\n        ('Ring 1', 'Accumulation Score', RING_COLORS['accum']['negative'], RING_COLORS['accum']['positive']),\n        ('Ring 2', 'Dark/Lit Ratio', RING_COLORS['dark_lit']['lit'], RING_COLORS['dark_lit']['dark']),\n        ('Ring 3', 'Finra Buy Volume', '#1A3A3A', '#72ADAF'),  # Example category gradient\n    ]\n    for ring_label, desc, c_start, c_end in ring_items:\n        xs = np.linspace(0.0, 0.20, 30)\n        points = np.column_stack([xs, np.full_like(xs, y)])\n        segments = np.stack([points[:-1], points[1:]], axis=1)\n        c0 = np.array(to_rgba(c_start))\n        c1 = np.array(to_rgba(c_end))\n        t_arr = np.linspace(0, 1, len(segments))[:, None]\n        colors = c0 * (1 - t_arr) + c1 * t_arr\n        leg.add_collection(LineCollection(segments, colors=colors, linewidths=4))\n        leg.text(0.24, y, f\"{ring_label}: {desc}\", color='white', fontsize=7, va='center')\n        y -= 0.09\n    \n    y -= 0.02\n    leg.text(0.0, y, '(each ring segment = 1 calendar day)', color='#888888', fontsize=7, va='top', style='italic')\n\n    # Upper-right Category Legend (dynamic, gradient squares)\n    present_categories = [cat for cat in ['GLOBAL_MACRO', 'MAG8', 'SECTOR_SUMMARY', 'SECTOR_CORE', 'COMMODITIES'] \n                         if grouped.get(cat)]\n    \n    if present_categories:\n        cat_leg = fig.add_axes([0.78, 0.78, 0.18, 0.18])\n        cat_leg.axis('off')\n        cat_leg.set_facecolor('none')\n        cat_leg.set_xlim(0, 1)\n        cat_leg.set_ylim(0, 1)\n        \n        y = 0.95\n        cat_leg.text(0.0, y, 'Categories', color='white', fontsize=10, fontweight='bold', va='top')\n        y -= 0.12\n        \n        for cat in present_categories:\n            base_color = CATEGORY_PALETTE.get(cat, '#A0A0A0')\n            dark_color = darken_color(base_color, 0.2)\n            \n            # Draw gradient square\n            for i in range(10):\n                t = i / 9.0\n                color = blend_color(dark_color, base_color, t)\n                rect = Rectangle((i * 0.008, y - 0.04), 0.008, 0.08, facecolor=color, edgecolor='none')\n                cat_leg.add_patch(rect)\n            \n            cat_leg.text(0.12, y, CATEGORY_LABELS.get(cat, cat), color='white', fontsize=8, va='center')\n            y -= 0.16\n\n    # Top inflow/outflow table\n    def top_tickers(net_map, positive=True, k=3):\n        if not net_map:\n            return 'n/a'\n        items = [(k, v) for k, v in net_map.items() if v is not None and np.isfinite(v)]\n        if positive:\n            items = sorted([x for x in items if x[1] > 0], key=lambda x: x[1], reverse=True)\n        else:\n            items = sorted([x for x in items if x[1] < 0], key=lambda x: x[1])\n        return ', '.join([k for k, _ in items[:k]]) or 'n/a'\n\n    table_rows = [\n        ('Accumulation Score', top_tickers(metric_nets.get('accum', {}), True), top_tickers(metric_nets.get('accum', {}), False)),\n        ('Daily Short Buy/Sell', top_tickers(metric_nets.get('short', {}), True), top_tickers(metric_nets.get('short', {}), False)),\n        ('Lit Buy/Sell Ratio', top_tickers(metric_nets.get('lit', {}), True), top_tickers(metric_nets.get('lit', {}), False)),\n    ]\n    col1, col2, col3 = 26, 24, 24\n    table_lines = [f\"{'Metric':<{col1}}{'Buy (Top)':<{col2}}{'Sell (Top)':<{col3}}\", '-' * (col1 + col2 + col3)]\n    for label, buy_str, sell_str in table_rows:\n        table_lines.append(f\"{label:<{col1}}{buy_str:<{col2}}{sell_str:<{col3}}\")\n    fig.text(0.50, 0.05, '\\n'.join(table_lines), ha='left', va='bottom', color='#C9D1D9', fontsize=9, fontfamily='monospace', linespacing=1.2)\n    \n    plt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}